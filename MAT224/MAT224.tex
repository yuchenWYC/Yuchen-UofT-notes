\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by Y.W.}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\litran}[0]{$T: V \rightarrow W$ }
\newcommand{\slitran}[0]{Let $ T: V \rightarrow W$ be a linear transformation }
\newcommand{\mt}[0]{$[T]_\alpha^\beta$ }
\newcommand{\qed}[0]{$\hfill\blacksquare$}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\mc}[0]{\mathbb{C}}
\newcommand{\vx}[0]{\tb{x}}
\newcommand{\vy}[0]{\tb{y}}
\newcommand{\vz}[0]{\tb{z}}
\newcommand{\vo}[0]{\tb{0}}
\newcommand{\vu}[0]{\tb{u}}
\newcommand{\vw}[0]{\tb{w}}
\newcommand{\vv}[0]{\tb{v}}
\newcommand{\va}[0]{\tb{a}}
\newcommand{\vb}[0]{\tb{b}}
\newcommand{\trans}[3]{{#1}: {#2} \rightarrow {#3}}

% Attr.
\title{MAT224 Linear Algebra II Winter 2019\\ Lecture Notes}
\author{\textcolor{blue}{\href{https://www.yuchenwyc.com}{Yuchen Wang}}}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Vector Spaces}
	\subsection{Vector Spaces}
	\paragraph{Definition 1.1.1} A (real) vector space is a set V (whose elements are called vectors) together with
	\begin{enumerate}
		\item an operation called vector addition, which for each pair of vectors $\vx, \vy \in V$ produced another vector in V denoted $\vx + \vy$, and
		\item an operation called multiplication by a scalar (a real number), which for each vector $\vx \in V$, and each scalar $c \in \real$ produced another vector in V denoted $c\vx$
	\end{enumerate}
	Furthermore, the two operations must satisfy the following axioms:
	$\forall \vx, \vy, \vz \in V, \forall c, d \in \real$,
	\begin{enumerate}
		\item $(\vx + \vy) + \vz = \vx + (\vy + \vz)$
		\item $\vx + \vy = \vy + \vx$
		\item $\exists \vo \in V$ s.t. $\vx + \vo = \vx$ (additive identity)
		\item $\exists -\vx \in V$ s.t. $\vx + -\vx = \vo$ (additive inverse)
		\item $c(\vx + \vy) = c\vx + c\vy$
		\item $(c + d)\vx = c\vx + d\vx$
		\item $(cd)\vx = c(d\vx)$
		\item $1\vx = \vx$
	\end{enumerate}
	\paragraph{Proposition 1.1.6}
	Let V be a vector space. Then
	\begin{enumerate}
		\item The zero vector $\vo$ is unique.
		\item For all $\vx \in V, 0\vx = \vo$.
		\item For each $\vx \in V$, the additive inverse $-\vx$ is unique.
		\item For all $\vx \in V$, and all $c \in \real, (-c)\vx = -(c\vx)$.
	\end{enumerate}
	\paragraph{Smooth functions}
	$C^\infty$ \newline
	Most functions are not smooth.
	\subsection{Subspaces}
	\paragraph{Example}
	$C^\infty(\real) < C^k(\real) <$ Differentiable functions $< C(\real) < F(\real)$  
	\paragraph{Definition} Let V be a vector space and Let $W \subseteq V$ be a subset. Then W is a (vector) subspace of V if W is a vector space itself under the operations of vector sum and scalar multiplication from V.
	\paragraph{Theorem 1.2.8}  Let V be a vector space and Let $W \subseteq V$ be a \textcolor{red}{nonempty} subset of V. Then W is a subspace of V iff $\forall \vx, \vy \in W$, and all $c \in \real$, we have $c\vx + \vy \in W$. \newline
	\begin{proof}
	$\rightarrow$: If W is a subspace of V, then $\forall \vx, \vy \in W$ and $c \in \real, c\vx + \vy \in W$ holds since W itself is a real vector space. \newline
	$\leftarrow$: If $\forall \vx, \vy \in W$, and all $c \in \real$, we have $c\vx + \vy \in W$ \newline
	Can have $c = 1$, so $\vx + \vy \in W$ (close under addition) \newline
	$c = -1$ and $\vy = \vx$, so $-\vx + \vx = \vo \in W$ (additive identity) \newline
	$\vy = \vo$, so  $c\vx \in W$ (close under scalar multiplication) \newline
	These implies all the axioms. \qed
	\end{proof}

	\paragraph{Examples}
	\begin{enumerate}
		\item $W = \{f\in C(\real) | f(\pi) = 0\}$. W subspace of $C(\real)$? -Yes
		\item $W = \{f\in C(\real) | f(e) = e\}$. W subspace of $C(\real)$? -No, not close under addition
		\item $W = \{(x_1,...,x_n) | x_i\geq 0 \forall i\}$. W subspace of $C(\real)$? -No, there is no additive inverse for each item in W.
	\end{enumerate}
	\paragraph{Theorem 1.2.13} Let V be a vector space. Then the intersection of any collection of subspaces of V is a subspace of V. \newline
	\begin{proof}
		Consider any collection of subspace of V. Note that the intersection of the subspaces is not empty since at least the zero vector from V is in it. Now let $\vx, \vy$ be any two vectors in the intersection, so they are in every single subspace in the collection. Therefore $c\vx + \vy$ is also in every single subspace in the collection, so that it is in the intersection as well. Hence the intersection is a subspace of V. \qed
	\end{proof}
	\paragraph{Application} The set of all solutions of any simultaneous system of equations is a subspace of $\real^n$.
	\paragraph{Corollary 1.2.14} Let $a_{ij} (1 \leq i \leq m, 1\leq j \leq n)$ be any real numbers and let $W = \{(x_1,...,x_n) \in \real^n | a_{i1}x_1 + \hdots +a_{in}x_n = 0 \mbox{ for all } i, 1 \leq i \leq m\}$. Then W is a subspace of $\real^n$.
	
	
	\subsection{Linear Combinations}
	\paragraph{Definition 1.3.1} Let S be a subset of a vector space V.
	\begin{enumerate}
		\item A \ti{linear combination} of vectors in S is any sum $a_1\vx_1 + \hdots +  a_n \vx_n$ where the $a_i \in \real$, and the $\vx_i \in S$
		\item If $S \neq \emptyset$, the set of all linear combinations of vectors in S is called the \ti{span} of S, and denoted Span(S). \textcolor{red}{If $S = \emptyset$, we define Span(S) = \{\vo\}}. (Remark: It is a mathematician convention)
		\item If W = Span(S), we say S spans (or generates) W.
	\end{enumerate}
	\paragraph{Theorem 1.3.4} Let V be a vector space and let S be any subset of V. Then Span(S) is a subspace of V. \newline\newline
	\begin{proof}
		Span(S) is non-empty by definition. Let $\vx, \vy \in Span(S)$, then they are linear combinations of vectors in S. Check that $c\vx + \vy$ is also a linear combination of vectors in S, so $c\vx + \vy \in Span(S)$. Hence Span(S) is a subspace of V. \qed
	\end{proof}
	\paragraph{Definition} Let $W_1$ and $W_2$ be subspaces of a vector space V. The \ti{sum} of $W_1$ and $W_2$ is the set $$W_1 + W_2 = \{ \vx \in V | \vx = \vx_1 + \vx_2, \mbox{ for some } \vx_1 \in W_1 \mbox{ and } \vx_2 \in W_2\}$$
	\paragraph{Proposition 1.3.8 The basis of sum is the union of two bases} Let $W_1 = Span(S_1)$ and $W_2 = Span(S_2)$ be subspaces of a vector space V. Then $W_1 + W_2 = Span(S_1 \cup S_2)$
	\paragraph{Theorem 1.3.9} Let $W_1$ and $W_2$ be subspaces of a vector space V. Then $W_1 + W_2$ is also a subspace of V.
	\paragraph{\textcolor{red}{Proposition 1.3.11}} $W_1 + W_2$ is the smallest subspace containing $W_1 \cup W_2$: \newline
	Let $W_1$ and $W_2$ be subspaces of a vector space V and let W be a subspace of V such that $W_1 \cup W_2 \subseteq W$. Then $W_1 + W_2 \subseteq W$
	\paragraph{Remark} $W_1 \cup W_2$ is a subspace of V iff one is contained in another.
	
	
	
	\subsection{Linear Dependence and Linear Independence} 
	\paragraph{Definitions 1.4.2} Let V be a vector space, and let S be a subset of V.
	\begin{enumerate}
		\item A \ti{linear dependence} among the vectors of S is an equation
		$$a_1\vx_1 + \hdots + a_n\vx_n = \vo$$
		where the $x_i \in S$, and the $a_i \in \real$ are not all zero (i.e., at least one of the $a_i \neq \vo$
		\item the set S is said to be \ti{linearly dependent} if there exists a linear dependence among the vectors in S.
	\end{enumerate}
	
	\paragraph{Fact}
	Let S be a set. If $\vo \in S$, then S is dependent. 
	
	\paragraph{Definition 1.4.4} A subset S of a vector space V is \ti{linearly independent} if whenever we have $a_i \in \real$ and $\vx_i \in S$ such that $a_1\vx_1 + \hdots + a_n\vx_n = \vo$, then $a_i = 0$ for all $i$.
	
	\paragraph{Example}
	\textcolor{red}{In any vector space the empty subset $\emptyset$ is linearly independent.}
	\paragraph{Proposition 1.4.7}
	\begin{enumerate}
		\item Let S be a linearly independent subset of a vector space V, and let S' be another subset of V that \textcolor{blue}{contains} S. Then S' is also linearly dependent.
		\item Let S be linearly independent subset of a vector space V and let S' be another subset of V that is \textcolor{blue}{contained} in S. Then S' is also linearly independent.
	\end{enumerate}
	
	\subsection{Interlude on Solving Systems of Linear Equations (MAT223)}
	\subsection{Bases And Dimension (Jan 17)}
	\paragraph{Definition} A subset S of vector space V is called a \ti{basis} of V if V = Span(S) and S is linearly independent.
	\paragraph{Remark}
	\textcolor{red}{A basis is the maximal set of linearly independent vectors / minimal set of spanning vectors.}
	\paragraph{Examples} 
	\begin{enumerate}
		\item the standard basis S = \{\tb{$e_1$},...,\tb{$e_n$}\} in $\mb{R}^n$, since every vector $(a_1, ..., a_n) \in \mb{R}^n$ may be written as the linear combination $(a_1,..., a_n) = a_1e_1+ ... + a_ne_n$
		\item The vector space $\mb{R}^n$ has many other bases as well. e.g., in $\mb{R}^2$, consider the set $S = \{(1,2),(1,-1)\}$, which is l.i.
		\item Let $V = P_n(\mb{R})$ and consider $ S = \{1, x, x^2, ..., x^n\}$, which is a basis of V.
		\newline
		\proof It is clear that S spans V. For independence, consider
		$$a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n = \tb{0}$$
		Take the derivative of both sides,
		$$\frac{d^n}{dx^n}(a_0 + a_1x+a_2x^2+...+a_{n-1}x^{n-1}+a_nx^n) = \frac{d^n}{dx^n}(0)$$
		$$n!a_n = 0 \implies a_n = 0$$
		Similarly, we have $a_i = 0$ for all $i$, as wanted.
		
		\item The empty subset, $\emptyset$, is a basis of the vector space consisting only of a zero vector, $\{\tb{0}\}$.
	\end{enumerate}
	\paragraph{Theorem 1.6.3} Let V be a vector space, and let S be a nonempty subset of V. Then S is a basis of V iff every vector $\tb{x} \in V$ may be written \textcolor{red}{uniquely} as a linear combination of the vectors in S. \newline
	\under{\ti{Proof:}}
	$\rightarrow:$ Assume  S is a basis of V, then given $\tb{x} \in V$, there are scalars $a_i \in \mb{R}$ and vectors $x_i \in S$ s.t. $\tb{x} = a_1x_1 + ... + a_nx_n$. To show this linear combination is unique, consider a possible second linear combination of vectors in S which also adds up to \tb{x}: $x= b_1x_1 + ...+b_nx_n$. Subtracting these two expressions for $\tb{x}$, we find that
	$$\tb{0} = a_1x_1 + ... + a_nx_n - (b_1x_1 + ...+b_nx_n)$$ 
	$$=(a_1 - b_1)x_1 + ...+(a_n-b_n)x_n$$
	\under{Since S is linearly independent}, the equation implies that $a_i = b_i$ for all $i$.\newline\newline
	$\leftarrow$: Assume every vector $\tb{x} \in V$ may be written uniquely as a linear combination of the vectors in S. This implies $Span(S) = V$. We must show that S is l.i. Consider an equation
	$$a_1x_1 +...+a_nx_n = \tb{0}$$
	Note that it is also the case that
	$$0\tb{x} = 0(x_1 + ...+x_n) = \tb{0}$$
	Since we assumed that every \tb{x} has a unique representation in S, then it must be true that $a_i = 0$ for all $i$. Hence S is l.i.
 	\paragraph{Theorem 1.6.6} Let V be a vector space that has a finite spanning set, and let S be a linearly independent subset of V. Then there exists a basis S' of V, with $S \subset S'$
	\paragraph{Lemma 1.6.8} Let S be a linearly independent subset of V and let $x \in V$, but $x \notin S$. Then $S \cup \{\tb{x}\}$ is l.i. iff $\tb{x} \notin Span(S)$.
	\paragraph{Insight} the number of vectors in a basis is, in a rough sense, a measure of ``how big" the space is.
	\paragraph{Theorem 1.6.10 (Basis Theorem)} Let V be a vector space and let S be a spanning set for V, which has m elements. Then no linearly independent set in V can have more than $m$ elements.\newline \newline
	\ti{\under{proof:}} It suffices to show that every set in V with more than m elements is linearly dependent. Write $S = {y_1, ..., y_m}$ and suppose $S' = {x_1, ..., x_n}$ is a subset of V with $n >m$ vectors. Consider an equation
	$$(1)a_1x_1 + ... + a_nx_n = \tb{0}$$
	Our goal is to show that $a_i$ not all 0.
	Since S spans V, there are scalars $b_{ij}$ s.t. for each $i$,
	$$x_i = b_{i1}y_1 + ... +b_{im}y_m$$
	Substituting these equations into (1), we get $$a_1(b_{11}y_1+...+b_{1m}y_m)+...+a_n(b_{n1}y_1+...+b_{nm}y_m)=\tb{0}$$
	Collecting terms and rearranging,
	$$(a_1b_{11}+...+a_nb_{n1})y_1+...+(a_1b_{1m}+...+a_nb_{nm})y_m = \tb{0}$$
	Since S is l.i., this is equivalent to solving the system
	$$b_{11}a_1+...+b_{n1}a_n = 0$$
	$$.$$
	$$.$$
	$$.$$
	$$b_{1m}a_1+...+b_{nm}a_n=0$$
	But this is a system with n unknowns and m equations and $n>m$, so there must exist a non-trivial solution $\{a_1,...,a_n\}$, which is what we wanted to show. \qed
	\paragraph{Corollary 1.6.11} Let $V$ be a vector space and let $S$ and $S'$ be two bases of $V$, with $m$ and $m'$ elements, respectively. Then $m = m'$.\newline \newline
	\proof\newline
	Since S is a spanning set of V and S' is l.i., we have $m'\leq m$. Since S' is a spanning set of V and S is l.i.m we have $m \leq m'$. Hence $m = m'$. \qed
	\paragraph{Definitions 1.6.12}
	\begin{enumerate}
		\item If V is a vector space with some finite basis(\textcolor{red}{possibly empty}), we say V is \under{\it{finite-dimentional}}.
		\item Let V be a finite-dimensional vector space. The dimension of V, denoted dim(V), is the number of vectors in a (hence any) basis of V.
		\item If $V = \{\tb{0}\}$, we define dim(V) = 0.
	\end{enumerate}
	\paragraph{Examples}
	\begin{enumerate}
		\item For each n, $\dim(\mb{R}^n) = n$, since the standard basis contains n vectors.
		\item dim($P_n(\mb{R})) = n + 1$, since a basis for $P_n(\mb{R})$ contains n + 1 functions.
		\item The vector spaces $P(\mb{R}), C^1(\mb{R})$ and $C(\mb{R})$ are not finite-dimensional. We say that such spaces are \under{\it{infinite-dimentional}}.
		\item $\dim(Span\{(1,2,3),(4,5,6),(7,8,9)\}) = 2$
	\end{enumerate}
	\paragraph{\textcolor{blue}{Corollary 1.6.14}} Let W be a subspace of a finite-dimensional vector space V. Then $\dim(W) \leq \dim(V).$ Furthermore, $	\dim(W) = \dim(V)$ iff $W = V$.
	
	\paragraph{Corollary 1.6.15} Let W be a subspace of $\mb{R}^n$ defined by a system of homogeneous linear equations. Then dim(W) is equal to the number of free variables in the corresponding echelon form system.
	\paragraph{Theorem 1.6.18} Let $W_1$ and $W_2$ be finite-dimensional subspaces of a vector space V. Then $$\dim(W_1 + W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1 \cap W_2)$$
	\paragraph{Remark} Analogous to the Principle of Inclusion-Exclusion \newline \newline
	\proof Result obvious if either $W_1$ or $W_2$ is \{\tb{0}\}. \newline
	Therefore, we assume that neither $W_1$ nor $W_2$ is  \{\tb{0}\}. Starting from a basis S of $W_1 \cap W_2$. We can always find sets $T_1$ and $T_2$ (disjoint from S) such that $S \cup T_1$ is a basis for $W_1$ and $S \cup T_2$ is a basis for $W_2$. We claim that $U = S \cup T_1 \cup T_2$ is a basis for $W_1 + W_2$, since 
	$$ U = S \cup T_1 \cup T_2 = (S \cup T_1) \cup (S\cup T_2)$$
	$$Span(U) = Span((S \cup T_1) \cup (S\cup T_2)) = W_1 + W_2$$
	Next, prove that U is linearly independent. Any potential linear dependence among the vectors in U must have the form 
	$$\tb{v} + \tb{w}_1 + \tb{w}_2 = \tb{0}$$
	where $\tb{v} \in Span(S) = W_1 \cap W_2, \tb{w}_1 \in Span(T_1) \subset W_1, \tb{w}_2 \in Span(T_2) \subset W_2$. (slice the linear combination into the sum of the vectors from 3 vector spaces). It suffices to prove that in any such potential linear dependence, we must have $\tb{v} = \tb{w}_1 = \tb{w}_2 = \tb{0}$ (each vector is a lin comb, and equals \tb{0}).  \newline
	Consider $\tb{w}_2 = -\tb{v} - \tb{w}_1$. Since $-\tb{v} - \tb{w}_1 \in W_1, \tb{w}_2 \in W_2,$ we must have $\tb{w}_2 \in W_1 \cap W_2$. By definition, $\tb{w}_2 \in Span(T_2)$ But $S \cap T_2 = \emptyset$, hence $Span(S) \cap Span(T_2) = \{\tb{0}\}.$ Therefore we must have $\tb{w}_2 = \tb{0}$. So then $-\tb{v} = \tb{w}_1 \in W_1 \cap W_2$. Since $S \cap T_1 = \emptyset$, $Span(S) \cap Span(T_1) = \{\tb{0}\}$ and we have $\tb{w}_1 = \tb{0}$, so $\tb{v} = \tb{0}$ as well. So U is independent. \newline
	\begin{align*}
		|U| &= |S| + |T_1| + |T_2|\\
		&=\dim{W_1\cap W_2} + (\dim{W_1}-\dim{W_1\cap W_2}) + (\dim{W_2} - \dim{W_1\cap W_2})\\
		&= \dim(W_1) + \dim(W_2) - \dim(W_1 \cap W_2)\\
	\end{align*}
	\paragraph{Exercises for 1.4}
	1.(k), 7
	\paragraph{Exercises for 1.6}
	1.(d)(e)(f), 3, 4, 16
	\newpage
	\section{Linear Transformations}
	\subsection{Linear Tranformations}
	A function T from V to W is denoted by $T: V \rightarrow W$. The vector $\tb{w} = T(\tb{v})$ in W is called the \ti{image} of \tb{v} under the function T. Loosely speaking, we want our functions to turn the algebraic operations of addition and scalar multiplication in V into addition and scalar multiplication in W.
	\paragraph{Definition 2.1.1} A function $T: V \rightarrow W$ is called a \ti{linear mapping} or a \ti{linear transformation} if it satisfies
	\begin{enumerate}
		\item $T(\tb{u} + \tb{v}) = T(\tb{u}) + T(\tb{v})$ for all \tb{u} and \tb{v} $\in$ V
		\item $T(a\tb{v}) = aT(\tb{v})$ for all $a \in \mb{R}$ and $\tb{v} \in V$
	\end{enumerate}
	V is called the \ti{domain} of T and W is called the \ti{target} of T. \newline
	We say that a linear transformation preserves the operations of addition and scalar multiplication.
	\paragraph{Property} \textcolor{red}{A linear mapping always takes the zero vector in the domain vector space to the zero vector in the target vector space.}
	\paragraph{Proposition 2.1.2} A function \litran is a linear transformation if and only if for all $a$ and $b \in \mb{R}$ and all $\tb{u}$ and $\tb{v} \in V$
	$$T(a\tb{u} + b\tb{v}) = aT(\tb{u}) + bT(\tb{v})$$
	\paragraph{Corollary 2.1.3} A function \litran is a linear transformation if and only if for all $a_1, .., a_k \in \mb{R}$ and for all $\tb{v}_1, ..., \tb{v}_k \in V$:
	$$T(\sum^k_{i = 1} a_i \tb{v}_i) = \sum^k_{i = 1}a_iT(\tb{v}_i)$$
	
	\paragraph{Remark} prove this by induction!
	\paragraph{Examples}
	\begin{enumerate}
		\item Let V be any vector space, and let W = V. The \ti{\under{identity transformation}} $I: V \rightarrow V$ is defined by I(\tb{v}) = \tb{v} for all \tb{v} $\in$ V.
		\item Let V and W be any vector spaces, and let \litran be the mapping that takes every vector in V to the zero vector in W:
		$$ T(\tb{v}) = \tb{0}_W$$
		for all \tb{v} $\in$ V. T is called \it{\under{zero transformation}}.
		\item $T(\vx) = a_1x_1 + ... + a_nx_n$
		\item Differentiation, definite integration
	\end{enumerate} 
	\paragraph{Remark} The inner product plays a crucial role in linear algebra in that it provides a bridge between algebra and geometry, which is the heart of the more advanced material that appears later in the text.
	\paragraph{Proposition 2.1.14} If \litran is a linear transformation and V is finite-dimensional, then T is uniquely determined by its values on the members of a basis of V. \newline
	\proof Show that if S and T are linear transformations that take the same values on each member of a basis for V, then in fact S = T. \newline
	\begin{align*}
		T(v) &= T(a_1v_1 + ... + a_kv_k)\\
		&= a_1T(v_1) + ... + a_kT(v_k)\\
		&= a_1S(v_1) + ... + a_kS(v_k)\\
		&= S(a_1v_1 + ... + a_kv_k)\\
		&= S(v)
	\end{align*}
	Therefore, S and T are equal as mappings from V to W. \qed
	\subsection{Linear Transformations Between Finite-Dimensional Vector Spaces}
	\paragraph{Proposition 2.2.1} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W. If $\{\tb{v}_1,...,\tb{v}_k\}$ is a basis for V and $\{\tb{w}_1,...,\tb{w}_l\}$ is a basis for W, then \litran is uniquely determined by the $l\cdot k$ scalars used to express $T(\tb{v}_j), j = 1,...,k$, in terms of $\tb{w}_1, ..., \tb{w}_l$.
	\paragraph{Definition 2.2.6} Let \litran be a linear transformation between the finite-dimensional vector spaces V and W, and let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$, respectively, be any bases for V and W. Let $a_{ij}, 1 \leq i \leq l$ and $1 \leq j\leq k$ be the $l \cdot k$ scalars that determine T with respect to the bases $\alpha$ and $\beta$. The matrix whose entries are the scalars $a_{ij}$, $1 \leq i \leq l$ and $1 \leq j\leq k$, is called the {\ti{matrix of the linear transformation T with respect to the bases $\alpha$ for V and $\beta$ for W}. This matrix is denoted by $[T]^\beta_\alpha$.
	\paragraph{Remark} The basis vectors in the domain and target spaces are written in some particular order.
	\paragraph{Definition of coordinate vectors} If $\tb{v} = a_1\tb{v}_1+...+a_k\tb{v}_k$ and $\tb{w} = b_1\tb{w}_1+...+b_l\tb{w}_l$, we can express \tb{v} and \tb{w} in coordinates, respectively, as a $k \times 1$ matrix and as an $l \times 1$ matrix, with respect to the chosen bases. These coordinate vectors will be denoted by $[\tb{v}]_\alpha$ and $[\tb{w}]_\beta$, respectively. Thus
	$[\tb{v}]_\alpha = \begin{bmatrix} 
a_1 \\
\vdots\\
a_k
\end{bmatrix}$
and
	$[\tb{w}]_\beta = \begin{bmatrix} 
b_1 \\
\vdots\\
b_l
\end{bmatrix}$
\paragraph{Proposition 2.2.15} Let \litran be a linear transformation between vector spaces V of dimension k and W of dimension $l$. Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W. Then for each $\tb{v} \in V$, $$[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$$
\proof Let $\tb{v} = x_1\tb{v}_1 + ...+x_k\tb{v}_k \in V$. Then if $T(\tb{v}_j) = a_{1j}\tb{w}_1 + ...+ a_{lj}\tb{w}_l$
\begin{align*}
	T(\tb{v}) &= \sum_{j=1}^kx_jT(\tb{v}_j)\\
	&=\sum_{j=1}^kx_j(\sum_{i=1}^la_{ij}\tb{w}_i)\\
	&=\sum_{i=1}^l(\sum_{j=1}^kx_ja_{ij})\tb{w}_i
\end{align*}
Thus, the $i$th coefficient of T(\tb{v}) in terms of $\beta$ is $\sum_{j=1}^kx_ja_{ij}$ and 
$[T(\tb{v})]_\beta = \begin{bmatrix}
	\sum_{j=1}^kx_ja_{1j}\\
	\vdots\\
	\sum_{j=1}^kx_ja_{lj}
\end{bmatrix}$ which is precisely $[T(\tb{v})]_\beta = [T]^\beta_\alpha[\tb{v}]_\alpha$. \qed

\paragraph{Proposition 2.2.19} 
 Let $\alpha = \{\tb{v}_1,...,\tb{v}_k\}$ be a basis for V and $\beta = \{\tb{w}_1,...,\tb{w}_l\}$ be a basis for W, and let $\tb{v} = x_1\tb{v}_1 +...+x_k\tb{v}_k \in V$
 \begin{enumerate}
 	\item If A is an $l \times k$ matrix, then the function $$T(\tb{v}) = \tb{w}$$
 	where $[\tb{w}]_\beta = A[\tb{v}]_\alpha$ is a linear transformation.
 	\item If $A = [S]_\alpha^\beta$ is the matrix of a transformation $S: V \rightarrow W$, then the transformation T constructed from $[S]_\alpha^\beta$ is equal to S.
 	\item If T is the transformation of (1) constructed from A, then $[T]_\alpha^\beta = A$
 \end{enumerate}
 
 \paragraph{Proposition 2.2.20} Let V and W be finite-dimensional vector spaces. Let $\alpha$ be a basis for V and $\beta$ a basis for W. Then the assignment of a matrix to a linear transformation from V to W given by T goes to $[T]_\alpha^\beta$ is injective and surjective.
 \paragraph{Notes}
 \begin{enumerate}
 	\item When proving a function T is not a linear transformation, can consider $T(\tb{0}) \neq \tb{0}$.
 \end{enumerate}
 \subsection{Kernel and Image}
 \paragraph{Definition 2.3.1} The $kernel$ of T, denoted $Ker(T)$, is the subset of V consisting of all vectors $\tb{v} \in V$ such that $T(\tb{v}) = 0$.
 \paragraph{Remark} Kernel is different from null spaces. A null space is about a matrix, and it is something in $\real ^n$.
 \paragraph{Proposition 2.3.2} Let \litran be a linear transformation. Ker(T) is a subspace of V.
 \paragraph{Examples}
 \begin{enumerate}
 	\item Let $V = P_3(\mb{R})$. Define $T: V \rightarrow V$ by $T(p(x)) = \frac{d}{dx}p(x)$. Ker(T) only consists constant polynomials.
 	\item Let $V = W = \mb{R}^2$. Let T be a rotation $R_\theta$. Then $Ker(T) = \{\tb{0}\}$.
 \end{enumerate}
 \paragraph{Proposition 2.3.7} \slitran of finite-dimensional vector spaces, and let $\alpha$ and $\beta$ be bases for V and W, respectively. Then $\tb{x} \in Ker(T)$ ife lf the coordinate vector of \tb{x}, $[\tb{x}]_\alpha$, satisfies the system of equations
 $$ a_{11}x_1+...+a_{1k}x_k = 0$$
 $$\vdots$$
 $$ a_{l1}x_1+...+a_{lk}x_k = 0$$
 where the coefficients $a_{ij}$ are the entries of the matrix \mt.
\paragraph{Remark} This says $$x \in \ker(T) \iff [x]_\alpha \in Nul[T]_\alpha^\beta$$
\paragraph{Proposition 2.3.8 Independence is Basis-Independent} Let V be a finite-dimensional vector space, and let $\alpha = \{\tb{v}_1, ..., \tb{v}_k \}$ be a basis for V. Then the vectors $\tb{x}_1,...,\tb{x}_m \in V$ are linearly independent iff their corresponding coordinate vectors $[\tb{x}_1]_\alpha, ..., [\tb{x}_m]_\alpha$ are linearly independent.
\paragraph{Definition 2.3.10} The subset of W consisting of all vectors $\tb{w} \in W$ for which there exists a $\tb{v} \in V$ such that $T(\tb{v}) = \tb{w}$ is called the $image$ of T and is denoted by $Im(T).$

\paragraph{Proposition 2.3.11} Let \litran be a linear transformation. The image of T is a subspace of W.

\paragraph{Proposition 2.3.12} If $\{ \tb{v}_1, ..., \tb{v}_m\}$is any set that spans V (in particular, it could be a basis of V), then $\{T(\tb{v}_1),...,T(\tb{v}_m)\}$ spans $Im(T)$.
\paragraph{Corollary 2.3.13} If $\alpha = \{ \tb{v}_1, ..., \tb{v}_k\}$ is a basis for V and $\beta = \{ \tb{w}_1, ..., \tb{w}_l\}$ is a basis for W, then the vectors in W, whose coordinate vectors (in terms of $\beta$) are the columns of $[T]_\alpha^\beta$, span $Im(T)$.
\paragraph{Rank-Nullity Theorem 2.3.17} If V is finite-dimensional vector space and \litran is a linear transformation, then
$$\dim(Ker(T)) + \dim(Im(T)) = \dim(V)$$
Equivalently, 
$$Nullity(T) + Rank(T) = \dim(V)$$


\subsection{Applications of the Dimension Theorem}
\paragraph{Proposition 2.4.2} A linear transformation \litran is injective iff $\dim(Ker(T)) = 0$, or $\dim(Im(T)) = \dim(V)$.
\paragraph{Remark} Analogously, in MAT223 we said that \textcolor{red}{a matrix is one-to-one if all the columns are l.i.}.
\paragraph{Corollary 2.4.3} A linear mapping \litran on a finite-dimensional vector space V is injective iff $\dim(Im(T)) = \dim(V)$.
\paragraph{Corollary 2.4.4} If $\dim(W) < \dim(V)$ and \litran is a linear mapping, then T is not injective. \newline
\begin{proof}
	$$\dim(Im(T)) \leq \dim(W) < \dim(V)$$
	$$\implies dim(Ker(T)) > 0$$
\end{proof}
\paragraph{Proposition 2.4.7} If W is finite-dimensional, then a linear mapping \litran is surjective iff $\dim(Im(T)) = \dim(W)$
\paragraph{Remark} Analogously, in MAT223 we said that \textcolor{red}{a matrix $A \in M_{m\times n}(\real)$ is onto if there is a pivot in every row, or the columns of A spans $\real^m$}.
\paragraph{Corollary 2.4.8} If V and W are finite-dimensional, with $\dim(V) < \dim(W)$, then there is no surjective linear mapping \litran \newline
\begin{proof}
	$\dim(Im(T)) \leq \dim(V) < \dim(W) \implies T$ is not surjective
\end{proof}
\paragraph{Corollary 2.4.9} A linear mapping \litran can be surjective iff $$\dim(V) \geq \dim(W)$$
\paragraph{Proposition 2.4.10} Let $\dim(V) = \dim(W)$. A linear transformation \litran is injective iff it is surjective.
\paragraph{Proposition 2.4.11} Let \litran be a linear transformation, and let $w \in Im(T)$. Let $v_1$ be any fixed vector with $T(v_1) = w$. Then every vector $v_2 \in T^{-1}(\{w\})$ can be written uniquely as $v_2 = v_1 + u$, where $u \in Ker(T)$
\paragraph{Remark} In this situation $T^{-1}(\{w\})$ is a subspace of V iff $w = 0$.
\paragraph{Corollary 2.4.15} Let \litran be a linear transformation of finite-dimensional vector spaces, and let $w \in W$. Then there is a unique vector $v \in V$ such that $T(v) = w$ iff
\begin{enumerate}
	\item $w \in Im(T)$, and
	\item dim(Ker(T)) = 0
\end{enumerate}
\paragraph{Proposition 2.4.16} With notation as before
\begin{enumerate}
	\item The set of solutions of the system of linear equations $A\tb{x} = \tb{b}$ is the subset $T^{-1}(\{\tb{b}\})$ of $V = \real^n$
	\item The set of solutions of the system of linear equations $A\tb{x} = \tb{b}$ is a subspace of V iff the system is homogeneous, in which case the set of solutions is $Ker(T)$.
\end{enumerate}
\paragraph{Corollary 2.4.17}
\begin{enumerate}
	\item The number of free variables in the homogeneous system $A\tb{x} = \tb{0}$ (or its echelon form equivalent) is equal to $\dim(Ker(T))$
	\item The number of basic variables of the system is equal to $\dim(Im(T))$
\end{enumerate}
\paragraph{Definition 2.4.18} Given an inhomogeneous system of equations, $A\tb{x} = \tb{b}$, any single vector $\tb{x}$ satisfying the system (necessarily $\tb{x} \neq \tb{0}$) is called a \under{particular solution} of the system of equations.
\paragraph{Proposition 2.4.19} Let $\tb{x}_p$ be a particular solution of the system $A\tb{x} = \tb{b}$. Then every other solution to $A\tb{x} = \tb{b}$ is of the form $\tb{x} = \tb{x}_p + \tb{x}_h$, where $\tb{x}_h$ is a solution of the corresponding homogeneous system of equations $A\tb{x} = \tb{0}$. Furthermore, given $\tb{x}$ and $\tb{x}_p$, there is a unique
	$\tb{x}_h$ such that $\tb{x} = \tb{x}_p + \tb{x}_h$.
\paragraph{Corollary 2.4.20} The system $A\tb{x} = \tb{b}$ has a unique solution iff $\tb{b} \in Im(T)$ and the only solution to $A\tb{x} = \tb{0}$ is the zero vector.
\subsection{Composition of Linear Transformations}
\paragraph{Definition} Let U, V, and W be vector spaces, and let $S: U \rightarrow V$ and $T: V \rightarrow W$ be linear transformations. The \ti{composition} of S and T is denoted $TS: U \rightarrow W$ and is defined by $$TS(\tb{v}) = T(S(\tb{v}))$$
Notice that this is well defined since the image of S is contained in V, which is the domain of T.
\paragraph{Proposition 2.5.1} Let $S: U \rightarrow V$ and $T: V \rightarrow W$ be linear transformations, then TS is a linear transformation.
\paragraph{Remark} In general, ST is not equal to TS. We emphasize that the composition is well defined only if the image of the first transformation is contained in the domain of the second.
\paragraph{Proposition 2.5.4}
\begin{enumerate}
	\item Let $\trans{R}{U}{V}, \trans{S}{V}{W} \mbox{ and } \trans{T}{W}{X}$ be linear transformations of the vector space U,V,W and X as indicated. Then
	$$T(SR) = (TS)R \mbox{ (associativity)}$$
	\item Let $\trans{R}{U}{V}, \trans{S}{V}{W} \mbox{ and } \trans{T}{W}{X}$ be linear transformations of the vector space U,V,W and X as indicated. Then
	$$T(R+S) = TR + TS \mbox{ (distributivity)}$$
	\item Let $\trans{R}{U}{V}, \trans{S}{V}{W} \mbox{ and } \trans{T}{W}{X}$ be linear transformations of the vector space U,V,W and X as indicated. Then
	$$(T+S)R = TR + SR \mbox{ (distributivity)}$$
\end{enumerate}
\paragraph{Proposition 2.5.6} Let $\trans{S}{U}{V}$ and $\trans{T}{V}{W}$ be linear transformations. Then
\begin{enumerate}
	\item $Ker(S) \subset Ker(TS)$
	\item $Im(TS) \subset Im(T)$
\end{enumerate}
\begin{proof}
	\begin{enumerate}
		\item If $\vu \in Ker(S), S(\vu) = \vo$. Then $TS(\vu) = T(\vo) = \vo$. Therefore $\vu \in Ker(TS)$.
		\item If $\vx \in Im(TS)$, then $\exists \vu \in U$ s.t. $TS(\vu) = T(S(\vu))= \vx$, then $\exists \tb{v} = S(\vu) \in V$ s.t. $T(\tb{v}) = \vx$. Therefore $\vx \in Im(T)$ \qed
	\end{enumerate}
\end{proof}
\paragraph{Corollary 2.5.7} Let $\trans{S}{U}{V}$ and $\trans{T}{V}{W}$ be linear transformations of finite-dimensional vector spaces. Then
\begin{enumerate}
	\item $\dim(Ker(S)) \leq \dim(Ker(TS))$
	\item $\dim(Im(TS)) \leq \dim(Im(T))$
\end{enumerate}
\paragraph{Proposition 2.5.9} If $[S]_\alpha^\beta$ has entries $a_{ij}$, $i = 1,\hdots,n$ and $ j = 1, \hdots, m$ and $[T]_\beta^\gamma$ has entries $b_{kl}$, $k = 1, \hdots, p$ and $l = 1, \hdots, n$, then the entries of $[TS]_\alpha^\gamma$  are $\sum_{l=1}^n b_{kl}a_{lj}$
\paragraph{Definition 2.5.10} Let A be an $n\times m$ matrix and B a $p\times n$ matrix, then the \ti{matrix product} BA is defined to be the $p \times m$ matrix whose entries are $\sum_{l = 1}^n b_{kl}a_{lj}$ for $k = 1, \hdots, p$ and $j = 1, \hdots, m$.
\paragraph{Proposition 2.5.13} Let $\trans{S}{U}{V}$ and $\trans{T}{V}{W}$ be linear transformations between finite-dimensional vector spaces. Let $\alpha, \beta$ and $\gamma$ be bases for U,V and W, respectively. Then $$[TS]_\alpha^\gamma = [T]_\beta^\gamma[S]_\alpha^\beta$$
In words, the matrix of the composition of two linear transformations is the product of the matrices of the transformations
\paragraph{Proposition 2.5.14}
\begin{enumerate}
	\item Let A,B and C be $m\times n, n\times p$ and $p \times r$ matrices, then
	$$(AB)C = A(BC) \mbox{ (associativity) }$$
	\item Let A,B and C be $m\times n, n\times p$ and $p \times r$ matrices, then
	$$A(B + C) = AB + AC \mbox{ (distributivity) }$$
	\item Let A,B and C be $m\times n, n\times p$ and $p \times r$ matrices, then
	$$(A+B)C = AC + BC \mbox{ (distributivity) }$$
\end{enumerate}
\subsection{The Inverse of a Linear Transformation}
\paragraph{Definition} If $f: S_1 \rightarrow S_2$ is a function from one set to another, we say that $g$ is the \ti{inverse function of f} if for every $x \in S_1, g(f(x)) = x$ and for every $y \in S_2, f(g(y)) = y$. \textcolor{red}{If such a $g$ exists, $f$ must be both injective and surjective(bijective).} \\
To see this, notice that if $f(x_1) = f(x_2)$, then $$x_1 = g(f(x_1)) = g(f(x_2)) = x_2$$ So that $f$ is injective. If $y \in S_2$, then for $x = g(y), f(x) = f(g(y)) = y$ so that $f$ is surjective.\\
Converse is true: bijective $\implies$ exists an inverse
\paragraph{Proposition 2.6.1} If \litran is injective and surjective, then the inverse function $S: W \rightarrow V$ is a linear transformation.\\\\
\begin{proof}
	Let $\vw_1$ and $\vw_2 \in W$ and $a$ and $b \in \real$. By definition, $S(\vw_1) = \vv_1$ and $S(\vw_2) = \vv_2$ are the unique vectors $\vv_1$ and $\vv_2$ satisfying $T(\vv_1) = \vw_1$ and $T(\vv_2) = \vw_2$. By definition, $S(a\vw_1 + b\vw_2)$ is the unique vector $\vv$ with $T(\vv) = a\vw_1 + b\vw_2$ but $\vv = a\vv_1+b\vv_2$ satisfies $T(a\vv_1+b\vv_2) = aT(\vv_1) + bT(\vv_2) = a\vw_1+b\vw_2$. \under{Thus, $S(a\vw_1 + b\vw_2) = a\vv_1 + b\vv_2 = aS(\vw_1) + bS(\vw_2)$} as we desired. \qed 
\end{proof}
\paragraph{Proposition 2.6.2} A linear transformation \litran has an inverse linear transformation S if and only if T is injective and surjective.
\paragraph{Definition 2.6.3} If \litran is a linear transformation that has an inverse transformation $S: W \rightarrow V$, we say that T is \under{invertible}, and we denote the inverse of $T$ by $T^{-1}$.
\paragraph{Definition 2.6.4} If \litran is an invertible transformation, $T$ is called an \under{isomorphism}, and we say $V$ and $W$ are \under{isomorphic vector spaces}.

\paragraph{Notes}
$T^{-1}T(\vv)$ is the identity linear transformation of V, $T^{-1}T = I_V$, and $TT^{-1}$ is the identity linear transformation of W, $TT^{-1} = I_W$. \textcolor{blue}{If $S$ is a linear transformation that is a candidate for the inverse, we need only verify that $ST = I_V$ and $TS = I_W$.}
\paragraph{Proposition 2.6.7} If V and W are finite-dimensional vector spaces, then there is an isomorphism \litran if and only if $\dim(V) = \dim(W)$.
\paragraph{Definition 2.6.10} An $n \times n$ matrix A is called \ti{invertible} if there exists an $n\times n$ matrix B so that $AB = BA = I$. $B$ is called the \ti{inverse} of A and is denoted by $A^{-1}$.
\paragraph{Proposition 2.6.11} Let \litran be an isomorphism of finite-dimensional vector spaces. Then for any choice of bases $\alpha$ for $V$ and $\beta$ for $W$
$$[T^{-1}]_\beta^\alpha = {[T]_\alpha^{\beta}}^{-1}$$

\subsection{Change of Basis}
\paragraph{Proposition 2.7.3} Let V be a finite-dimensional vector space, and let $\alpha$ and $\alpha'$ be bases for V. Let $\vv \in V$. Then the coordinate vector $[\vv]_{\alpha'}$ of $\vv$ in the basis $\alpha'$ is related to the coordinate vector $[\vv]_\alpha$ of $\vv$ in the basis $\alpha$ by $$[I]_\alpha^{\alpha'}[\vv]_\alpha = [\vv]_{\alpha'}$$
\paragraph{Theorem 2.7.5} Let \litran be a linear transformation between finite-dimensional vector spaces V and W. Let $I_V: V \rightarrow V$ and $I_W: W \rightarrow W$ be the respective identity transformations of V and W. Let $\alpha$ and $\alpha'$ be two bases for V, and let $\beta$ and $\beta'$ be two bases for $W$. Then
$$[T]_{\alpha'}^{\beta'} = [I_W]_\beta^{\beta'}\cdot[T]_\alpha^\beta\cdot[I_V]_{\alpha'}^{\alpha}$$
\paragraph{Definition 2.7.6} Let A,B be $n \times n$ matrices. A and B are said to be \ti{similar} if there is an invertible $n\times n$ matrix Q such that
$$B = Q^{-1}AQ$$

\section{The Determinant Function}
\subsection{The Determinant as Area}
\paragraph{Corollary 3.1.2} Let $V = \real^2$. $T: V \rightarrow V$ is an isomorphism if and only if the area of the parallelogram constructed previously is nonzero.
\paragraph{Proposition 3.1.3} The function $Area(\tb{a}_1,\tb{a}_2)$ has the following properties for $\tb{a}_1, \tb{a}_2,, \tb{a}'_1$, and $\tb{a}'_2 \in \real^2$
\begin{enumerate}
	\item $Area(b\va_1 + c\va'_1, \va_2) = b Area(\va_1, \va_2) + c Area(\va'_1, \va_2)$ for $b, c\in \real$
	\item $Area(\va_1, b\va_2 + c\va'_2) = b Area(\va_1, \va_2) + c Area(\va_1, \va'_2)$ for $b, c\in \real$
	\item $Area(\va_1, \va_2) = -Area(\va_2, \va_1)$
	\item $Area((1,0),(0,1)) = 1$
\end{enumerate}
\paragraph{Proposition 3.1.4} If $B(\va_1,\va_2)$ is any real-valued function of $\va_1$ and $\va_2 \in \real^2$ that satisfies Properties (i),(ii),(iii) of Proposition (3.1.3), then B is equal to the area function.
\paragraph{Definition 3.1.5}
The determinant of a $2 \times 2$ matrix A, denoted by $\det(A)$ or $\det(\va_1,\va_2)$, is the unique function of the rows of A satisfying
\begin{enumerate}
	\item $\det(\va_1, b\va_2 + c\va'_2) = b \det(\va_1, \va_2) + c \det(\va_1, \va'_2)$ for $b, c\in \real$
	\item $\det(\va_1, \va_2) = -\det(\va_2, \va_1)$
	\item $\det(\tb{e}_1, \tb{e}_2) = 1$
\end{enumerate}
As a consequence of (3.1.4), $\det(A)$ is given explicitly by\\
$$\det(A) = a_{11}a_{22} - a_{12}a_{21}$$
We can rephrase the work of this section as follows
\paragraph{Proposition 3.1.6}
\begin{enumerate}
	\item A $2\times 2$ matrix A is invertible if and only if $\det(A) \neq 0$
	\item If $T: V \rightarrow V$ is a linear transformation of a two-dimensional vector space V, then T is an isomorphism if and only if $\det([T]_\alpha ^\alpha) \neq 0$
\end{enumerate}
\subsection{The Determinant of an $n\times n$ Matrix}
\paragraph{Definition 3.2.1} A function $f$ of \textcolor{red}{the rows of a matrix A} is called \under{multilinear} if $f$ is a linear function of each of its rows when the remaining rows are held fixed. That is, $f$ is multilinear if for all $b$ and $b' \in \real$,
	$$f(\va_1,\hdots,b\va_l+b'\va'_l,\hdots,\va_n) = bf(\va_1,\hdots,\va_l,\hdots,\va_n)+ b'f(\va_1,\hdots,\va'_l,\hdots,\va_n)$$
\paragraph{Definition 3.2.2} A function $f$ of the rows of a matrix A is said to be \under{alternating} if whenever any two rows of A are interchanged $f$ changes sign. That is, for all$i \neq j, 1\leq i, j\leq n$, we have
$$f(\va_1,\hdots,\va_l,\hdots,\va_j,\hdots,\va_n) = -f(\va_1,\hdots,\va_j,\hdots,\va_i,\hdots,\va_n)$$
\paragraph{Lemma 3.2.3} If $f$ is an alternating real-valued function of the rows of an $n\times n$ matrix and two rows of the matrix A are identical, then $f(A) = 0$
\paragraph{Definition 3.2.4} Let A be an $n\times n$ matrix with entries $a_{ij}, i,j = 1,\hdots,n$. The $ij$th \under{minor} of A is defined to be the $(n-1)\times(n-1)$ matrix obtained by deleting the $i$th row and $j$th column of A. The $ij$th minor is denoted by $A_{ij}$.
\paragraph{Proposition 3.2.5} Let A be a $3 \times 3$ matrix, and let $f$ be an alternating multilinear function. Then
$$f(A) = [a_{11}\det(A_{11}) - a_{12}\det(A_{12}) + a_{13}\det(A_{13})]f(I)$$
\paragraph{Corollary 3.2.6} There exists exactly one multilinear alternating function $f$ of the rows of a $3 \times 3$ matrix such that $f(I) = 1$
\paragraph{Definition 3.2.7} The determinant function of a $3 \times 3$ matrix is the unique alternating multilinear function $f$ with $f(I) = 1$. This function will be denoted by $\det(A)$.
\paragraph{Theorem 3.2.8} There exists exactly one alternating multilinear function $f: M_{n\times n}(\real) \rightarrow \real$ satisfying $f(I) = 1$, which is called the determinant function $f(A) = \det(A)$. Further, any alternating multilinear function $f$ satisfies $f(A) = \det(A)f(I)$
\paragraph{Proposition 3.2.10} If an $n \times n$ matrix A is not invertible, then $\det(A) = 0$.
\paragraph{Proposition 3.2.11} $$\det(\va_1,\hdots,\va_n) = \det(\va_1,\hdots,\va_i + b\va_j, \hdots, \va_n)$$
\paragraph{Lemma 3.2.12} If A is an $n \times n$ diagonal matrix, then $\det(A) = a_{11}a_{22}\hdots a_{nn}$
\paragraph{Proposition 3.2.13} If A is invertible, then $\det(A) \neq 0$
\paragraph{Theorem 3.2.14} Let A be an $n\times n$ matrix. A is invertible if and only if $\det(A) \neq 0$
\subsection{Further Properties of the Determinant}
Let $A'$ be the matrix whose entries $a'_{ij}$ are the scalars $(-1)^{i+j}\det(A_{ji})$. The quantity $a'_{ij}$ is called the $ji$th \under{cofactor} of A.
\paragraph{Proposition 3.3.1} $$AA' = det(A)I$$
\paragraph{Corollary 3.3.2} If A is an invertible $n \times n$ matrix, then $A^{-1}$ is the matrix whose $ij$th entry is $(-1)^{i+j}\det(A_{ji})/\det(A)$
\paragraph{Proposition 3.3.4} For any fixed j, $1 \leq j \leq n$,
$$\det(A) = \sum_{i=1}^n(-1)^{i+j}a_{ij}\det(A_{ij})$$
\paragraph{Remark 3.3.5} In general, if $\vb$ is a vector in $\real^n$, $A'\vb$ is a vector whose $i$th entry is $\sum_{j=1}^n a'_{ij}b_j=\sum_{j=1}^nb_j(-1)^{i+j}\det(A_{ji})$. This is the determinant of the matrix whose columns are $\va_1,\hdots,\va_{i-1},\vb,\va_{i+1},\hdots,\va_n$, where $\va_j, 1\leq j\leq n$, is the $j$th column of A. The determinant is expanded along the $i$th column. This fact will be used in the discussion of Cramer's rule, which appears later in this section.
\paragraph{Proposition 3.3.7}If A and B are $n \times n$ matrices, then
\begin{enumerate}
	\item $\det(AB) = \det(A)\det(B)$
	\item If A is invertible, then $\det(A^{-1}) = 1/\det(A)$
\end{enumerate}
\paragraph{Corollary 3.3.8} If $T: V \rightarrow V$ is a linear transformation, $\dim(V) = n$, then $$\det([T]_\alpha^\alpha) = \det([T]_\beta^\beta)$$ for all choices of bases $\alpha$ and $\beta$ for V.
\paragraph{Definition 3.3.9} The \under{determinant} of a linear transformation $T: V \rightarrow V$ of a finite-dimensional vector space is the determinant of $[T]_\alpha^\alpha$ for any choice of $\alpha$. We denote this by $\det(T)$.
\paragraph{Proposition 3.3.11} A linear transformation $T: V \rightarrow V$ of a finite-dimensional vector space is an isomorphism if and only if $\det(T) \neq 0$
\paragraph{Proposition 3.3.12} Let $S: V \rightarrow V$ and $T: V \rightarrow V$ be linear transformations of a finite-dimensional vector space, then
\begin{enumerate}
	\item $\det(ST) = \det(S)\det(T)$ and 
	\item if T is an isomorphism, $\det(T^{-1}) = \det(T)^{-1}$
\end{enumerate}
\paragraph{Proposition 3.3.13 (Cramer's rule)} Let A be an invertible $n\times n$ matrix. The solution $\vx$ to the system of equations $A\vx = \vb$ is the vector whose $j$th entry is the quotient $$\det(B_j)/\det(A)$$
where $B_j$ is the matrix obtained from A by replacing the $j$th column of A by the vector $\vb$.
\section{Eigenvalues, Eigenvectors, Diagonalization, and the Spectral Theorem in $\real^n$}
\subsection{Eigenvalues and Eigenvectors}
\paragraph{Definition 4.1.2} Let $T: V \rightarrow V$ be a linear mapping
\begin{enumerate}
	\item A vector $\vx \in V$ is called an eigenvector of T if \textcolor{red}{$\vx \neq \vo$} and there exists a scalar $\lambda \in \real$ such that $T(\vx) = \lambda\vx$
	\item If $\vx$ is an eigenvector of T and $T(\vx) = \lambda \vx$, the scalar $\lambda$ is called the \ti{eigenvalue} of T corresponding to $\vx$.
\end{enumerate}
\paragraph{Proposition 4.1.15} A vector $\vx$ is an eigenvector of T with eigenvalue $\lambda$ if and only if $\vx \neq \vo$ and $\vx \in Ker(T - \lambda I)$.
\paragraph{Definition 4.1.16} Let $T: V \rightarrow V$ be a linear mapping, and let $\lambda \in \real$. The $\lambda$-eigenspace of T, denoted $E_\lambda$, is the set
$$E_\lambda = \{\vx\in V|T(\vx) = \lambda\vx\}$$
That is, $E_\lambda$ is the set containing all the eigenvectors of T with eigenvalue $\lambda$, together with the vector $\vo$. \textcolor{red}{If $\lambda$ is not an eigenvalue of T, then we have $E_\lambda = \{\vo\}$.}
\paragraph{Proposition 4.1.7} $E_\lambda$ is a subspace of V for all $\lambda$.
\paragraph{Proposition 4.1.9} Let $A \in M_{n\times n}(\real)$. Then $\lambda \in \real$ is an eigenvalue of A if and only if $\det(A - \lambda I) = 0$.
\paragraph{Definition 4.1.11} Let $A \in M_{n\times n}(\real)$. The polynomial $\det(A - \lambda I)$ is called the \ti{characteristic polynomial} of A.
\paragraph{Remark} The characteristic polynomial should only depends on the linear mapping defined by the matrix A and not on the matrix itself. (if change to another basis, the characteristic polynomial should be the same.)
\paragraph{Proposition 4.1.12} Similar matrices have equal characteristic polynomials.
\begin{proof}
	Suppose A and B are two similar matrices, so that $B = Q^{-1}AQ$ for some invertible matrix Q. Then we have
	\begin{align*}
		\det(B-\lambda I) &= \det(Q^{-1}AQ - \lambda I) \\
		&= \det(Q^{-1}AQ - Q^{-1}\lambda IQ) \\
		&= \det(Q^{-1}(A - \lambda I)Q)\\
		&= \det(Q^{-1})\det(A - \lambda I)\det(Q)\\
		&= \frac{1}{\det(Q)}\det(A - \lambda I)\det(Q) \\
		&= \det(A - \lambda I)
	\end{align*}
	\qed
\end{proof}
\paragraph{Examples 4.1.13}
\begin{enumerate}
	\item For a general $2 \times 2$ matrix $A = \begin{pmatrix} a&b\\c&d \end{pmatrix}$ we have $$\det(A - \lambda I) = \lambda^2 - Tr(A)\lambda + \det(A)$$
	\item If we substitute A into its own characteristic polynomial, we get $p(A) = 0$. We find that A satisfies its own polynomial equation.
	\item For a general $3 \times 3$ matrix $A = \begin{pmatrix} a&b&c\\d&e&f\\g&h&i \end{pmatrix}$ we have $$\det(A - \lambda I) = -\lambda^3 + Tr(A)\lambda^2 - ((ae - bd) + (ai - cg) + (ei - fh))\lambda + \det(A)$$
	\item For any $n \times n$ matrix A, the characteristic polynomial has the form $$(-1)^n\lambda^n + (-1)^{n-1}Tr(A)\lambda^{n-1} + c_{n-1}\lambda^{n-2} + \hdots + c_1\lambda + \det(A)$$ where the $c_i$ are other polynomial expressions in the entries of the matrix A.
\end{enumerate} 
\paragraph{Corollary 4.1.14} Let $A \in M_{n\times n}(\real)$. Then A has no more than n distinct eigenvalues. In addition, if $\lambda_1, \hdots, \lambda_k$ are the distinct eigenvalues of A and $\lambda_i$ is an $m_i$-fold root of the characteristic polynomial, then $m_1 + \hdots + m_k \leq n$

\paragraph{Theorem 4.1.18} Let $A \in M_{n \times n}(\real)$, and let $p(t) = \det(A - tI)$ be its characteristic polynomial. Then $p(A) = 0$ (the $n \times n$ zero matrix).
\subsection{Diagonalizability}
\paragraph{Definition 4.2.1} Let V be a finite-dimensional vector space, and let $T: V \rightarrow V$ be a linear mapping. T is said to be diagonalizable if there exists a basis of V, all of whose vectors are eigenvectors of T.
\paragraph{Proposition 4.2.2} $T: V \rightarrow V$ is diagonalizable if and only if, for any basis $\alpha$ of V, the matrix $[T]_\alpha^\alpha$ is similar to a diagonal matrix.

\paragraph{Proposition 4.2.4} Let $\vx_i(1\leq i\leq k)$ be eigenvectors of a linear mapping $T: V \rightarrow V$ corresponding to distinct eigenvalues $\lambda_i$. Then $\{\vx_1,\hdots,\vx_k\}$ is a linearly independent subset of V.
\paragraph{Corollary 4.2.5} For each $i (1 \leq i \leq k)$, let $\{\vx_{i,1},\hdots,\vx_{l,n_l}\}$ be a linearly independent set of eigenvectors of T all with eigenvalue $\lambda_i$ and suppose the $\lambda_i$ are distinct. Then $S = \{ \vx_{1,1},\hdots,\vx_{1,n_1} \} \cup \hdots \cup \{ \vx_{k,1},\hdots,\vx_{k,n_k} \}$ is linearly independent.
\paragraph{Proposition 4.2.6} Let V be finite-dimensional, and let $T: V \rightarrow V$ be linear. Let $\lambda$ be an eigenvalue of T, and assume that $\lambda$ is an $m$-fold root of the characteristic polynomial of T. Then we have
	$$1\leq \dim(E_\lambda) \leq m$$
\paragraph{Theorem 4.2.7} Let $T: V \rightarrow V$ be a linear mapping on a finite-dimensional vector space V, and let $\lambda_1, \hdots, \lambda_k$ be its distinct eigenvalues. Let $m_l$ be the multiplicity of $\lambda_i$ as a root of the characteristic polynomial of T. Then T is diagonalizable if and only if 
	\begin{enumerate}
		\item $m_1 + \hdots + m_k = n = \dim(V)$, and
		\item for each $i, \dim(E_{\lambda_i}) = m_i$ 
	\end{enumerate}
\paragraph{Corollary 4.2.8} Let $T: V \rightarrow V$ be a linear mapping on a finite-dimensional space V, and assume that T has $n = \dim(V)$ distinct real eigenvalues. Then T is diagonalizable.
\paragraph{Corollary 4.2.9} A linear mapping $T: V \rightarrow V$ on a finite-dimensional space V is diagonalizable if and only if the sum of the multiplicities of the real eigenvalues is $n = \dim(V)$, and either
\begin{enumerate}
	\item We have $\sum_{i=1}^k \dim(E_{\lambda_i}) = n$, where the $\lambda_i$ are the distinct eigenvalues of T, or
	\item We have $\sum_{i=1}^k (n - \dim(Im(T - \lambda_iI))) = n$, where again $\lambda_i$ are the distinct eigenvalues. 
\end{enumerate}
\paragraph{Remark}
In order for a linear mapping or a matrix to be diagonalizable, it must have enough linearly independent eigenvectors to form a basis of V.
\subsection{Geometry in $\real^n$}
\paragraph{Example}
$$f\cdot g = \int_a^b f(x)g(x) \, dx$$ defines an inner product on $[a, b]$.
\paragraph{Definition 4.3.5} The angle, $\theta$, between two nonzero vectors $\vx, \vy \in \real^n$ is defined to be 
$$ \theta = \cos^{-1}(\frac{<\vx, \vy>}{||x||\cdot ||y||})$$
\paragraph{Definition of Orthogonal and Orthonormal Sets}
\begin{enumerate}
	\item S is an \under{orthogonal set} if $\forall \vx, \vy \in S, \vx \neq \vy \implies \vx \cdot \vy = 0$.
	\item S is an \under{orthonormal set} if it is orthogonal and \textcolor{blue}{all elements are unit vectors.}
\end{enumerate}

\paragraph{Proposition 4.3.10} If $\vx, \vy \in \real^n$ are orthogonal, nonzero vectors, then $\{ \vx, \vy\}$ is linearly independent.

\paragraph{Theorem} Orthogonal sets of nonzero vectors are independent.\\
\proof 
	$$S = \{x_1, \hdots, x_n\}$$
Suppose $c_1x_1 + c_2x_2 + \hdots + c_nx_n = \vo$
\begin{align*}
	0 &= x_i \cdot \vo \\
	&= x_i \cdot \sum_{j=1}^n c_jx_j \\
	&= \sum_{j=1}^n c_j(x_i\cdot x_j)\\
	&= \sum_{j=1}^n c_j(0 \mbox { if } i \neq j) \\
	&= c_i ||x_i||^2
\end{align*}
Since $x_i$ nonzero, then $c_i = 0 \forall i$
\qed

\paragraph{Definition of Bilinearity}
A mapping $B: V \times V \rightarrow \real$ is said to be \under{bilinear} if B is linear in each variable, or more precisely if
\begin{enumerate}
	\item $B(c\vx+\vy,\vz) = cB(\vx, \vz) + B(\vy, \vz)$ and
	\item $B(\vx, c\vy + \vz) = cB(\vx, \vy) + B(\vx, \vz)$ for all $\vx, \vy, \vz \in V$ and all $c \in \real$
\end{enumerate}

\subsection{Orthogonal Projections and the Gram-Schmidt Process}
\paragraph{Definition 4.4.1} The \ti{orthogonal complement} of W, denoted $W^{\perp}$, is the set $W^{\perp} = \{ \vv \in \real^n|<\vv,\vw> = 0 \mbox{ for all } \vw \in W \}$
\paragraph{Remark}If we choose a basis $\{\vw_1,\hdots,\vw_k\}$ for W, then $\vv \in W^\perp$ iff $\vv$ is orthogonal to every vector in the basis.
\paragraph{Examples}
\begin{enumerate}
	\item $W = \{\vo\}$, then $W^\perp = \real^n$
	\item $u_1, u_2 \in \real^3\\ Span\{u_1, u_2\}^\perp = \{x|x\cdot u_1 = 0\} \cap \{x|x\cdot u_2 = 0\} = Ker\begin{pmatrix}u_1\\u_2\end{pmatrix}$
\end{enumerate}
\paragraph{Proposition 4.4.3}
\begin{enumerate}
	\item For every subspace W of $\real^n$, $W^\perp$ is also a subspace of $\real^n$
	\item We have $\dim(W) + \dim(W^\perp) = \dim(\real^n) = n$
	\item For all subspaces W of $\real^n, W \cap W^\perp = \{\vo\}$
	\item Given a subspace W of $\real^n$, every vector $\vx \in \real^n$ can be written uniquely as $\vx = \vx_1 + \vx_2$, where $\vx_1 \in W$ and $\vx_2 \in W^\perp$. In other words, $\real^n = W \bigoplus W^\perp$· 
\end{enumerate}

\paragraph{Definition of Orthogonal Projection}
Every vector $\vx \in \real^n$ can be written uniquely as $\vx = \vx_1 + \vx_2$, where $\vx_1 \in W$ and $\vx_2 \in W^\perp$. Define $P_W: \real^n \rightarrow \real^n$ by $P_W(\vx) = \vx_1$.

\paragraph{Proposition 4.4.5}
\begin{enumerate}
	\item $P_W$ is a linear mapping
	\item $Im(P_W) = W$, and if $\vw \in W$, then $P_W(\vw) = \vw$ (Identity transformation)
	\item $Ker(P_W) = W^\perp$
\end{enumerate}

\paragraph{Proposition 4.4.6} Let $\{ \vw_1,\hdots,\vw_k\}$ be an orthonormal basis for the subspace $W \subset \real^n$
\begin{enumerate}
	\item For each $\vw \in W$, we have $$\vw = \sum_{i=1}^k <\vw,\vw_i>\vw$$
	\item For all $\vx \in \real^n$, we have $$P_W(\vx) = \sum_{i=1}^k <\vx,\vw_i>\vw_i$$
\end{enumerate}
\paragraph{Remarks}
The real meaning of the statement is that we can use the inner product to compute the scalars needed to express the relevant vector in W as a linear combination of the basis vectors $\vw_i$\\
\proof see textbook p194
\paragraph{Gram-Schmidt Orthogonalization Process}
Suppose we are given vectors $\{\vu_1,\hdots,\vu_k\}$ that are linearly independent but not necessarily orthogonal, and we want to construct an orthogonal set of vectors $\{\vv_1, \hdots, \vv_k\}$ with the property that $Span(\{\vu_1,\hdots,\vu_k\}) = Span(\{\vv_1,\hdots, \vv_k\}$).\\
\begin{align*}
&\vv_1 = \vu_1\\
&W_1 = Span\{\vu_1\}\\
&\vv_2 = \vu_2 - P_{W_1}(\vu_2)\\
&W_2 = Span\{\vv_1, \vv_2\}\\
&\vv_3 = \vu_3 - P_{W_2}(\vu_3)\\
&\hdots \\
&W_k = Span\{\vv_1, \hdots, \vv_{k-1}\}\\
&\vv_k = \vu_k - P_{W_k}(\vu_k)
\end{align*}
By proposition 4.4.6, we see that $$P_{W_j}(\vv) = \sum_{i=1}^j\frac{<\vv_i,\vv>}{<\vv_i, \vv_i>}\vv_i$$
Therefore,
\begin{align*}
&\vv_1 = \vu_1\\
&\vv_2 = \vu_2 - \frac{<\vv_1,\vu_2>}{<\vv_1, \vv_1>}\vv_2\\
&\vdots \\
&\vv_{j+1} = \vu_{j+1} - \sum_{i=1}^j\frac{<\vv_i,\vu_{j+1}>}{<\vv_i, \vv_i>}\vv_i
\end{align*}
\paragraph{Remark} The real meaning of the statement is that we can use the inner product to compute the scalars needed to express $\vw$ and $P_W(\vx)$ as a linear combination of the basis vectors $\vw_i$.
\paragraph{Theorem 4.4.9} Let W be a subspace of $\real^n$. Then there exists an orthonormal basis of W.
\subsection{Symmetric Matrices}
\paragraph{Definition 4.5.1} A square matrix A is said to be \ti{symmetric} if $A = A^T$, where $A^T$ denotes the transpose of A.
\paragraph{Proposition 4.5.2a} Let $A \in M_{n\times n}(\real)$.
\begin{enumerate}
	\item For all $\vx, \vy \in \real^n, <A\vx,\vy> = <\vx,A^T\vy>$
	\item A is symmetric if and only if $<A\vx,\vy> = <\vx,A\vy>$ for all vectors $\vx,\vy \in \real^n$
\end{enumerate}
\proof \\
\begin{enumerate}
	\item $\vx, \vy \in \real^n, <A\vx,\vy>=(A\vx)^T\vy=\vx^TA^T\vy = <\vx,A^T\vy>$
	\item Obvious
\end{enumerate}
\qed
\paragraph{Corollary 4.5.2b} Let V be any subspace of $\real^n$, let $T: V \rightarrow V$ be any linear mapping, and let $\alpha = \{\vx_1,\hdots,\vx_k\}$ be any orthonormal basis of V. Then $[T]_\alpha^\alpha$ is a symmetric matrix if and only if $<T(\vx),\vy> = <\vx,T(\vy)>$ for all vectors $\vx, \vy \in V$.
\paragraph{Definition 4.5.3} Let V be a subspace of $\real^n$. A linear mapping $T: V \rightarrow V$ is said to be \under{symmetric} if $<T(\vx),\vy> = <\vx,T(\vy)>$ for all vectors $\vx, \vy \in V$.
\paragraph{Example}
An important class of symmetric mappings is orthogonal projections
see textbook p202
\paragraph{Remark}
Write out the matrix of orthogonal projection transformation with an orthonormal basis, we see a direct proof that orthogonal projections are \under{diagonalizable}.
\paragraph{Fact 4.5.6} For any symmetric matrix:
\begin{enumerate}
	\item All the roots of the characteristic polynomial are real.
	\item eigenvectors corresponding to distinct eigenvalues are orthogonal.
\end{enumerate}
\proof \\
WLOG assume $\lambda_1 \neq 0$
\begin{align*}
	<v_1,v_2> &= <\frac{T(v_1)}{\lambda_1},v_2> \\
	&= \frac{1}{\lambda_1}<T(v_1),v_2> \\
	&= \frac{1}{\lambda_1}<v_1,T(v_2)> \\
	&= \frac{1}{\lambda_1}<v_1,\lambda_2v_2> \\
	&= \frac{\lambda_2}{\lambda_1}<v_1,v_2>  
\end{align*}
Since $\lambda_1 \neq \lambda_2$, we have $<v_1,v_2> = 0$
\qed
\paragraph{Theorem 4.5.7} Let $A \in M_{n\times n}(\real)$ be a symmetric matrix, let $\vx_1$ be an eigenvector of A with eigenvalue $\lambda_1$, and let $\vx_2$ be an eigenvector of A with eigenvalue $\lambda_2$, where $\lambda_1 \neq \lambda_2$. Then $\vx_1$ and $\vx_2$ are orthogonal vectors in $\real^n$. 
\subsection{The Spectral Theorem}
\paragraph{Theorem 4.6.1 - The Spectral Theorem} Let $T: \real^n \rightarrow \real^n$ be a symmetric linear mapping. Then there is an orthonormal basis of $\real^n$ consisting of eigenvectors of T. In particular, T is diagonalizable.\\
\proof By induction\\\\
\tb{Base Case}\\
If $n=1$, then every linear mapping is symmetric and diagonalizable.\\\\
\tb{Inductive Step}\\
Assume the theorem is true for mappings from $\real^k$ to $\real^k$ and consider $T: \real^{k+1} \rightarrow \real^{k+1}$. \\
Let $\lambda$ be any one of the eigenvalues, and let $\vx_1$ be any unit eigenvector with eigenvalue $\lambda$.\\
Let $W = Span(\{\vx_1\})$. Note that $W^\perp$ is a $k$-dimensional subspace of $\real^{k+1}$, so $W^\perp$ is isomorphic to $\real^k$, and we can apply I.H. to $T|_{W^\perp}$
\begin{enumerate}
	\item To see that T takes vectors in $W^\perp$ to vectors in $W^\perp$, note that if $\vy \in W^\perp$, then
	\begin{align*}
		<\vx_1,T(\vy)> &= <T(\vx_1),\vy> \\
		&=<\lambda\vx_1,\vy> \\
		&= 0 \tag{Since $\vy \in W^\perp$}\\
	\end{align*}
	Hence $T(\vy) \in W^\perp$
	\item To see that the restriction of T to $W^\perp$ is still symmetric, note that if $\vy_1, \vy_2 \in W^\perp$, then $<T(\vy_1),\vy_2> = <\vy_1, T(\vy_2)>$, since this holds more generally for all vectors in $\real^{k+1}$.
\end{enumerate}
Hence by I.H. applied to $T|_{W^\perp}$, there exists an orthonormal basis $\{\vx_2,\hdots,\vx_{k+1}\}$ of $W^\perp$, consisting of eigenvectors of the restricted mapping. Union with $\vx_1$, we have the conclusion. \qed

\paragraph{Theorem 4.6.3} Let $T: \real^n \rightarrow \real^n$ be a symmetric linear mapping, and let $\lambda_1,\hdots,\lambda_k$ be the distinct eigenvalues of T. let $P_i$ be the orthogonal projection of $\real^n$ onto the eigenspace $E_{\lambda_i}$. Then
\begin{enumerate}
	\item $T = \lambda_1P_1 + \hdots + \lambda_kP_k$, and
	\item $I = P_1 + \hdots + P_k$
\end{enumerate}
\paragraph{Remark} Spectral Decomposition. This says that $\vx$ can be recovered or built up from its projections on the various eigenspaces of T.\\\\



\section{Complex Numbers and Complex Vector Spaces}
\subsection{Complex Numbers}
\paragraph{Definition 5.1.1} The set of \ti{complex numbers}, denoted $\mathbb{C}$, is the set of ordered pairs of real numbers $(a,b)$ with the operations of addition and multiplication defined by
$$ (a,b) + (c,d) = (a+c,b+d)$$
and the \ti{product} of $(a,b)$ and $(c,d)$ is the complex number defined by
$$(a,b)(c,d) = (ac - bd, ad + cb)$$

\paragraph{Definition 5.1.2} Let $z = a + bi \in \mc$, The \ti{real part} of $z$, denoted Re($z$), is the real number $a$. The \ti{imaginary part} of $z$, denoted Im($z$), is the real number $b$. $z$ is called a \ti{real number} if Im($z$) = 0, and \under{purely imaginary} if Re($z$) = 0.

\paragraph{Definition 5.1.4} A \under{field} is a set $F$ with two operations, defined on ordered pairs of elements of $F$, called \ti{addition} and \ti{multiplication}. Addition assigns to the pair $x$ and $y \in F$ there \ti{sum}, which is denoted by $x+y$ and multiplication assigns to the pair $x$ and $y \in F$ their \ti{product}, which is denoted by $x\cdot y$ or $xy$. These two operations must satisfy the following properties for all$x,y$ and $z \in F$:
\begin{enumerate}
	\item Commutativity of addition: $x+y = y+x$
	\item Associativity of addition: $(x+y)+z=x+(y+z)$
	\item Existence of an additive identity: There is an element $0 \in F$, called zero, such that $x+0 = x$
	\item Existence of additive inverses: For each $x$ there is an element $-x \in F$ such that $x + (-x) = 0$
	\item Commutativity of multiplication: $xy = yx$
	\item Associativity of multiplication: $(xy)z = x(yz)$
	\item Existence of a multiplicative identity: There is an element $1 \in F$, called 1, such that $x \cdot 1 = x$
	\item Existence of multiplicative inverses: If $x \neq 0$, then there is an element $x^{-1} \in F$ such that $xx^{-1} = 1$
\end{enumerate}

\paragraph{Examples}
\begin{enumerate}
	\item $F = \mc$
	\item $F = \real$
	\item $F = \mb{Q}$
	\item $F = \mb{Z} / p\mb{Z}$, $p$ prime
	\item Algebraic numbers = $\{x|p(x) = 0$, for a polynomial $p$ with integer coefficients$\}$
\end{enumerate}
\paragraph{Counter-Example}
$P_n(\real)$

\paragraph{Proposition 5.1.5} The set of complex numbers is a field with the operations of addition and scalar multiplication as defines previously.
\paragraph{More definitions about complex numbers}
$z = a + bi$\\
\under{complex conjugate}: $\bar{z} = a - bi$\\
$z^{-1} = \frac{\bar{z}}{z\bar{z}}$ since $z\bar{z} = a^2 + b^2$
\paragraph{Proposition 5.1.7}
\begin{enumerate}
	\item The additive identity in a field is unique
	\item The additive inverse of an element of a field is unique
	\item The multiplicative identity of a field is unique
	\item The multiplicative inverse of \textcolor{red}{a nonzero element of a field} is unique
\end{enumerate}
\paragraph{Definition 5.1.8}
The \under{absolute value} of the complex number $z = a+bi$ is the nonnegative real number $\sqrt{a^2 + b^2}$ and is denoted by $|z|$ or $r = |z|$. The \ti{argument} of the complex number $z$ is the angle $\theta$ of the polar coordinate representation of $z$. Can write $z = |z|(\cos (\theta) + i\sin(\theta))$
\paragraph{Remark}
In general, if $n$ is an integer,
$$z^n = r^n(\cos(n\theta) + i\sin(n\theta))$$ 
\paragraph{Definition 5.1.11}
A field F is called \under{algebraically closed} if \textcolor{red}{every polynomial $p(z) = a_nz^n + \hdots + a_1z + a_0$ with coefficients in F, $a_i \in F$ for $i = 0,\hdots,n$, has $n$ roots in $F$.}

    \paragraph{Statement of De Moivre's Theorem.}
    \begin{equation*}
        \forall x\in \real, n\in \mathbb{Z}, \left(\cos(x) + i\sin(x)\right)^n = \cos(nx) + i\sin(nx)
    \end{equation*}
    We can also reformulate this into the familiar notation that we used above, denoting the absolute
    value, or length, of the complex number, we have
    \begin{equation*}
        z^n = |z|^n\left(\cos(n\theta) + i\sin(n\theta)\right)
    \end{equation*}

    \paragraph{Relation w/ Euler's Formula.} First, we recall the Euler Formula as below
    \begin{equation*}
        \textcolor{red}{e^{i\theta} = \cos \theta + i\sin \theta}
    \end{equation*}
    Notice that in a special case of $\theta = \pi$, the above identity is a.k.a Euler's 
    Identity($e^{i\pi}+1=0$). Considering any $z\in \mc$, to derive the above identity, we have the following
    \begin{align*}
    	z &= |z|e^{i\theta}\\
        z^n &= |z|^n\left(e^{i\theta}\right)^n\\
        &= |z|^ne^{i\theta n} \\
        &= |z|^n\left(\cos n\theta + i\sin n \theta\right)
    \end{align*}
    notice that we can now interchange, as we please, $\cos \theta + i\sin \theta$ with $e^{i\theta}$. \qed
\paragraph{Theorem 5.1.12} $\mc$ is algebraically closed and $\mc$ is the smallest algebraically closed field containing $\real$
\subsection{Vector Spaces Over a Field}
\paragraph{Definition 5.2.1} A vector space over a field F is a set V (whose elements are called vectors) together with addition and multiplication and 8 axioms as in chapter 1.
\paragraph{Example} $F^n = \{\vx = (x_1, \hdots, x_n) | x_i\in F,$ for $i = 1,\hdots,n\}$

\subsection{Geometry in a complex vector space}
\paragraph{Definition 5.3.1} Let V be a complex vector space, A \textcolor{red}{Hermitian inner product} on V is a complex valued function on pairs of vectors in V, denoted by $<\vu, \vv> \in \mc$ for $\vu, \vv \in V$, which satisfies the following properties:
\begin{enumerate}
	\item For all $\vu, \vv$, and $\vw \in V$ and $a,b\in \mc$, $<a\vu + b\vv, \vw> = a<\vu,\vw> + b<\vv, \vw>$
	\item For all $\vu, \vv, \in V, <\vu, \vv> = \overline{<\vv,\vu>}$, and
	\item For all $\vv \in V, <\vv, \vv> \geq 0$ and $<\vv, \vv> = 0$ implies $\vv = \vo$
\end{enumerate}
\paragraph{Example 5.3.2} Hermitian inner product on $\mc^n$\\
For $\vx = (x_1, \hdots, x_n)$ and $\vy = (y_1, \hdots, y_n) \in \mc^n$, we define their inner product by $<\vx, \vy> = x_1\bar{y_1} + \hdots + x_n\bar{y_n}$, which satisfies the Hermitian inner product properties.
\paragraph{Definition 5.3.7} Let V be a finite dimensional Hermitian inner product space and let $\alpha$ be an orthonormal basis for V. The \under{adjoint} of the linear transformation $T: V \rightarrow V$ is the linear transformation $T^*$ whose matrix with respect to the orthonormal basis $\alpha$ is the matrix $([\bar{T}]_\alpha^\alpha)^t$; that is, $[T^*]_\alpha^\alpha = ([\bar{T}]_\alpha^\alpha)^t$
\paragraph{Proposition 5.3.8} Let V be a finite dimensional Hermitian inner product space. The adjoint of $T: V \rightarrow V$ satisfies $<T(\vv,\vw)> = <\vv, T^*(\vw)>$ for all $\vv$ and $\vw \in V$.
\paragraph{Definition 5.3.9} $T:V \rightarrow V$ is called \textcolor{blue}{Hermitian} or \textcolor{blue}{self-adjoint} if $<T(\vu), \vv> = <\vu,T(\vv)>$ for all $\vu$ and $\vv \in V$. Equivalently, T is Hermitian or self-adjoint if $T = T^*$ or $[\bar{T}]_\alpha^{\alpha t}= [T]_\alpha^\alpha$ for an orthonormal basis $\alpha$. An $n\times n$ complex matrix is called Hermitian or self-adjoint if $A = A^*$.
\paragraph{Proposition 5.3.10}
If $\lambda$ is an eigenvalue of the self-adjoint linear transformation T, then $\lambda \in \real$
\paragraph{Proposition 5.3.11} If $\vu$ and $\vv$ are eigenvectors, respectively, for distinct eigenvalues $\lambda$ and $\mu$ of a self adjoint transformation $T: V \rightarrow V$, then $<\vu, \vv> = 0$, so $\vu$ and $\vv$ are orthogonal.
\paragraph{Theorem 5.3.12} Let $T: V \rightarrow V$ be a self-adjoint transformation of a complex vector space V with Hermitian inner product $<,>$. Then there is an orthonormal basis of V consisting of eigenvectors for T and, in particular, T is diagonalizable.
\paragraph{Theorem 5.3.13} Let $T: V \rightarrow V$ be a self-adjoint transformation of a complex vector space V with Hermitian inner product $<,>$. Let $\lambda_1,\hdots,\lambda_k \in \real$ be the distinct eigenvalues for T, and Let $P_i$ be the orthogonal projections of V onto the eigenspaces $E	_{\lambda_i}$, then
\begin{enumerate}
	\item $T = \lambda_1P_1 + \hdots +\lambda_kP_k$
	\item $I = P_1 + \hdots + P_k$
\end{enumerate}
\section{Jordan Canonical Form}
A next best form after a diagonal form for the matrices of linear mappings that are not necessarily diagonalizable
\subsection{Triangular Form}
\paragraph{Definition 6.1.2} Let $T: V\rightarrow V$ be a linear mapping. A subspace W $\subset V$ is said to be \ti{invariant} (or \ti{stable}) under T if $T(W) \subset W$.
\paragraph{Proposition 6.1.4}
Let V be a vector space, let $T: V \rightarrow V$ be a linear mapping, and let $\beta = \{x_1, \hdots, x_n\}$ be a basis for V. Then $[T]_\beta^\beta$ is upper triangular if and only if each of the subspaces $W_i = Span(\{x_1,\hdots,x_i\})$ is invariant under T.\\
Note that the subspaces $W_i$ in the proposition are related as follows:
$$\{\vo\} \subset W_1 \subset W_2 \subset \hdots \subset W_{n-1} \subset W_n = V$$
The $W_i$ form an \ti{increasing sequence} of subspaces.
\paragraph{Definition 6.1.5} We say that a linear mapping $T: V \rightarrow V$ on a finite-dimensional vector space V is \under{triangularizable} if there exists a basis $\beta$ such that $[T]_\beta^\beta$ is upper-triangular.
\paragraph{Proposition 6.1.6} Let $T: V \rightarrow V$, and let $W \subset V$ be an invariant subspace. Then the characteristic polynomial of $T|_W$ divides the characteristic polynomial of T.
\paragraph{Remark} Every eigenvalue of $T|_W$ is also an eigenvalue of T (the set of eigenvalues of $T|_W$ is some subset of the eigenvalues of T on the whole space).
\paragraph{Theorem 6.1.8} Let V be a finite-dimensional vector space over a field F, and let $T: V \rightarrow V$ be a linear mapping. Then T is \under{triangularizable} if and only if the characteristic polynomial equation of $p(t)$ has $dim(V)$ roots (counted with multiplicities) in the field F.
\paragraph{Remark} The theorem implies that every matrix $A \in M_{n\times n}(\mc)$ may be triangularized.
\paragraph{Proof of Lemma}
Let $\alpha = \{x_1,\hdots,x_k\}$ be a basis for W and extend $\alpha$ by adjoining $\alpha' = \{x_{k+1},\hdots, x_n\}$ to form a basis $\beta = \{x_1,\hdots, x_k, x_{k+1},\hdots, x_n\}$ for V. Let $W' = Span(W')$.\\
Define $P:V\rightarrow V$ by $$P(a_1x_1+\hdots+a_nx_n) = a_1x_1+\hdots+a_kx_k$$
Notice that $Ker(P) = W', Im(P)=W, P^2 = P$. \textcolor{blue}{P is called the projection on $W$ with kernel $W'$.} Then $I - P$ is the projection on $W'$ with kernel $W$.
Then $I-P$ is the projection on $W'$ with kernel $W$.\\
Let $S = (I-P)T$. Since $Im(I-P) = W'$, we see by prop2.5.6 that $Im(S) \subset Im(I-P) = W'$. Hence W' is an invariant subspace of S.
Then the eigenvalues of $S|_{W'}$ is a subset of the set of eigenvalues of T. Since all the eigenvalues of T lie in the filed F, the same is true of all the eigenvalues of $S|_{W'}$. Hence there is some nonzero vector $\vx \in W'$ and some $\lambda \in F$ such that $S(x) = \lambda\vx$. So
$$(I-P)T(\vx) = \lambda\vx$$
$$\implies T(\vx) - PT(\vx) = \lambda \vx$$
$$\implies T(\vx) = \lambda\vx + PT(\vx)$$
where $\lambda\vx \in Span(\{\vx\})$ and $PT(\vx) \in W$. Therefore $W + Span(\{\vx\})$ is also invariant under T and this finishes the proof. \qed
\paragraph{Proof of Theorem 6.1.8}
$\rightarrow:$ If T is triangularizable, then there exists a basis $\beta$ for V such that $[T]_\beta^\beta$ is upper-triangular. The eigenvalues of T are the diagonal entries of this matrix, so they are elements of the field F.\\
$\leftarrow:$ If all the eigenvalues are in F:\\
Let $\lambda$ be any eigenvalue of T, and let $x_1$ be an eigenvector of $\lambda$, let $W_1 = Span(\{x_1\})$. By definition $W_1$ is invariant under T. Now, assume by induction that we have constructed invariant subspaces $W_1 \subset W_2 \subset \hdots \subset W_k$ with $W_i = Span(\{x_1,\hdots,x_l\})$ for each i. By Lemma 6.1.10 there exists a vector $x_{k+1} \notin W_k$ such that the subspace $W_{k+1} = W_k + Span(\{x_{k+1}\})$ is also invariant under T. We continue this process until we have produced a basis for V. Hence, T is triangularizable. \qed

\paragraph{Lemma 6.1.10} Let $T: V \rightarrow V$ be as in the theorem, and assume that the characteristic polynomial of T has $n = \dim(V)$ roots in F. If $W \subsetneqq V$ is an invariant subspace under T, then there exists a vector $\vx \neq\vo$ in V such that $\vx \notin W$ and $W + Span(\{\vx\})$ is also invariant under T.
\paragraph{Remark}
What this says is that we can make a T-invariant subspace 1-dimension bigger.
\paragraph{Corollary 6.1.11} If $T: V \rightarrow V$ is triangularizable, with eigenvalues $\lambda_i$ with respective multiplicities $m_i$, then there exists a basis $\beta$ for V such that $[T]_\beta^\beta$ is upper-triangular, and the diagonal entries of $[T]_\beta^\beta$ are $m_1\lambda_1$'s, followed by $m_2 \lambda_2$'s, and so on.
\paragraph{Theorem 6.1.12 (Cayley-Hamilton)} If $T: V \rightarrow V$ be a linear mapping on a finite-dimensional vector space V, and let $p(t) = \det(T - tI)$ be its characteristic polynomial. Assume that $p(t)$ has $\dim(V)$ roots in the field F over which V is defined. Then $p(T) = 0$ 
\subsection{A Canonical Form For Nilpotent Mappings}
\paragraph{Definition} A linear mapping $N: V \rightarrow V$ is \under{nilpotent} if $N^k = 0$ for some integer $k \geq 1$.
\paragraph{Proposition} $N: V \rightarrow V$ is nilpotent if and only if it has one eigenvalue $\lambda = 0$ with multiplicity $n = \dim(V)$.
\paragraph{Proposition 6.2.3} With all notations as before:
\begin{enumerate}
	\item $N^{k-1}(\vx)$ is an eigenvector of N with eigenvalue $\lambda = 0$
	\item $C(\vx)$ is an invariant subspace of V under N.
	\item The cycle generated by $\vx \neq 0$ is a linearly independent set. Hence $\dim(C(\vx)) = k$, the length of the cycle.
\end{enumerate}
\paragraph{Proposition 6.2.4} Let $\alpha_1 = \{N^{k_i - 1}(\vx_i),\hdots,\vx_i\} (1\leq i \leq r)$ be cycles of lengths $k_i$, respectively. If the set of eigenvectors $\{N^{k_i-1}(\vx_1),\hdots,N^{k_r-1}(\vx_r)\}$ is linearly independent, then $\alpha_1 \cup \hdots \cup \alpha_r$ is linearly independent.
\paragraph{Remark} For a given $\vx \in V$, either $\vx = \vo$ or there is a unique integer $k$, $1 \leq k \leq n$, such that $N^k(\vx) = \vo$ but $N^{k-1}(\vx) \neq \vo$. 
\paragraph{Definitions 6.2.1} Let $N, \vx \neq \vo$ and $k$ be as before
\begin{enumerate}
	\item The set $\{ N^{k-1}(\vx),N^{k-2}(\vx),\hdots,\vx\}$ is called the cycle generated by $\vx$. $\vx$ is called the \under{initial vector} of the cycle.
	\item The subspace $Span(\{N^{k-1}(\vx), N^{k-2}(\vx),\hdots,\vx\}$ is called the \under{cyclic subspace} generated by $\vx$, and denoted $C(\vx)$
	\item The integer $k$ is called the \under{length} of the cycle
\end{enumerate}
\paragraph{Definition 6.2.5} We say that the cycles $\alpha_i = \{N^{k_i-1}(\vx_i),\hdots,x_i\}$ are \under{non-overlapping cycles} if $\alpha_1 \cup \hdots \cup \alpha_r$ is linearly independent.
\paragraph{Definition 6.2.7} Let $N: V \rightarrow V$ be a nilpotent mapping on a finite-dimensional vector space V. We call a basis $\beta$ for $V$ a \under{canonical basis} (with respect to N) if $\beta$ is the union of a collection of nonoverlapping cycles for N.
\paragraph{Theorem 6.2.8 (Canonical form for nilpotent mappings)} Let $N: V \rightarrow V$ be a nilpotent mapping on a finite-dimensional vector space. There exists a canonical basis $\beta$ of V with respect to N.
\paragraph{Lemma 6.2.9} Consider the cycle tableau corresponding to a canonical basis for a nilpotent mapping $N: V \rightarrow V$. As before, let $r$ be the number of rows, and let $k_i$ be the number of boxes in the $i$th row ($k_1\geq k_2 \geq \hdots \geq k_r$). For each $j ( 1\leq j \leq k_1)$, the number of boxes in the $j$th column of the tableau is $\dim(Ker(N^j)) - \dim(Ker(N^{j-1})$. 
\paragraph{Corollary 6.2.11} The canonical form of a nilpotent mapping is unique (provided the cycles in the canonical basis are arranged so the lengths satisfy $k_1 \geq k_2 \geq \hdots \geq k_r)$
\subsection{Jordan Canonical Form}

\paragraph{Proposition 6.3.1} Let $T: V \rightarrow V$ be a linear mapping whose characteristic polynomial has $\dim(V)$ roots ($\lambda_i$ with respective multiplicities $m_i, 1 \leq i\leq k$) in the field $F$ over which V is defined.
\paragraph{(a)}There exist subspaces $V'_i\subset V (1\leq i \leq k)$ such that
\begin{enumerate}
	\item Each $V'_i$ is invariant under $T$
	\item $T|_{V'_i}$ has exactly one distinct eigenvalue $\lambda_i$, and
	\item $V = V'_1 \bigoplus \hdots \bigoplus V'_k$
\end{enumerate}
\paragraph{(b)} There exists a basis $\beta$ for V such that $[T]_\beta^\beta$ has a direct sum decomposition into upper-triangular blocks of the form 	$\begin{bmatrix}
		\lambda &1&0&\hdots&0\\
		0&\lambda&1&\hdots&0\\
		\vdots & \ddots&\ddots&\ddots&1\\
		0&&\hdots&0&\lambda
	\end{bmatrix}$
\paragraph{Definition 6.3.2} Let $T: V \rightarrow V$ be a linear mapping on a finite-dimensional vector space V. Let $\lambda$ be an eigenvalue of T with \textcolor{red}{multiplicity $m$.}
\begin{enumerate}
	\item The \under{$\lambda$-generalized eigenspace}, denoted by $K_\lambda$, is the kernel of the mapping $(T - \lambda I)^m$ on V.
	\item The nonzero elements of $K_\lambda$ are called \under{generalized eigenvectors} of T.
\end{enumerate}

\paragraph{Definitions 6.3.5}
\begin{enumerate}
	\item A matrix of the form
	$\begin{bmatrix}
		\lambda_i &1&0&\hdots&0\\
		0&\lambda_i&1&\hdots&0\\
		\vdots & \ddots&\ddots&\ddots&1\\
		0&&\hdots&0&\lambda_i
	\end{bmatrix}$ is called a \under{Jordan block matrix}
	\item A matrix $A \in M_{n \times n}(\mb{F})$ is said to be in \under{Jordan canonical form} if A is a direct sum of Jordan block matrices.
\end{enumerate}
\paragraph{Theorem 6.3.6 (Jordan Canonical Form)} Let $T: V \rightarrow V$ be a linear mapping on a finite-dimensional vector space V whose characteristic polynomial has $\dim(V)$ roots in the field $\mb{F}$ over which V is defined.
\begin{enumerate}
	\item There exists a basis $\gamma$ (called a canonical basis) of V such that $[T]_\gamma^\gamma$ has a direct sum decomposition into Jordan block matrices.
	\item In this decomposition the number of Jordan blocks and their sizes are uniquely determined by T. (The order in which the blocks appear in the matrix may be different for different canonical bases, however).
\end{enumerate} 
\subsection{Computing Jordan Form}
\paragraph{Algorithm}
\begin{enumerate}
	\item Find all the eigenvalues of T and their multiplicities by factoring the characteristic polynomial completely (assume the field is algebraically closed)
	\item For each distinct eigenvalue $\lambda_i$ in turn, construct the cycle tableau for a canonical basis of $K_{\lambda_i}$ with respect to the mapping $N_i = (T - \lambda_iI)|K_{\lambda_i}$ using the method: for each $j$, the number of boxes in the $j$th column of the tableau for $\lambda_i$ will be $$\dim(Ker(T-\lambda_i I)^j) - \dim(Ker(T-\lambda_i I)^{j-1}$$
	\item Form the corresponding Jordan blocks and assemble the matrix of T.
\end{enumerate}


\newpage
\section{Problem Notes}
\paragraph{1} $S = \{\tb{a}\} \subseteq \real^2$, then we cannot determine whether S is dependent (when $\tb{a} = \vo$) or independent (when $\tb{a} \neq \vo$)
\paragraph{2} If a set in a vector space contains the zero vector, then it is linearly dependent.
\paragraph{3} The order of Jordan blocks does not matter: if you change the order of Jordan blocks, it is still equivalent to the original one.



\section{Proof Clinic - JCF}
\paragraph{Facts}
\begin{enumerate}
	\item $E_\lambda \subset K_\lambda$, and both are T-invariant
	\item $\forall \mu \neq \lambda, (T - \mu I)|_{K_\lambda}$ is bijective
	\item $K_\lambda = Ker((T-\lambda I)^{m_i})$
	\item Bases $\beta_i, \beta_j$ for $K_{\lambda_i}$ and $K_{\lambda_j}$, respectively, are disjoint if $\lambda_i \neq \lambda_j$
	\item $\underset{\lambda} {\cup}\beta_{K_\lambda}$ is a basis for V if each $\beta_{K_\lambda}$ is a basis for $K_\lambda$
	\item T is diagonalizable $\iff K_\lambda = E_\lambda \, \forall \lambda$
	\item $V = \underset{\lambda}{\bigoplus} K_\lambda$
	\item Similar matrices have the same JCF
\end{enumerate}
Suppose $\beta = \underset{\lambda}{\cup}\gamma_\lambda$ is a basis of V, where each $\gamma_\lambda$ is a cycle of generalized eigenvectors of T. Then $Span(\gamma_\lambda)$ is T-invariant and $[T|_{Span(\gamma_\lambda)}]_{\gamma_\lambda}$ is a Jordan block and $\beta$ is a Jordan canonical basis.
\end{document}
