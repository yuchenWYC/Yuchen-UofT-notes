\documentclass[11pt]{article}

% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}

% Property settings.
\MakePerPage{footnote}
\pagestyle{fancy}
\lhead{Notes by Y.W.}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof: }}}
\newcommand{\stdn}[0]{N(0, 1)}
\newcommand{\qed}[0]{$\hfill\blacksquare$}

\newcommand{\expect}[1]{\mathbb{E}({#1})}
% Attr.
\title{STA261 Probability and Statistics II \\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
\section{Converge in distribution}
\section{Normal Distribution Theory}
\paragraph{Theorem: Sum of independent normal random variables}
Suppose $X_i \sim N(\mu_i, \sigma^2_i)$ for $i = 1, 2,...,n$ and that they are independent random variables. Let $Y = (\Sigma_ia_iX_i) + b$ for some constants $\{a_i\}$ and $b$. Then
$$Y \sim N((\Sigma_ia_i\mu_i)+b, \Sigma_ia^2_i\sigma^2_i)$$
\paragraph{Corollary: The distribution of the sample mean of normal random variables} Suppose $X_i \sim N(\mu, \sigma^2)$ for $i=1,2,...,n$ and that they are independent random variables, If $\bar X = (X_1 + ... +X_n)/n$, then $\bar X \sim N(\mu,\sigma^2/n)$
\paragraph{Theorem: The covariance of sums of normal random variables}Suppose $X_i \sim N(\mu_i, \sigma_i^2)$ for $i=1,2,...,n$ and also that the $\{X_i\} $are independent. Let $U=\Sigma^n_{i=1}a_iX_i$ and $V=\Sigma^n_{i=1}b_iX_i$ for some constants $\{a_1\}$ and $\{b_i\}$. Then $Cov(U,V) = \Sigma_ia_ib_i\sigma^2$. Furthermore, $Cov(U,V) = 0$ if and only if U and V are independent.

\section{Expectation and Covariance}
\subsection{Expectation -Discrete case}
\paragraph{Definition of expectation} Let X be a discrete random variable, taking on distince values $x_1,x_2,...$, with $p_i = P(X = x_i)$. Then the \ti{expected value} (or \ti{mean} or \ti{mean value}) of X, written E(X) (or $\mu_x$), is defined by $$E(X) = \Sigma_i x_ip_i$$
\paragraph{Theorem: expectation involving nested functions} 
\begin{enumerate}
\item Let X be a discrete random variable, and let $g: \mb{R} \rightarrow \mb{R}$ be some function such that the expectation of the random variable $g(X)$ exists. Then $$E(g(X)) = \Sigma_x g(x)P(X=x)$$
\item Let X and Y be discrete random variables, and let $h: \mb{R}^2 \rightarrow \mb{R}$ be some function such that the expectation of the random variable $h(X,Y)$ exists. Then
$$E(h(X,Y)) = \Sigma_{x,y}h(x,y)P(X=x,Y=y)$$
\end{enumerate}
\paragraph{Theorem: Linearity of expected values} Let X and Y be discrete random variables, let $a$ and $b$ be real numbers, and put $Z = aX + bY$. Then $$E(Z) = aE(X) + bE(Y)$$
\paragraph{Theorem: Expectation of product of independent r.v}Let X and Y be discrete random variables that are independent. Then $$E(XY) = E(X)E(Y)$$
\paragraph{Monotonicity} Let X and Y be discrete random variables, and suppose that $X \leq Y$ (Remember that this means $X(s) \leq Y(s)$ for all $s\in S)$ Then $E(X) \leq E(Y)$.
\subsection{Expectation - Continuous case}
\paragraph{Definition of expectation} Let X be an absolutely continuous random variable, with density function $f_X$. Then the \ti{expected value} of X is given by
$$ E(x) = \int_{-\infty}^{\infty}xf_X(x)dx$$
\paragraph{Theorem: expectation involving nested functions} 
\begin{enumerate}
\item Let X be a an absolutely continuous random variable with density function $f_X$, and let $g: \mb{R} \rightarrow \mb{R}$ be some function such that the expectation of the random variable $g(X)$ exists. Then $$E[g(x)]=\int_{-\infty}^{\infty} g(x)f_X(x)dx$$
\item Let X and Y be discrete random variables, and let $h: \mb{R}^2 \rightarrow \mb{R}$ be some function such that the expectation of the random variable $h(X,Y)$ exists. Then
 $$E(h(X,Y)) = \int_{-\infty}^{\infty}h(x,y)f_{X,Y}(x,y)dxdy$$
 \end{enumerate}
 \paragraph{Theorem: Linearity of expected values} Let X and Y be jointly absolutely continuous random variables, let $a$ and $b$ be real numbers. Then $$E(aX + bY) = aE(X) + bE(Y)$$
\paragraph{Monotonicity} Let X and Y be jointly continuous random variables, and suppose that $X \leq Y$ (Remember that this means $X(s) \leq Y(s)$ for all $s\in S)$ Then $E(X) \leq E(Y)$.
\subsection{Variance, Covariance and Correlation}
\paragraph{Definition of variance} The \ti{variance} of a random variable X is the quantity
$$\sigma^2_x = Var(X) = E((X - \mu_X)^2)$$
where $\sigma_X$ is the \ti{standard deviation} of X.
\paragraph{Theorem} Let X be any r.v. with $\mu_X = E(X)$ and variance Var(X). Then the following hold true:
\begin{enumerate}
	\item $Var(X) \geq 0$
	\item If $a$ and $b$ are real numbers, $Var(aX+b) = a^2Var(X)$
	\item $Var(X) = E(X^2) - (\mu_X)^2 = E(X^2) - E(X)^2$
	\item $Var(X) \leq E(X^2)$
\end{enumerate}
\paragraph{Definition of covariance} $$Cov(X,Y) = E((X - \mu_X)(Y - \mu_Y))$$
\paragraph{Theorem: Linearity of covariance} Let X, Y and z be three r.v.s. Let $a$ and $b$ be real numbers. Then
$$Cov(aX + bY. Z) = aCov(X,Z) + bCov(Y,Z)$$
\paragraph{Theorem} Let X and Y be r.v.s. Then
$$Cov(X,Y) = E(XY) - E(X)E(Y)$$
\paragraph{Theorem} If X and Y are independent, then $$Cov(X,Y) = 0$$
\paragraph{Theorem} 
\begin{enumerate}
\item For any r.v.s X and Y,
$$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$
\item More generally, for any r.v.s $X_1,...,X_n$,$$Var(\Sigma_i X_i) = \Sigma_i Var(X_i) + 2\Sigma_{i < j}Cov(X_i,X_j)$$
\end{enumerate}
\paragraph{Corollary}
\begin{enumerate}
	\item If X and Y are independent, then $Var(X+Y) = Var(X) + Var(Y)$
	\item If $X_1,...X_n$ are independent, then $Var(\Sigma_{i=1}^n X_i) = \Sigma_{i=1}^nVar(X_i)$
\end{enumerate}
\paragraph{Definition} The \ti{correlation} of two r.v.s X and Y is given by
$$Corr(X,Y) = \frac{Cov(X,Y)}{Sd(X)Sd(Y)}$$ 
provided $Var(X) < \infty$ and $Var(Y) < \infty$
\section{Independent Random Variables}
\paragraph{Definition 1} Let $X$ and $Y$ be two continuous random variables. We say X and Y are independent if
$$f_{X,Y}(x,y) = f_X(x) \times f_Y(y)$$
$\forall x,y \in \mathbb{R}$
\paragraph{Lemma 1} X and Y are two continuous random variables. If X and Y are independent, then
$$E[g(X)h(Y)] = E(g(X)] \times E[h(Y)]$$
\textcolor{red}{for any two functions $g()$ and $h()$.} \\\\
\proof\\
\begin{align*}
	E[g(X)h(Y)] &= \int_{-\infty}^\infty \int_{-\infty}^\infty g(x)h(y)f_{X,Y}(x,y)\,dxdy\\
	&= \int_{-\infty}^\infty \int_{-\infty}^\infty g(x)h(y)f_X(x)f_Y(y)\,dxdy\\
	&= \int_{-\infty}^\infty f_Y(y)h(y)\int_{-\infty}^\infty g(x)f_X(x)\,dxdy\\
	&= \int_{-\infty}^\infty f_Y(y)h(y)E[g(X)]\,dy\\
	&= E[g(X)]\int_{-\infty}^\infty f_Y(y)h(y)\,dy\\
	&= E[g(X)]E[h(Y)]
\end{align*}
\qed
\section{Types of Inferences}
\paragraph{Estimation:}
\begin{enumerate}
	\item Point estimation: Based on the sample observations, calculating a particular value as an estimate of the parameter $\theta$
	\item Interval estimation: Calculating a range of values that is likely to contain the parameter $\theta$
\end{enumerate}
\paragraph{Hypothesis testing} Based on the sample, assess whether a hypothetical value $\theta_0$ is a plausible value of the parameter $\theta$ or not.
\section{Different Types of Estimation}
\subsection{Method of Moments Estimation}
Let $X_1, X_2, ..., X_n$ are independently and identically distributed (i.i.d.) random variables. \newline
Let the $k^{th}$ population moment be
$$\mu_k = E[X^k]$$
$k^{th}$ sample moment based on sample
$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}X^k_i$$
We use $\hat{\mu}_k$ as an estimator of $\mu_k$ \newline
In other words, we use the sample moments as estimators of the population moments.
\subsection{Maximum Likelihood Estimation}
\paragraph{Definition of Likelihood Function}
Suppose $X_1, X_2,...,X_n$ has a joint density or mass function $f(x_1, x_2,...,x_n|\theta)$ \newline
We observe sample, $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$ \newline
Given the sample, the likelihood function of $\theta$, noted as $L(\theta|x_1, x_2,...,x_n)$, is defined as
$$L(\theta|x_1, x_2,...,x_n) = f(x_1, x_2,...,x_n|\theta)$$
Often written as $L(\theta)$, is a function of $\theta$. \newline
If X follows a discrete distribution, it gives the probability of observing the sample as a function of the parameter $\theta$ \newline
If $X_1, X_2,...,X_n$ are i.i.d. then their joint density is the product of marginal densities, $f_\theta(x)$\newline
Hence, in i.i.d. case we write
$$L(\theta) = \Pi_{i=1}^n f_\theta(x_i)$$
\paragraph{Comments}
\begin{enumerate}
	\item $L(\theta)$ is NOT a pdf or pmf of $\theta$
	\item Likelihood introduces a belief ordering on parameter space, $\Omega$
	\item For $\theta_1, \theta_2 \in \Omega$, we believe in $\theta_1$ as the true value of $\theta$ over $\theta_2$ whenever $L(\theta_1) > L(\theta_2)$
	\item Which means, the data is more likely to come from $f_{\theta_1}$ than $f_{\theta_2}$
	\item The value $L(\theta)$ is very small for every value of $\theta$
	\item So often, we are interested in the likelihood ratios:
	$$\frac{L(\theta_1)}{L(\theta_2)}$$
\end{enumerate}
\paragraph{Maximum Likelihood Estimation}
\begin{enumerate}
	\item Let's say we are interested in a point estimate of $\theta$
	\item A sensible choice will be to pick $\hat{\theta}$ that maximizes $L(\theta)$
	\item So $\hat{\theta}$ satisfies $L(\hat{\theta}) \geq L(\theta)$ for all $\theta \in \Omega$
	\item $\hat{\theta}$ is called the \under{maximum likelihood estimate} (MLE) of $\theta$
\end{enumerate}
\paragraph{Computation of the MLE}
\begin{enumerate}
	\item Define, log-likelihood function, $l(\theta) = \ln{L(\theta)}$
	\item $\ln(x)$ is a 1-1 increasing function of $x > 0 \implies L(\hat{\theta}) \geq L(\theta)$ for $\theta \in \Omega$ iff $l(\hat{\theta}) \geq l(\theta)$
	\item In other words, if $L(\theta)$ is maximized at $\hat{\theta}$ then $l(\theta)$ will also be maximized at $\hat{\theta}$
	\item Therefore, $$l(\theta) = \ln{(\Pi_{i=1}^nf_\theta(x_i))} = \sum_{i=1}^n\ln{f_\theta(x_i)}$$
	\item The obvious benefit: It's much easier to differentiate a sum than a product
	\item Solve the equation, $\frac{\partial l(\theta)}{\partial \theta} = 0$ for $\theta$
	\item Say, $\hat{\theta}$ is the solution. But it's still not the MLE
	\item Need to check whether or not $$\frac{\partial^2 l(\theta)}{\partial \theta^2}\vert_{\theta = \hat{\theta}} < 0$$
\end{enumerate}
\paragraph{Properties of MLE}
\begin{enumerate}
	\item MLE is not unique
	\item MLE may not exist
	\item The likelihood may not always be differentiable.
\end{enumerate}
\section{Sampling Distribution of an Estimator}
\begin{enumerate}
	\item Recall: An Estimator (T) is a random variable (infinite number of sample means)
	\item If we repeat the sampling procedure and keep calculating T for each set of sample and finally draw a density histogram based on the T values we get the sampling distribution of T
	\item \tb{\textcolor{blue}{Standard error}:} Standard deviation of an estimator is called the standard error (SE)
\end{enumerate}

\paragraph{Definition of Mean Squared Error}
Let $\psi(\theta)$ be any real valued function of $\theta$, suppose T is an estimator of $\psi(\theta)$
$$MSE_\theta(T) = E_\theta[(T - \psi(\theta))^2]$$
\paragraph{Corollary}
$$MSE_\theta(T) = Var_\theta(T) + (E_\theta(T) - \psi(\theta))^2$$
\proof
\begin{align*}
	MST(T) &= E[(T - \psi({\theta}))^2] \\
	&= E[(T - E(T) + E(T) - \psi({\theta}))^2] \\
	&= E[(T - E(T))^2 + (E(T) - \psi({\theta}))^2 + 2(T-E(T))(E(T)-\psi({\theta}))] \\
	&= E[(T - E(T))^2] + (E(T) - \psi({\theta}))^2 +  2E[T-E(T)](E(T)-\psi({\theta}))\\
	&= E[(T - E(T))^2] + (E(T) - \psi({\theta}))^2 \tag{Since $E[T-E(T)] =  E(T)-E(T) = 0$}\\
	&= Var(T) + (E(T) - \psi({\theta}))^2\\
	&= Var(T) + Bias^2(T)
\end{align*}\qed

\paragraph{Bias} The bias of an estimator T of $\psi(\theta)$ is given by $$E_\theta(T) - \psi(\theta)$$
\paragraph{Unbiased estimator:} When the bias of an estimator is zero, it's called unbiased
\paragraph{Remark}
\begin{enumerate}
	\item For unbiased estimators, $$MSE_\theta(T) = Var_\theta(T)$$
	\item If all the other properties are similar, then an unbiased estimator is preferred over a biased estimator.
	\item In practice, often an biased estimator with lower variance is preferred over an unbiased estimator with really high variance. \tb{We minimize MSE}.
\end{enumerate}
\section{Population Variance $(\sigma^2)$}
\paragraph{Definition} $\sigma^2 = E[(X - \mu)^2]$ where $\mu = E[X]$. \newline
If we have equally likely N data points in our population, this is equivalent of $$\sigma^2 = \frac{1}{N}\sum_{i = 1}^N(X_i - \mu)^2$$
\paragraph{In words:} It's the average squared difference of each of the data points ($X_i$) from the mean ($\mu$)
\paragraph{Estimate $\sigma^2$ based on a sample of size $n$}
When we are estimating based on the sample of size $n$, we replace $\mu$ by $\bar X$, so the numerator is $\sum_{i=1}^n(X_i-\bar X)^2$. We can divide it by \under{both $n$ or $n-1$}. The latter one is unbiased! \newline
The fraction, $\frac{n-1}{n} \rightarrow 1$ as $n \rightarrow \infty$. So for large $n$, both estimator will produce similar estimate. In statistical literature, whenever we say \ti{sample variance} we refer to the $unbiased$ one. Hence, from now on,
\paragraph{Definition of sample variance}
$$S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar X)^2$$
\section{Sampling distribution of $S^2$(under Normal Distribution}
\paragraph{Theorem} Suppose $X_1, X_2,...,X_n \sim N(\mu,\sigma^2)$ iid,
$\bar X = \frac{1}{n}\sum_{i=1}^nX_i$ and $S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar X)^2$. \newline
Then 
\begin{enumerate}
	\item $\bar X$ and $S^2$ are independent, and 
	\item $\frac{(n-1)S^2}{\sigma^2} \sim \chi_{(df=n-1)}^2$
\end{enumerate}
\proof \newline
\paragraph{Part 1}
Let 
\begin{align*}
    U &= \bar X = \frac{1}{n}X_1 + ... +\frac{1}{n}X_n \\
	V &= X_1 - \bar X = X_1 - (\frac{1}{n}X_1 + ... +\frac{1}{n}X_n) \\
	&= (1 - \frac{1}{n})X_1 - (\frac{1}{n}X_2 + ... +\frac{1}{n}X_n) \\\\
	Cov(\bar X, X_1 - \bar X) &= Cov(\bar X, X_1) - Cov(\bar X, \bar X) \\
	&= Cov(\frac{1}{n}X_1 + ... +\frac{1}{n}X_n, X_1) - \frac{\sigma^2}{n}\\
	&= \frac{1}{n}Cov(X_1,X_1) -  \frac{\sigma^2}{n}\\
	&=  \frac{\sigma^2}{n} -  \frac{\sigma^2}{n}\\
	&= 0 \\
\end{align*}
Hence by E\&R theorem, U and V are independent.
Similarly, we can show $\bar X$ is independent to each $X_i - \bar X$ for $i = 1, ..., n$ \newline
Therefore, $\bar X$ is independent to  $\sum_{i=1}^n(X_i - \bar X)^2$ \newline
Therefore, $\bar X$ is independent to  $\frac{\sum_{i=1}^n(X_i - \bar X)^2}{n-1} = S^2$
\paragraph{Part 2}
\begin{align*}
	\sum_i(X_i - \mu)^2 &= \sum_i(X_i - \bar X + \bar X - \mu)^2 \\
	&= \sum_i(X_i - \bar X)^2 + \sum_i(\bar X - \mu)^2 + 2\sum_i(X_i - \bar X)(\bar X - \mu)\\
	&= \sum_i(X_i - \bar X)^2 + \sum_i(\bar X - \mu)^2 + 2(\bar X - \mu)\sum_i(X_i - \bar X)\\
	&= \sum_i(X_i - \bar X)^2 + \sum_i(\bar X - \mu)^2 + 2(\bar X - \mu)(\sum_i X_i - n\bar X)\\
	&= \sum_i(X_i - \bar X)^2 + \sum_i(\bar X - \mu)^2 + 2(\bar X - \mu)(n\bar X - n\bar X)\\
	&= \sum_i(X_i - \bar X)^2 + n(\bar X - \mu)^2 \\
	\implies \sum_i(X_i - \bar X)^2 &= \sum_i(X_i - \mu)^2 - n(\bar X - \mu)^2
\end{align*}
\begin{align*}
\frac{\sum_{i=1}^n(X_i - \mu)^2}{\sigma^2} &= \frac{\sum_i(X_i - \bar X)^2}{\sigma^2} + \frac{ n(\bar X - \mu)^2}{\sigma^2} \\
	\implies \sum_{i}(\frac{X_i - \mu}{\sigma})^2 &= \frac{\sum_i(X_i - \bar X)^2}{\sigma^2} + (\frac{\bar X - \mu}{\sigma/\sqrt{n}})^2 \\
	&= \frac{(n-1)S^2}{\sigma^2} + (\frac{\bar X - \mu}{\sigma/\sqrt{n}})^2 \\
	\implies \chi_{(n)}^2 &= \frac{(n-1)S^2}{\sigma^2} + \chi^2_{(1)}\\
	\implies MGF(\chi_{(n)}^2) &= MGF(\frac{(n-1)S^2}{\sigma^2} + \chi^2_{(1)})\\
	&= MGF(\frac{(n-1)S^2}{\sigma^2})* MGF(\chi^2_{(1)})\\
	\implies MGF(\frac{(n-1)S^2}{\sigma^2}) &=  \frac{MGF(\chi_{(n)}^2)}{MGF(\chi^2_{(1)})}\\
	&= \frac{(1-2t)^{-\frac{n}{2}}}{(1-2t)^{-\frac{1}{2}}} \\
	&= (1-2t)^{-\frac{n-1}{2}} \\
\end{align*}
which is the MGF of $\chi^2_{(n-1)}$\qed
\paragraph{E\&R theorem 4.6.2}
 $X_1, X_2,..., X_n \sim N(\mu,\sigma ^2)i.i.d., $, U and V are two different linear combinations of the $X_i$'s, then \newline
 $Cov(U,V) = 0 \iff$ U and V are independent.
 \paragraph{Note} In general, zero covariance doesn't imply independent \newline
 Example: $X \sim N(0,1), Y = X^2$, clearly X and Y are dependent, but 
 \begin{align*}
 Cov(X,Y) &= E[XY] - E[X]E[Y] \\
 &=E[X^3]-0\cdot E[Y]\\
 &=E[X^3]\\
 &=\int x^3f(x)dx\\
 &=0 \tag{since $x^3f(x)$ is centro-symmetric}
 \end{align*}
 \paragraph{Unbiasedness of $S^2$ using the Chi-sq distribution}
 $$E[\frac{(n-1)S^2}{\sigma^2}] = n-1$$
 $$\implies E[S^2] = \sigma^2$$
 This proves $S^2$ is an unbiased estimator for $\sigma^2$ under Normal distribution \newline
 There's another way to prove it under any arbitrary distribution with the assumption that $X_i$'s are i.i.d. and $\mu, \sigma^2$ exists.
 \section{Some relationships among distributions}
 \begin{enumerate}
 	\item $\frac{\bar X - \mu}{S/\sqrt{n}} \sim t_{(n-1)}$
 	\item $\frac{\chi^2_{(m)}}{m} \overset{P}{\rightarrow}1$
 \end{enumerate} 
 \section{Difference between sample variance and variance of sample mean}
 \paragraph{variance of sample mean:} Expectation of squared difference of sample mean from the \under{true mean}
 \paragraph{sample variance:} average squared difference of each data points in the sample from the \under{sample mean}
 \section{Consistent Estimator}
 \paragraph{Definition}
 Let $T_n$ be an estimator of parameter $\theta$, $T_n$ is said to be consistent (in probability) if $$T_n \overset{P}{\rightarrow} \theta$$
 In words, $T_n$ converges to $\theta$ in probability.
 \paragraph{Note}
 If $T_n \overset{a.s.}{\rightarrow}\theta$ then $T_n$ is called consistent (almost surely). In this course we will only talk about consistent (in probability)
 \paragraph{Proving consistency using LLN}
 LLN tells us, $\bar X = \frac{1}{n}\sum X_i \overset{P}{\rightarrow} E[X_i]$ for any distribution. Immediately that tells us:
 \begin{enumerate}
 	\item If $X_i \overset{iid}{\sim} N(\mu,\sigma^2)$ then $\bar X$ is a consistent estimator of $\mu$
 	 \item If $X_i \overset{iid}{\sim} Poisson(\lambda)$ then $\bar X$ is a consistent estimator of $\lambda$
 	 \item And we can say this for few other known distributions
 \end{enumerate}
 Goal: prove consistency when the estimator is not simply $\bar X$
 Still use LLN but with the help of a well known Lemma and the continuous mapping theorem
 \paragraph{Slutsky's Lemma}
 We have two different sequence $X_n$ and $Y_n$ \newline
 If $X_n \overset{P}{\rightarrow}X \mbox{ and } Y_n \overset{P}{\rightarrow}Y$, then $X_n + Y_n \overset{P}{\rightarrow} X + Y$ \newline
 If $X_n \overset{P}{\rightarrow}X \mbox{ and } Y_n \overset{P}{\rightarrow}Y$, then $X_nY_n \overset{P}{\rightarrow} XY$
 \paragraph{Continuous mapping theorem}
 Let $X_n \overset{P}{\rightarrow} X$ and $g()$ be a continuous function, then $g(X_n) \overset{P}{\rightarrow} g(X)$
 \paragraph{Proving $S^2$ is a consistent estimator of $\sigma^2$}
 $\hdots$
 \paragraph{MSE consistent}
 An estimator $T_n$ is called \under{MSE consistent} if
 $$MSE(T_n) \rightarrow 0 \mbox{ as } n \rightarrow \infty$$
 Example: for $N(\mu,\sigma^2)$
 $MSE(\bar X) = \sigma^2 / n \rightarrow 0 \mbox{ as } n \rightarrow \infty$
 Therefore $\bar X$ is a MSE consistent estimator of $\mu$ \newline
 In naive words, after you have calculated the MSE of an estimator, just check if it goes to zero for large $n$
 \paragraph{Note}
 \textcolor{red}{MSE consistent $\implies$ consistent (in probability)}
 \section{Efficient Estimator}
 \paragraph{Definition of Efficiency}
 Let $T_1$ and $T_2$ be two different estimators of $\theta$, \under{Efficiency of $T_1$ relative to $T_2$} is defined as 
 $$eff(T_1,T_2) = \frac{var[T_2]}{var[T_1]}$$
 \paragraph{Remark}
 \begin{enumerate}
 	\item $eff(T_1, T_2) > 1 \implies T_1$ has smaller variance $\implies T_1$ is more efficient
 	\item This comparison is meaningful when $T_1$ and $T_2$ are both unbiased or both have the same bias.  
 \end{enumerate}
 \paragraph{Lower bound of the variance of an unbiased estimator}
 This famous inequality provides a lower bound for the variance of all the unbiased estimators. In other words it gives a lower bound of the MSE (since Bias = 0). The estimator whose variance achieves this lower bound is said to be efficient. Before we state the inequality let's define few terms...
 \paragraph{Score function, $S(\theta)$}
 The derivative of the log-likelihood $$S(\theta) = \frac{\partial l(\theta)}{\partial \theta}$$
 For the random variable X, $S(\theta| X = x) = \frac{\partial}{\partial \theta} \ln f_\theta(x)$. For an observed i.i.d sample, it's written as $S(\theta|x_1,x_2,\hdots,x_n)$ with $$S(\theta|x_1,x_2,\hdots,x_n) = \frac{\partial}{\partial \theta} \sum_i \ln f_\theta(x_i) = \sum_i \frac{\partial}{\partial \theta} \ln f_\theta(x_i) = \sum_i S(\theta|x_i)$$
 \paragraph{Fisher Information, $I(\theta)$}
 The function $$I(\theta) = var_\theta[S(\theta|X)]$$ It's the amount of information that each observable random variable X contains about $\theta$. \newline
 Information of a sample of size $n = var[S(\theta|x_1,x_2,\hdots,x_n)] = nI(\theta)$
 \paragraph{A plot showing the randomness of $S(\theta)$}
 The likelihood function looks different for different data!
 \paragraph{One important property of $S(\theta)$}
 Under some assumptions, $$E[S(\theta|X = x)] = 0$$
 Which implies $$E[S(\theta|x_1,x_2,\hdots,x_n)] = \sum_i E[S(\theta|x_i)] = 0$$
 \paragraph{Cramer-Rao Inequality}
 Let $X_1, X_2, \hdots, X_n$ be i.i.d. with density $f_\theta(x)$,
 $T(X_1, X_2, \hdots, X_n)$ be an unbiased estimator of $\theta$,
 Then under some assumptions on $f_\theta(x)$, $$var[T] \geq \frac{1}{nI(\theta)}$$
 $\frac{1}{nI(\theta)}$ is also known as the Cramer-Rao lower bound (CRLB)
 \paragraph{Proof of Cramer-Rao Inequality}
 	$\hdots$
 \paragraph{Definition of sufficient statistic}
 A statistic $T(X_1, X_2, \hdots, X_n)$ is said to be \under{sufficient} for $\theta$ if the conditional distribution of $X_1, X_2, \hdots, X_n$, given $T = t$, does not depend on $\theta$\\
 \end{document}