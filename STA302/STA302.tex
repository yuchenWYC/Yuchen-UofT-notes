\documentclass[11pt]{article}
% Libraries.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage{float}
\usepackage{esint}
\usepackage{graphicx}
\graphicspath{ {./images/} }

% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}

% Commands
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\bx}[0]{\mathbf{x}}
\newcommand{\bv}[0]{\mathbf{v}}
\newcommand{\bw}[0]{\mathbf{w}}
\newcommand{\real}[0]{\mathbb{R}}
\newcommand{\under}[1]{\underline{#1}}
\newcommand{\proof}[0]{\textit{\underline{proof:} }}
\newcommand{\func}[3]{\tb{#1}: {#2} \rightarrow {#3} }
\newcommand{\vx}[0]{\tb{x}}
\newcommand{\vy}[0]{\tb{y}}
\newcommand{\vz}[0]{\tb{z}}
\newcommand{\vo}[0]{\tb{0}}
\newcommand{\va}[0]{\tb{a}}
\newcommand{\vb}[0]{\tb{b}}
\newcommand{\vc}[0]{\tb{c}}
\newcommand{\ve}[0]{\tb{e}}
\newcommand{\vm}[0]{\tb{m}}
\newcommand{\vh}[0]{\tb{h}}
\newcommand{\vf}[0]{\tb{F}}
\newcommand{\vi}[0]{\tb{i}}
\newcommand{\vj}[0]{\tb{j}}
\newcommand{\vk}[0]{\tb{k}}
\newcommand{\vg}[0]{\tb{G}}
\newcommand{\vn}[0]{\tb{n}}
\newcommand{\vu}[0]{\tb{u}}
\newcommand{\vL}[0]{\tb{L}}
\newcommand{\ff}[0]{\tb{f}}
\newcommand{\fg}[0]{\tb{g}}
\newcommand{\rational}[0]{\mathbb{Q}}
\newcommand{\p}[0]{\partial}
\newcommand{\qed}[0]{$\hfill\blacksquare$}
\newcommand{\qerat}{\tag*{$\blacksquare$}}
\newcommand{\lima}{\underset{\vx \rightarrow \va}{\lim}}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}


% Attr.
\title{Methods of Data Analysis \\ -- STA302 Summer 2019 Course Notes}
\author{\textcolor{blue}{\href{https://www.yuchenwyc.com}{Yuchen Wang}}}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage

\section{May 7th - Introduction, p-values and statistical significance}
\paragraph{Definition 1.1 - Statistical Analysis} 
Data Analysis that relies on Probability theory to account for the variability of the data.
\paragraph{Permutation Test 1.2}
Insert random premise, observe two samples Group A and Group B. \\
If the groups have no effect, all of the permutations are equally likely. \\
We can plot the Permutation Distribution with respect to difference between sample means.
\paragraph{Characteristics of Permutation Test 1.3}
\begin{enumerate}
	\item Involves simple probability theory
	\item distribution-free
	\item listing all the permutation for large dataset is almost impossible
\end{enumerate}

\paragraph{Definition 1.4 - Statistical Significance}
We say a difference is \textbf{statistically significant} if it's less probable than our pre-determined significance level. (when p-value $p <$  significance level $\alpha$)
\paragraph{Definition 1.5 - Significant Effect} We say the groups have a \textbf{significant effect} if it causes the variable of interest to be significantly different.

\section{May 9th - Hypothesis testing, t-test and ANOVA}
\subsection{The basics}
\paragraph{Fact 2.1.1} If $H_0$ is true, the p-value $\sim U(0,1)$ \\\\
\under{\ti{remarks}}: This is saying that if p-value is greater than significance level, then it does not say anything about our confidence, it's just a value. Proof can be found online.
\paragraph{Tradeoff Between Type I and Type II Error}
It's common to fix $\alpha$ (significance level or type-I error) and minimize type-II error.
\subsection{t-test}
Under the model $(y_i|X=xi) \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$, an unbiased estimator of $\sigma^2$ is 
$$S^2 = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-2} $$
Then
$$ \frac{\hat{\beta_1} - \beta_1}{\sqrt{\frac{S^2}{SXX}}} \sim t_{(n-2)}$$


\subsection{One-way Analysis of variance (ANOVA)}
Suppose the response $Y$ is quantitative and the predictor $X$ is categorical, taking $t$ values or levels denoted $1, \hdots, t$. With the regression model, we assume that the only aspect of the conditional distribution of $Y$, given $X = x$, that changes as $x$ changes, is the mean. \\
Suppose we are interested in assessing whether or not there is a relationship between the response and the predictor. There is no relationship if and only if all the conditional distributions are the same. This is true under our assumptions if and only if all the means are equal.
In our case, one-way ANOVA is an extension of the t-test to 3 or more samples focus analysis on group differences.\\
$H_0$: All groups are the same.
If groups are different, we expect there is a bigger difference between groups (\textcolor{blue}{the group effect}) than within groups (\textcolor{blue}{natural variability of the data}).
\paragraph{Basic Definitions}
Suppose we have $T$ groups and $n_t$ observations for the $t$-th group, and we denote each observation as $y$.
\begin{enumerate}
	\item \under{SST}: This is the sum of the squared deviations between each observation and the overall mean:
	$$SST = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y})^2$$
	\item \under{SSE}: This is the sum of the squared deviations between each observation and the mean of the group to which it belongs:
	$$SSE = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y_t})^2$$
	\item \under{SSG}: This is the sum of the squared deviations between each group mean and the overall mean:
	$$SSG = \sum_{t=1}^T\sum_{i=1}^{n_t}(\bar{y_t}-\bar{y})^2$$
\end{enumerate}

\paragraph{Sum of Squares Decomposition}
Total sum of squares = Within group sum of squares + Between group sum of squares \\
$$\sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y})^2 = \sum_{t=1}^T\sum_{i=1}^{n_t}(y_{i,t}-\bar{y_t})^2 + \sum_{t=1}^T\sum_{i=1}^{n_t}(\bar{y_t}-\bar{y})^2$$
In shorthand:
$$SST = SSE + SSG$$
\proof\\
add $-\bar{y_t}+\bar{y_t}$ inside the squared error term and everything is just like a short proof in STA261, nothing interesting. \qed
\paragraph{ANOVA}
We want to assess how large is SSG relative to SSE, but it would be hard to establish a distribution for SSG/SSE. Knowing a sum of squares divided by its degrees of freedom has a chi-square distribution, we can conclude that
$$SSG/(T-1) \sim \chi_{T-1}^2, \quad SSE/(n-T) \sim \chi_{n-T}^2$$
\paragraph{Theorem 2.2.1} If between-groups and within-groups variance are equal ($\sigma_T = \sigma_\varepsilon$), then $\frac{SSG/(T-1)}{SSE/(n-T)} \sim F_{T-1, n-T}$ \\\\
\proof \\
In STA261, we've proven that if $\sigma_x^2 = \sigma_y^2$, then $\frac{\hat{\sigma}_x^2}{\hat{\sigma}_y^2} \sim F_{n-1,n-1}$.\\ Since SSG/(T-1) is an estimation for the variation between groups ($\sigma_T$) and $SSE/(n-T)$ is an estimation for the variation within groups ($\sigma_\varepsilon$), then the result follows. \qed
\paragraph{Remarks 2.2.2} Thus a small p-value indicates theses variances are different, which is evidence for the existence of some group effect.
\paragraph{Theorem 2.2.3 One-way ANOVA Table} if p-value $< \alpha$, we reject $H_0$: groups have no effect. \\
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Source & Sum of Squares & df & Mean Squares & Test Statistic \\
		\hline
		Between & SSG & $T-1$ & $MSG = \frac{SSG}{T-1}$ & $F = \frac{MSG}{MSE}$ \\
		\hline
		Within & SSE & $n - T$ & $MSE = \frac{SSE}{n - T}$ &\\
		\hline
		Total & $SST$ & $ n -1$ &&\\
		\hline
	\end{tabular}
\section{May 14th - Linear Regression: Least Square Error Formulation}
\subsection{Matrix Notation}
$\vx = \begin{bmatrix}
	x_1 \\ x_2
\end{bmatrix}$ is a random variable. \\In addition, $A = \begin{bmatrix}
	a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
\end{bmatrix}$, $\vc = \begin{bmatrix}
	c_1\\c_2\\c_3
\end{bmatrix}$.\\
Then $$E[\vx] = \mu = \begin{bmatrix}
	\mu_1 \\ \mu_2
\end{bmatrix}$$
$$Var[\vx] = \Sigma = \begin{bmatrix}
	\sigma^2_1 & \sigma^2_{12} \\ \sigma^2_{12} & \sigma^2_2
\end{bmatrix}$$
where $\sigma^2_{ij} = cov(x_i, x_j)$.
\paragraph{Theorem 3.1.1}
Let $\vz = A\vx + \vc$. Then $$E[\vz] = A\mu + \vc$$
$$Var[\vz] = A\Sigma A^T$$
\subsection{Linear Regression}
We have a vector of $n$ predictors $\vx = [x_1,\hdots,x_n]$, as well as $n$ associated response variables $\vy = [y_1, \hdots, y_n]$. We want to estimate the parameters $\beta_0$ and $\beta_1$ that best fit the model $y = \beta_0 + \beta_1 x$. (In matrix notation: $\vy = X\beta$ where $\vy = \begin{bmatrix}
	y_1 \\ \vdots \\ y_n \end{bmatrix}, X = \begin{bmatrix}
		1 & x_1 \\ \vdots & \vdots \\ 1 & x_n
	\end{bmatrix}, \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}$).
\subsubsection{Least Square Estimation}
Minimize sum of squared errors: $$\sum_{i=1}^n (\beta_0 + \beta_1 x_i - y_i)^2$$
We take derivative of $\sum_{i=1}^n (\vy - X\hat{\beta})^2$ wrt $\hat{\beta}$, set this to 0 and get 
\paragraph{Theorem 3.2.1.1}$$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
$$\hat{\vy} = X\hat{\beta} = X(X^TX)^{-1}X^T\vy$$
\paragraph{Remark 3.2.1.2}
$X(X^TX)^{-1}X^T$ is called \under{the hat matrix} since it puts the hat on $\vy$. This matrix $(H)$ has the following properties:
\begin{enumerate}
    \item $H^T = H$
    \item $HH = H$
    \item $HX = X$
\end{enumerate}
\subsubsection{ANOVA}
Estimate how good a linear regression model is.
\paragraph{Basic Definitions}
$\bar{y}$ is called base estimation.
\begin{enumerate}
	\item \under{SST}: This is the sum of the squared deviations between each observation and the mean:
	$$SST = \sum_{i=1}^n(y_{i}-\bar{y})^2$$
	\item \under{SSE}: This is the sum of the squared deviations between each observation and the corresponding prediction
	$$SSE = \sum_{i=1}^n(\hat{y_i} - y_i)^2$$
	\textcolor{blue}{unexplained variation: How much our explanation is away from the true observation?}
	\item \under{SSG}: This is the sum of the squared deviations between each prediction and the mean.
	$$SSG = \sum_{i=1}^n(\hat{y_i}-\bar{y})^2$$
	\textcolor{blue}{explained variation: How much our explanation takes us away from the base prediction?}
\end{enumerate}
\paragraph{Coefficient of Determination}
$$R^2 = \frac{SSG}{SST} = 1- \frac{SSE}{SST}$$
($0 \leq R^2 \leq 1$)\\
The closer $R^2$ is from 1, the better the fit is.
\section{May 16th - Linear Regression: Maximum Likelihood Formulation}
\subsection{the Linear Regression Model}
\paragraph{Definition 4.1.1}
The \under{best linear unbiased estimator (BLUE)} is the unbiased estimator with the lowest variance.
\paragraph{Gauss-Markov Assumptions 4.1.2}
If $E[e_i] = 0, Cov(e_i, e_j) = 0 \, \forall i \neq j$ and $Var(e_i) = \sigma^2 < \infty \, \forall i$, then \textcolor{red}{the best linear unbiased estimator for $\beta$'s are given by minimizing the MSE}
\paragraph{the Linear Regression Model}
$$y_i = \beta_0 + \beta_1x_i + e_i$$
where $e_i$ is a random variable that represents the residual. \\
\paragraph{Assumptions}
\begin{enumerate}
    \item $e_i \overset{i.i.d.}{\sim} N(0, \sigma^2)$ which follows the Gauss-Markov assumptions. 
    \item $(\vy|\vx) \sim N(X\beta, I\sigma^2)$ or $(Y|X = x) \sim N(\beta_0 + \beta_1x, \sigma^2)$
\end{enumerate}
\subsection{Maximum Likelihood Estimation}
$$l(\beta | \vx) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} (\vy - X \beta)^T(\vy - X\beta)$$
Maximizing this term wrt $\beta$ is equivalent to minimizing $(\vy - X\beta)^T(\vy-X\beta)$, which gives $$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
\paragraph{Corollary 4.2.1}
Minimizing MSE and the likelihood function leads to the same estimate $\hat{\beta}$.
\paragraph{A Biased Estimator of $\sigma^2$}
$$l(\beta | \vx) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2} (\vy - X \beta)^T(\vy - X\beta)$$
Maximizing the likelihood function wrt to $\sigma^2$:
$${\hat{\sigma}}^2 = \frac{(\vy - X\hat{\beta})^T(\vy-X\hat{\beta})}{n} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}$$
This is MLE, a biased estimator of $\sigma^2$. \\
The unbiased estimator of $\sigma^2$ is \textcolor{red}{$$\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n-2}$$}
\subsection{Inference}
The action of extracting information about parameters given a dataset.\\
\paragraph{Mean and Variance of $\vy$}
Since $\vy \sim N(X\beta, I\sigma^2$, then $E[\vy] = X\beta$ and $Var[\vy] = I\sigma^2$.
\paragraph{Mean and Variance of $\hat{\beta}$}
\begin{align*}
    E[\hat{\beta}] &= E[(X^TX)^{-1}X^T\vy]\\
    &= (X^TX)^{-1}X^TE[\vy]\\
    &= (X^TX)^{-1}X^TX\beta \\
    &= \beta
\end{align*}
$\implies$ $\hat{\beta}$ is an unbiased estimator of $\beta$.
\begin{align*}
    Var[\hat{\beta}] &= Var[(X^TX)^{-1}X^T\vy]\\
    &= (X^TX)^{-1}X^T \, Var[\vy|X]((X^TX)^{-1}X^T)^T\\
    &= (X^TX)^{-1}X^T \, I\sigma^2 ((X^TX)^{-1}X^T)^T \\
    &= \sigma^2 (X^TX)^{-1}X^T((X^TX)^{-1}X^T)^T \\
    &= \sigma^2 (X^TX)^{-1}X^T(X((X^TX)^{-1})^T)\\
    &= \sigma^2 (X^TX)^{-1}X^T(X((X^TX)^{T})^{-1})\\
    &= \sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}\\
    &= \sigma^2 (X^TX)^{-1}
\end{align*}
\paragraph{Theorem 4.3.0.1} $$\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$$
\proof\\
$\hat{\beta} = (X^TX)^{-1}X^T\vy$, so $\hat{\beta}$ is a linear combination of normal r.v.'s($y_i$'s), therefore $\hat{\beta}$ follows normal distribution with mean and variance we have calculated. \qed
\subsubsection{Inference for $\beta_1$}
$$\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{SSX})$$
where $SSX = \sum_{i=1}^n(x_i - \bar{x})^2$
Then $$\frac{\hat{\beta_1} - \beta_1}{\sigma/\sqrt{SSX}} \sim N(0,1)$$
\paragraph{Theorem 4.3.1.1}
$$\frac{(n-2)S^2}{\sigma^2} \sim \chi^2_{(n-2)}$$
where $S^2 = \frac{1}{n-2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)^2$
\paragraph{Theorem 4.3.1.2}
$$\frac{\hat{\beta_1} - \beta_1}{S/\sqrt{SSX}} \sim t_{n-2}$$
where $SSX = \sum_{i=1}^n(x_i - \bar{x})^2$
\paragraph{Model Checking}
$H_0$: $\beta_1 = 0$
Then under $H_0$, Theorem 4.3.1.2 applies. 
\begin{enumerate}
    \item If the p-value is small, then $\vy$ and $\vx$ are statistically significant.
    \item 0.95 confidence level for $\beta_1$:
    $$(\hat{\beta_1} - t_{(n-2)(1-\frac{\alpha}{2})}\frac{S}{\sqrt{SSX}}, \hat{\beta_1} + t_{(n-2)(1-\frac{\alpha}{2})}\frac{S}{\sqrt{SSX}})$$
\end{enumerate}
\subsubsection{Inference for $\beta_0$}
$$\hat{\beta_0} \sim N(\beta_0, \sigma^2\frac{\sum x_i^2}{n\,SSX})$$

\section{May 23th - Diagnostic for the linear regression model}
\paragraph{Review of the model}
$$y_i = \beta_0 + \beta_1 x_i + e_i$$ where $e_i \sim N(0, \sigma^2)$
$$\vy = X\beta + \ve \implies \vy|X \sim N(X\beta, I\sigma^2)$$
where $$\hat{\beta}_{MLE} = (X^TX)^{-1}X^T\vy$$
$$\hat{\beta} \sim N(\beta, \sigma^2(X^TX)^{-1})$$
\paragraph{Notes}
\textcolor{red}{In all cases, we have $\sum_{i=1}^n e_i$ = 0.}

\subsection{Predictive Inference}
Since $\hat{\vy} = X\hat{\beta}$, then
$$\hat{\vy} \sim N(X\beta, \sigma^2X(X^TX)^{-1}X^T)$$
\paragraph{Prediction}
For a new, unobserved predictor $x^*$, a simple prediction for the response could be $$y^* = \beta_0 + \beta_1x^*$$
\paragraph{Predictive Distribution}
We have $\hat{\beta} = [\hat{\beta}_0, \hat{\beta}_1]^T$, then
$$\hat{y}^* = \hat{\beta}_0 + \hat{\beta}_1x^*$$
Matrix notation:
$$\hat{\vy}^* = X^*\hat{\beta}$$
where $X^*$ is a vector of new observation $\vx^*$ added a column of 1s. \\
then
$$\hat{\vy}^* \sim (X^*\beta, \sigma^2X^*(X^TX)^{-1}X^{*T})$$
\paragraph{Confidence Interval for $X^*\beta$}
$$\frac{\hat{\vy}^*-X^*\beta}{\sigma\sqrt{X^*(X^TX)^{-1}X^{*T}}} \sim N(0,I)$$
Then $$0.95 CI = \hat{y}^*_i \pm 1.96 * \sigma \sqrt{[X^*(X^TX)^{-1}X^{*T}]_{ii}}$$
\paragraph{Remarks}
The confidence interval reflects our uncertainty about the population regression line
\paragraph{the Prediction Error}
$$y^* - \hat{y}^* = \beta_0 + \beta_1x^* + \varepsilon^* - (\hat{\beta}_0 + \hat{\beta}_1x^*)$$
Matrix notation:
$$\vy^* - \hat{\vy}^* = X^*\beta + \varepsilon - X^*\hat{\beta}$$
Distribution:
$$\vy^* - \hat{\vy}^* \sim N(\mu, \Sigma)$$ where
\begin{align*}
    \mu &= E[X^*\beta + \varepsilon - X^*\hat{\beta}]\\
    &= X^*\beta + E[\varepsilon] - E[X^*\hat{\beta}]\\
    &= X^*\beta + 0 - X^*\beta \\
    &= 0 \\
    \Sigma &= Var[X^*\beta + \varepsilon - X^*\hat{\beta}] \\
    &= Var[\varepsilon - X^*\hat{\beta}]\\
    &= Var[\varepsilon] + Var[X^*\hat{\beta}] + 2Cov[\varepsilon, X^*\hat{\beta}] \\
    &= \sigma^2I + \sigma^2X^*(X^TX)^{-1}X^{*T} + 0\\
    &= \sigma^2[I + X^*(X^TX)^{-1}X^{*T}]
\end{align*}
Then we can construct a CI for $\vy^*$ using t-distribution ($df = n-2$)
\paragraph{\textcolor{red}{Remarks}}
Prediction intervals reflect uncertainty from both $\hat{\beta}$ and $\varepsilon$ (i.e. the irreducible error). The irreducible error is estimated using training sample.
\paragraph{\textcolor{red}{Reducible and irreducible errors}}
\begin{enumerate}
	\item \under{Reducible error} is the error arising from the mismatch between $\hat{f}$ and $f$. $f$ is the true relationship between $X$ and $Y$, but we can't see $f$ directly - we can only estimate it. We can reduce the gap between our estimate and the true function by applying improved methods.
	\item \under{Irreducible error} arises from the fact that X doesn't completely determine Y. That is, there are variables outside of X - and independent of X - that still have some small effect on Y. The only way to improve prediction error related to irreducible error is to identify these outside influences and incorporate them as predictors.
\end{enumerate}
\subsection{Checking the Model Assumption}
We will divide model checking into 3 pieces:
\begin{enumerate}
    \item Error assumption ($e_i \overset{i.i.d}{\sim} N(0,  \sigma^2)$)
    \item Identical distribution (checking for unexpected observations)
    \item Model assumption (linearity)
\end{enumerate}
\subsubsection{Checking Error Assumption}
We only have access to the residuals (observed errors)
$$\hat{e}_i = y_i - \hat{y}_i$$
$$\hat{\ve} = (I - H)\vy$$
\paragraph{Constant variance}
Check $Var(e_i) = \sigma^2 \, \forall i$\\
Plot the residuals against the fitted values($\hat{y}_i$)
% \paragraph{Zero mean/ Independence}
% Plot the residuals against the predictors, see if clustered around zero and looks random
\paragraph{Normality of residuals}
Quantile to Quantile plot (QQplot)
\paragraph{Uncorrelatedness / Independence}
\begin{enumerate}
    \item Scatter plot (y against x)
    \item Residual plot against predictors (see if clustered around zero and looks random)
    \item Residual Sequence plot
\end{enumerate}
\paragraph{Remarks}
We rarely check for this assumption

\subsubsection{Unusual Observations}
\paragraph{Definition 5.2.2.1 - Leverage points}
A \under{leverage point} is a point whose $x$-value is distant from the other $x$-values
\paragraph{leverages}
In the linear regression model, the \under{leverage} for the $i$-th observation is defines as:
$$h_i = H_{ii} = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{j=1}^n(x_j-\bar{x})^2}$$
where $H = X(X^TX)^{-1}X^T$ \\
Property: $\sum_{i=1}^n h_i = 2$ (number of parameters)
\paragraph{Remarks}
The average value for $h$ is $2/n$. Usually leverages larger than $4/n$ should be looked at more closely.
\paragraph{Definition 5.2.2.2 - Outliers}
An \under{outlier} is a data point whose $y$-value differs significantly from other observations.
\paragraph{Remarks}
Usually large residual $\hat{y}_i - y_i$ might indicate outliers
\paragraph{Definition 5.2.2.3 - Influential observations}
An \under{influential point} is one whose removal from the dataset would cause a large change in the fit. They could be leverage points, outliers but usually the both.
\paragraph{Remarks}
An outlier with a large leverage will definitely be an influential observation. It is sometimes called a \under{bad leverage}
\paragraph{Definition 5.2.2.4 - Cook's Distance}
For observation $i$, the \under{Cook's distance} is $$D_i = \frac{r_i^2}{2} \frac{h_i}{1-h_i}$$ where $r_i$ is the standardized residual and $h_i$ is the leverage.
\paragraph{Remarks}
$r_i$ measures the extent of outlying, $h_i$ measures the leverage. Thus, a large value of $D_i$ indicates influential observations.
\paragraph{Simple rules of thumb} There is a problem when
\begin{enumerate}
    \item $D_i > 4/n$ on large datasets
    \item $D_i > 1$ on small datasets
    \item $D_i$ is separated by a large gap from the other $D_j$s
\end{enumerate}

\section{May 28th - Dummy variables and introduction to multiple linear regression}
\subsection{Transformation}
We can use transformations to fix 2 problems:
\begin{enumerate}
	\item Non-constant variance
	\item Non-linearity
\end{enumerate}
When the \textcolor{blue}{variance is exploding}, typically we raise $y$ to a power between 0 and 1 or apply a logarithmic transformation.
\paragraph{Logarithmic Transformation}
$$\log(y_i) = \beta_0 + \beta_1 x_i + e_i$$
$$y_i = \exp(\beta_0)\exp(\beta_1x_i)\exp(e_i)$$
\paragraph{Box-Cox Transformation}
Let's consider a family of possible transformations $g_\lambda(y)$
$$g_\lambda(y)=\begin{cases}
	\frac{y^\lambda-1}{\lambda} & \text{if $\lambda \neq 0$} \\
	\log(y) & \text{if $\lambda = 0$}
\end{cases}$$
$\lambda$ is selected by the model achieving the highest log-likelihood:
$$g_\lambda(y_i) \sim N(\beta_0 + \beta_1x_i, \sigma^2)$$
$$l(\lambda|\vy) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n(g_\lambda(y_i) - \beta_0 - \beta_1x_i)^2$$
\paragraph{Conclusions}
\begin{enumerate}
	\item Transforming the response (or the predictors) might help with violated assumptions
	\item It makes the model less interpretable.
\end{enumerate}
\subsection{Multiple Linear Regression}
Suppose we have $p$ predictors.
$$y = \beta_0 + \beta_1x_1 + \hdots + \beta_p x_p + e$$ where $e \sim N(0, \sigma^2)$
$$y = \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix},
X = \begin{bmatrix}
	1 & x_{11} & \hdots & x_{1p} \\
	\vdots & \vdots & \hdots & \vdots \\
	1 & x_{n1} & \hdots & x_{np}
\end{bmatrix}, \beta = \begin{bmatrix}
	\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{bmatrix}$$
Matrix notation:
$$\vy = X\beta + \ve$$ where $\ve \sim N(0, I\sigma^2)$ (a vector of independent normal variables)
Therefore,
$$ \vy \sim N(X\beta, I\sigma^2)$$
\paragraph{Inference}
$$l(\theta|x_i) = -\frac{n}{2} \log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}(\vy - X\beta)^T(\vy - X\beta)$$
which leads to $$\hat{\beta} = (X^TX)^{-1}X^T\vy$$
\subsection{Dummy variables}
A set of binary variables to represent a categorical variable.
Allows to fit a parameter for every possible categories of a categorical variable.
\paragraph{Model - 2 groups}
Fit a linear regression:
$$y = \beta_0 + \beta_1x + e$$ where $e \sim N(0, \sigma^2)$
$$y \sim N(\beta_0 + \beta_1 x, \sigma^2)$$
This is a model that consists only \textcolor{red}{two different intercepts}, no slope.
\paragraph{Inference}
$$E(y) = \begin{cases}
	\beta_0 & \text{if $x = 0$} \\
	\beta_0 + \beta_1 &\text{if $x = 1$}
\end{cases}$$
$$H_0: \beta_1 = 0$$
Applying t-test, this is exactly same to lecture 2.
\paragraph{Model - 3 groups}
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + e$$
where $e \sim N(0, \sigma^2)$
\paragraph{Inference}
$$E(y) = \begin{cases}
	\beta_0 & \text{for Group A} \\
	\beta_0 + \beta_1 &\text{for Group B} \\
	\beta_0 + \beta_2 &\text{for Group C}
\end{cases}$$
$H_0: \beta_1 = \beta_2 = 0$ for testing if all groups are the same; $H_0: \beta_1 = 0$ for testing if Group B is same as Group A.

\section{May 30th - Interactions and multiple linear regression assumptions}
\subsection{Interactions}
\paragraph{Definition} \under{Interaction} is the effect of predictor $x_1$ on the effect of predictor $x_2$ on $y$. With the interaction term, the linear regression model becomes
$$y = \beta_0 + \beta_1x_1 + \beta_2 x_2 + \beta_{1,2}x_1x_2 + e$$
where $e \sim N(0,1)$
\paragraph{Remarks}
As $x_1$ varies, 
\begin{enumerate}
	\item the effect of $x_2$ on $y$ is different.
	\item the relationship between $x_2$ and $y$ is different.
\end{enumerate}
Interaction between two categorical predictors $\implies$ every combination are categories with respective effects\\
Interaction between a categorical predictor and a numerical predictor $\implies$ different intercepts and different slopes\\
\paragraph{Example: numerical and categorical}
Suppose $x_1$ is numerical, $x_2$ is categorical.\\
Then for a fixed $x_1$, 
$$E(y) = \begin{cases}
	\beta_0 + \beta_1x_1 & \text{if $x_2 = 0$} \\
	(\beta_0 + \beta_2) + (\beta_1 + \beta_{1,2}x_2) & \text{if $x_2 = 1$}
\end{cases}$$
Now $\beta_2$ indicates difference in intercept and $\beta_{1,2}$ indicates difference in slope (interaction)
\paragraph{Example: categorical and categorical}
$\beta_{1,2}$ allow for a different effect from change of predictor 1 depending on the value of predictor 2.
\paragraph{Example: numerical and numerical}
More complicated. \\
The slope and intercept of $x_1$ is different across different values of $x_2$.
\paragraph{Remarks}
The model is completely wrong without the interaction term.But interaction terms make the number of parameters explode quickly. ($p$ predictors $\implies {p \choose 2}$ interaction terms)

\subsection{Polynomial fit}
\begin{enumerate}
	\item Can be understood as a special case of interactions. 
	\item Can also solve the issue of observable pattern in the residuals
\end{enumerate}
$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

\subsection{Model Checking}
\subsubsection{Collinearity}
\paragraph{Correlation}
$$\rho_{X,Y} = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}$$
range: $[-1, 1]$.\\
when $\rho = 1/-1$, we call it "perfect correlation".
\paragraph{Theorem}
$\rho = \pm 1$ iff $P(Y = a + bX) = 1$ for some constants $a$ and $b$. \\
\proof \\
Rice p143.
\paragraph{Collinearity}
If two variables are perfectly correlated, it implies a perfect increasing(decreasing) linear relationship ($x_i = a+ bx_j$). This implies that $\det(X) = 0 = \det(X^TX) = 0$.\\
$\implies X^TX$ not invertible \\
$\implies \hat{\beta} = (X^TX)^{-1}X^T\vy$ does not exist \\
$\implies$ perfect correlation should not happen. \\
We also do not want a correlation close to -1 or 1, since the variance of $\hat{\beta}$ would be extremely large.
\paragraph{Checking for collinearity}
\begin{enumerate}
	\item Look at the correlation matrix of the predictors. Large pairwise correlation indicates a problem.
	\item Build a regression model for $x_i$ as response on all other predictors to assess $R_i^2$. High $R_i^2$ (close to 1) indicates a problem.
	\item Look at the eigenvalues of $X^TX$. Small eigenvalues indicates a problem.
\end{enumerate}

\paragraph{Variance Inflating Factor (VIF)}
$$Var(\hat{\beta_j}) = \frac{1}{1-R_j^2} \times \frac{\sigma^2}{(n-1)SSX_j}$$
$\frac{1}{1-R_j^2}$ are defined as the VIF.

\paragraph{Remarks}
If the variable $X_j$ is uncorrelated then VIF = 1


\section{June 6th - Model selection and variable selection}
\subsection{Big Data}
\paragraph{Properties}
Big data usually includes datasets with sizes beyond the ability of commonly used software tools.
\begin{enumerate}
	\item Volume/Tall data: Large number of observations (large n)
	\item Wide data: Large number of predictors (large p)
	\item Variety: Multiple styles of data from texts to images to audio and video files.
	\item Velocity: The speed at which the data is generated.
\end{enumerate}
\paragraph{Challenges}
\begin{enumerate}
	\item Data storage
	\item Data analysis
	\item Data visualization
	\item Information privacy
\end{enumerate}

\subsubsection{Large number of predictors}
\paragraph{We have a large number of predictors when we}
\begin{enumerate}
	\item have a large number of parameters
	\item want to investigate many interaction terms and interaction of high order
	\item want to add multiple polynomial terms
\end{enumerate}

\paragraph{Why is it a problem}?
\begin{enumerate}
	\item It reduces interpretation \\
	"The simplest is best". it is easier to explain a simpler model. In order to get the big picture, we are willing to sacrifice small details.
	\item It increases the variance of the estimates
	$$ s^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2 / (n- (p+1))$$
	\item It is more prone to overfitting.\\
	More parameters increase the detriment of generalization abilities.
	\item It increases the chances of collinearity issues. 
	Too many similar information.
\end{enumerate}
\subsection{Overfitting}
\paragraph{Definition 8.2.1}
Overfitting happens when we detect a pattern in the data set that does not exist for new observations.
\paragraph{Definition 8.2.2}
Poor performances of the model on non-observed points.
\paragraph{Remarks}
We say that a model overfits when it offers poor generalization abilities. 
\paragraph{Performance Metric}
We can observe symptoms of overfitting by selecting a performance metric and comparing its value on the training set to its value on the test set as we change the model complexity.\\
Example:
\begin{enumerate}
	\item Mean Squared Error (MSE)
	\item Maximum Likelihood (MLE)
	\item $R^2$ coefficient
	\item log-likelihood
\end{enumerate} 
\subsection{Variable Selection}
We could select the set of variables that maximizes the likelihood or the $R^2$ coefficient. But to prevent overfitting, we penalizes for high number of parameters.
\paragraph{Adjusted $R^2$}
Recall that $$R^2 = \frac{SS_{reg}}{SST} = 1 - \frac{SSE}{SST}$$
$R^2$-adjusted includes a penalty per parameter:
$$Adjusted \, R^2 = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$$
\paragraph{Remarks}
A large Adjusted $R^2$ indicates a good improvement over $\bar{y}$.

\paragraph{Akaike information criterion (AIC)}
The AIC is a likelihood-based metric with a penalty for the number of parameters.
$$AIC = 2p - 2I$$
where $p$ = number of parameters and $I$ = log-likelihood of the current model.
\paragraph{Remarks}
The smaller the AIC, the better the model is.
\paragraph{Bayesian information criterion (BIC)}
The BIC is also a likelihood-based metric with a penalty for the number of parameters.
$$BIC = p\log{n} - 2I$$
where $p$ = number of parameters and $I$ = log-likelihood of the current model.
\paragraph{Remarks}
The smaller the BIC, the better the model is.
\paragraph{AIC vs BIC}
\begin{enumerate}
	\item BIC has a stronger penalty for the number of parameter.
	\item Sometimes BIC leads to selecting an underdeveloped model but AIC leads to selecting a model that overfits.
\end{enumerate}
\paragraph{Variable Selection}
If we have $p$ predictors, it would lead to $2^p$ different models. It might be reasonable to fit all of the $2^p$ models and select the one with the highest adjusted $R^2$ of lowest AIC/BIC.
\paragraph{Hierarchical models}
Start with only $x$, then include $x^2$ and compare the two models with one of our metrics. We can increase the order of the model as long as the adjusted $R^2$ (AIC/BIC) keeps increasing (decreasing) or as long as the added terms are significant.
\paragraph{Forward Selection}
\under{Forward Selection} is a stepwise subset technique that starts with the simplest model (no predictors) and sequentially add predictors to the model. At every step, for all predictors that are not in the model, we check their p-value if they were added to the model and add the predictor that would have \textcolor{red}{the smallest p-value}. We stop the process when $R^2$ (AIC/BIC) decreases (increases) or when all parameters were added to the model.
\paragraph{Backward Elimination}
\under{Backward elimination} start with all of the possible predictors. At every step, for all predictors in the model, we take out the predictor with \textcolor{red}{the largest p-value}. We stop the process when $R^2$ (AIC/BIC) decreases (increases) or when all parameters were taken out of the model.
\paragraph{Advantages}
These two technique share similar pros:
\begin{enumerate}
	\item They are easy to use
	\item They are intuitive
	\item They are computationally cheap (they are both a lot cheaper than looking through all subsets)
\end{enumerate}
\paragraph{Weaknesses}
\begin{enumerate}
	\item They don't use p-values appropriately
	\item By testing model sequentially we might stop before finding the best model
	\item The selection procedure disturbs inference and predicton
\end{enumerate}
\paragraph{Post-selection inference}
The selection process changes the properties of the estimators as well as the standard inferential procedures (such as tests and confidence intervals). The regression coefficients obtained after variable selection are biased.
\begin{enumerate}
	\item The p-values are usually much smaller.
	\item And the t-statistics and F-statistic can be misleading.
\end{enumerate}

\section{June 11th - Ridge and Lasso regression}
\subsection{Principal Component Analysis (PCA)}
A reparametrization of the current system in order to create uncorrelated predictors. \\
We project the predictors onto an orthogonal space.
\paragraph{Remarks}
\begin{enumerate}
	\item It completely solves the collinearity problem of any predictor set
	\item Reduces number of variables to keep the variance low and chances of overfitting low
	\item \textcolor{red}{It makes the interpretation even harder.}
	\item This transformation is extremely simple and fast
\end{enumerate}
\paragraph{Steps}
\begin{enumerate}
	\item Ideally we would like to do a projection that keeps observations as distinguishable as possible. We want to maximize the variance of the new vectors.
	\item Define $S$ as the observed covariance matrix:
	$$ S = \begin{bmatrix}
		\sum_{i=1}^n (x_{i,1} - \bar{x}_1)^2 & \hdots & \sum_{i=1}^n(x_{i,1} - \bar{x}_1)(x_{i,p} - \bar{x}_p) \\
		&\ddots \\
		\sum_{i=1}^n (x_{i,p} - \bar{x}_p)(x_{i,1} - \bar{x}_1) &\hdots & \sum_{i=1}^n(x_{i,p} - \bar{x}_p)^2
	\end{bmatrix} $$
	\item With $Z = X\vu$ where $Z$ is our 1d space and $\vu_{p\times 1}$ is the projection vector. We want $\vu$ to be a direction in the original predictor space, so define $\vu$ as a vector of norm 1.
	\item Since the $Var(\vz) = \vu^TS\vu$, we do the maximization problem using Lagrange multipliers with one constraint:
	$$\begin{cases}
		\text{Maximize} & \vu^TS\vu  \\
		\text{Subject to} & \vu^T\vu = 1
	\end{cases} $$
	\item This leads to $$S\vu = \lambda\vu$$ which implies that $\lambda$ is an eigenvalue of $S$ and $\vu$ is an eigenvector of $S$.
	\item Left-multiply $S\vu$ by $\vu^T$ we get $\vu^TS\vu = \lambda$, thus $\lambda$ is the variance of the projected data.
	\item \textcolor{blue}{To maximize the variance, we select $\vu$ as the eigenvector associated with the largest eigenvalue.}
\end{enumerate}

\paragraph{Generalization}
To generalize the process, suppose we have $p$ predictors. We can project the predictor matrix $X$ on a lower dimension orthogonal space $Z$ with dimension $m < p$ using a projection matrix $\vu_{p\times m}$:
$$Z_{n\times m} = X_{n \times p}\vu_{p\times m}$$
\under{Problem}
	$$\begin{cases}
		\text{Maximize} & \vu^TS\vu  \\
		\text{Subject to} & \vu^T\vu =1 \\
		\text{and} & \vu_i \text{'s are orthogonal}
	\end{cases} $$
The matrix $\vu_{p\times m}$ consists eigenvectors associated with the $m$ largest eigenvalues of the data correlation matrix $S$. \\
Then we can fit a linear model using $Z: \vy = Z\beta + \ve$ that we have lower number of predictors and they are all uncorrelated.


\subsection{Ridge and Lasso Regression}
\subsubsection{Ridge Regression}
A \under{shrinkage technique}: technique that reduce the size of the parameters.
\paragraph{Motivation}
We could prevent overfitting by controlling the size of the parameters.\\
\textcolor{red}{$\bar{y}$ does not overfit.} If $\beta_i = 0$ for $i \in \{1, \hdots, p\}$, then $\bar{y} = \beta_0$. Intuitively, if the $\beta_i$'s are small, they can only affect the $y$ so that it minimizes the chances of overfitting.
\paragraph{Review}
We established our $\hat{\beta}$ by minimizing sum of square error $(\vy - X\hat{\beta})^T(\vy - X\hat{\beta})$
\paragraph{Regularization}
One way to force small $\hat{\beta}$ would be to minimize $$\sum_{i=1}^n \beta_i^2$$
Thus we can establish $\hat{\beta}_{ridge}$ as the solution of the minimization of 
$$(\vy - X\hat{\beta})^T(\vy - X \hat{\beta}) + \lambda \sum_{i=1}^n \beta_i^2$$ 
\textcolor{red}{where $\lambda$ is a hyper-parameter controlling the penalty on $\sum_{i=1}^n \beta_i^2$}. \\
If $\lambda = 0$ then we have our regular $\hat{\beta} = (X^TX)^{-1}X^T\vy$; as $\lambda \rightarrow \infty$ then all of $\beta_i$'s goes to 0 and we have a model that predicts $\bar{y}$.
\paragraph{A constrained optimization problem}
The ridge regression can be expressed as a constrained optimization problem. In fact:
$$\hat{\beta}_{ridge} = \begin{cases}
	\underset{\beta}{argmin} & \sum_{i=1}^n(y_i - (\beta_0 + (\sum_{j=1}^p \beta_j x_{i,j}))^2 \\
	\text{Subject to} & \sum_{j=1}^p \beta_j^2 \leq t
\end{cases} $$
\textcolor{blue}{Just know that there exist a one-to-one correspondence between $\lambda$ and $t$, and that both of the formulations lead to the same solution.}
\paragraph{Parameter tuning for $\lambda$}
Establish a huge list of possible $\lambda$s. Then try them all and select one that best result on the validation set.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{ridge.png}
	\caption{x-axis: $\frac{1}{\lambda}$. As the penalty decrease, the parameters converge to LSE.}
\end{figure}

\paragraph{Remarks}
The model does NOT reduce the number of parameters, thus does not improve interpretability.

\subsubsection{Lasso Regression}
Actually enforce \under{sparsity}: reduce some of the parameters. \textcolor{red}{But this method is non-convex, so it is much more complicated to solve and we cannot compute the exact solution.}
\paragraph{Regularization}
We want to minimize
$$ 
\sum_{i=1}^n(y_i - (\beta_0 + \sum_{j=1}^p \beta_j x_{i,j}))^2 + \lambda \sum_{j=1}^p |\beta_j|$$
\paragraph{A constrained optimization problem}
$$ \hat{\beta}_{lasso} = \begin{cases}
 \underset{\beta}{argmin} & 	\sum_{i=1}^n(y_i - (\beta_0 + \sum_{j=1}^p \beta_j x_{i,j}))^2 
 \\
 \text{subject to} & \sum_{j=1}^p |\beta_j| \leq t
 \end{cases} $$
 \textcolor{blue}{There exist a one-to-one correspondence between $\lambda$ and $t$, and that both of the formulations lead to the same solution.}
 
 \begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{lasso.png}
	\caption{x-axis: $\frac{1}{\lambda}$. As the penalty decrease, the parameters converge to LSE.}
\end{figure}

 \paragraph{Remarks}
 Computing the lasso solution is a quadratic programming problem, but efficient algorithms are available for computing the entire path of solutions as $\lambda$ varies with the same computational cost as for ridge regression.
 \paragraph{Limitations}
 \begin{enumerate}
 	\item If $p > n$, the lasso selects at most $n$ variables. (The number of selected genes is bounded by the number of samples)
 	\item Grouped variables: the lasso fails to do grouped selection. It tends to select one variable from a group and ignore the others.
 \end{enumerate}
 
 \subsubsection{Ridge v.s. Lasso}
  \begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{vs.png}
	\caption{Contour of the prediction errors and Regularizations. Different constraints lead to different contour shapes. Notice that lasso always has the smallest error at one of the axis, thus induces sparsity.}
\end{figure}

 
 \paragraph{related R packages}
 The GLMnet and Elastic net packages in R are freely available.


 \paragraph{Post-selection inference}
 Post-selection inference is still a problem problem when using these techniques. In fact there is not even a distribution in the model yet. SelectiveInference package in R allow to do valid inference for parameters that were selected using Elastic net.
 
 \subsubsection{Elastic Net Regularization}
 $$\hat{\beta} = \underset{\beta}{argmin} |\vy - X\beta|^2 + \lambda_2|\beta|^2 + \lambda_1 |\beta|$$
 \paragraph{Properties}
 \begin{enumerate}
 	\item Removes the limitation on the number of selected variables
 	\item Encourages \under{grouping effect}
 	\item Stabilizes the $l_1$ regularization path
 \end{enumerate}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{elastic.png}
	\caption{Singularities at the vertexes (necessary for sparsity) and strict convex edges (grouping). The strength of convexity varies with $\alpha$}
\end{figure}
\paragraph{Lasso v.s. Elastic Net}

 \begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{vs2.png}
	\caption{Elastic Net encourages group effect.}
\end{figure}
 
 
 
 

\end{document}