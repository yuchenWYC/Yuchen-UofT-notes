\documentclass[11pt]{article}
% Libraries.

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[margin=3cm]{geometry}
%\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[
	colorlinks=true,
	linkcolor=black,
	urlcolor=black
]{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage[dvipsnames, pdftex]{xcolor}
\usepackage{float}
\usepackage{xargs}
\usepackage[
	colorinlistoftodos,
	prependcaption,
	textsize=tiny
]{todonotes}
\usepackage{../raina}


% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}

% Attr.
\title{STA414\\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}



\begin{document}
    \maketitle
    \tableofcontents
    \newpage

\section{Introduction}
\section{Introduction to Probabilistic Models}
\subsection{Overview of probabilistic models}
In general, we have random variables $X = (X_1, \hdots, X_N)$ that are either \ti{observed} or \ti{unobserved}. Need a model that captures the relationship between these variables. The approach of probabilistic generative models is to relate all variables by a learned joint probability distribution $p_\theta(X_1, \hdots, X_N)$. We assume there is a true joint $p_*$, which we are trying to learn with a model $p_\theta$. \\
Assume we have the joint probability $p(X,C,Y)$
\paragraph{Regression}
$$p(Y|X) = \frac{p(X,Y)}{p(X)} = \frac{p(X,Y)}{\int p(X,Y)\, dY}$$

\paragraph{Classification / Clustering}
$$p(C|X) = \frac{p(X,C)}{\sum_C p(X,C)}$$
Now that we have a distribution over class labels, we have a choice for how to assign the class label:
\begin{enumerate}
	\item $c^* = \arg \max_c p(C=c|X)$
	\item Sample the class assignment from our distribution, $c^* \sim p(C|X)$
	\item Output the class assignment (however we chose it) along with its density under our distribution $(c^*, p(C=c^*|X))$. \red{Can inform us of the model's uncertainty or confidence of the prediction.}
\end{enumerate}

\paragraph{Latent/hidden Variables}
Variables which are never observed in the dataset.

\paragraph{Operations on Probabilistic Models}
\begin{itemize}
	\item \tb{Generate Data} For this we will need to know how to \blue{sample} from the model
	\item \tb{Estimate Likelihood} When all variables are either observed or marginalized, the result is a single real number which is the \blue{probability} of all variables taking on whose specific values.
	\item \tb{Inference}: Compute \blue{expected value} of some variables given others which are either observed or marginalized.
	\item \tb{Learning}: Set the parameters of the joint distribution given some observed data to \blue{maximize} the probability of the observed data.
\end{itemize}

\paragraph{Goals of joint distributions}
\begin{enumerate}
	\item Facilitate \tb{efficient computation} of marginal and conditional distributions
	\item Have \tb{compact representation} so the size of the parameterization scales well for joint distributions over many variables.
\end{enumerate}

\paragraph{Joint Dimensionality}
Suppose $n$ is the number of variables and $k$ is the number of states of each variable. Then  dimensionality of our parameters is $k^n$.



\subsection{Sufficient statistics}
\theorem[Fisher-Neyman factorization theorem]
If $f_\theta(x)$ is a pdf, then $T$ is sufficient for $\theta$ if and only if \blue{nonnegative} functions $g$ and $h$ can be found such that
$$f_\theta(x) = h(x)g_\theta(T(x))$$
i.e. the density $f$ can be factored into a product such that one factor $h$ does not depend on $\theta$ and the other factor, which does depend on $\theta$, depends on $x$ only through $T(x)$.
\definition[Statistic and Sufficient statistic]
A \under{statistic} is a (possibly vector valued) deterministic function of a (set of) random variable(s). A \under{sufficient statistic} is a statistic that conveys exactly the same information about the data generating process that created the data as the entire data itself. Formally, we say that $T(X)$ is a sufficient statistic for $X$ if
$$T(x^{(1)}) = T(x^{(2)}) \implies L(\theta; x^{(1)}) = L(\theta; x^{(2)}) \quad \forall \theta$$
where $L$ is the likelihood function. \\
Alternatively,
$$P(\theta | T(X)) = P(\theta|X)$$
\blue{Equivalently (by the Neyman factorization theorem) we can write
\[
P(\theta | T(X))=h(x, T(x)) g(T(x), \theta)
\]
}

\example[Bernoulli Trials]
We observe $N$ iid coin flips. \\
Model: $p(H) = \theta, P(T) = 1 - \theta$\\
Likelihood: $l(\theta; D) = \log \theta \sum_n x^{(n)} + \log (1-\theta) \sum_n (1 - x^{(n)})$\\
\begin{align*}
	l(\theta; D) &= \log \theta \sum_n x^{(n)} + \log (1-\theta) \sum_n (1 - x^{(n)})\\
	&= \log \theta N_H + \log(1-\theta)N_T
\end{align*}
Notice that our likelihood depends on $N_H = \sum_n x^{(n)}$ (and $N_T$).\\
$\implies$ If we know this summary statistic $T(x) = \sum_n x^{(n)}$, then we know everything that is useful from our sample to do inference.\\
$$l(\theta; D) = T(X)\log \theta + (N - T(X)) \log (1 - \theta)$$
Then we take the derivative and set it to 0 to find the maximum
\[
\begin{aligned}
\Rightarrow \frac{\partial \ell}{\partial \theta}=& \frac{T(X)}{\theta}-\frac{N-T(X)}{1-\theta} \\
& \Rightarrow \hat{\theta}=\frac{T(X)}{N}
\end{aligned}
\]
This is our maximum likelihood estimation of the parameters \(\theta, \theta_{M L E}^{\star}\).\\
\blue{sufficient statistics: counts}

\example[Multinomial]
We observe $M$ iid die rolls ($K$-sided). \\
Model: $p(k) = \theta_k, \sum_k \theta_k = 1$ \\
Likelihood: $l(\theta; D) = \sum_k N_k \log \theta_k$ \\
Take derivatives and set to zero (enforcing \(\sum \theta_{k}=1\) ):
\[
\begin{aligned}
\frac{\partial \ell}{\partial \theta_{k}}=& \frac{N_{k}}{\theta_{k}}-M \\
 \Rightarrow \theta_{k}^{*}&=\frac{N_{k}}{M}
\end{aligned}
\]
\blue{sufficient statistics: counts}
\example[exponential family of distributions]
The result of the previous example distributions show that the MLE are just normalized counts. However, the simplicity of the sufficient statistics and MLE are due to those being members of the exponential family.
In general, exponential family members have simple sufficient statistics and MLE for the natural statistics $\eta$
\[
p(x | \eta)=h(x) \exp \left\{\eta^{T} T(x)-A(\eta)\right\}
\]
where:
\begin{itemize}
	\item $\eta$ are the parameters
	\item $T(x)$ are the sufficient statistics
	\item $h(x)$ is the base measure
	\item $A(\eta)$ is the normalizing constant
\end{itemize}
with log-likelihood
\begin{align*}
	l(\eta; D) &= \log p(D;\eta)\\
	&= (\sum_n\log h(x_n)) - NA(\eta) + (\eta^T\sum_n T(x_n))
\end{align*}
Finding the derivative and setting to zero, we get
$$\eta_{MLE} = \frac{1}{N}\sum_n T(x_n)$$
which is the normalized counts of the data.

\example{univariate normal}
We have $N$ i.i.d. samples $\{x_i\}_1^N$\\
Model: $p(x|\theta) = \frac{1}{\sqrt{2\pi\sigma}}\exp{-\frac{1}{2\sigma^2}(x - \mu)^2}$\\
Gaussian distribution is a member of the exponential family, so we can put it into a natural form
$$p(x|\theta) = \frac{1}{\sqrt{2\pi\sigma}}\exp\{-\frac{1}{2\sigma^2}\mu^2\}\exp\{\begin{bmatrix}
	\frac{\mu}{\sigma^2} & \frac{-1}{2\sigma^2}\end{bmatrix}\begin{bmatrix}
	x \\ x^2
\end{bmatrix}\}$$
From here, it is clear that the natural parameters and the sufficient statistics are
\begin{itemize}
	\item $\eta = \begin{bmatrix}
		\frac{\mu}{\sigma^2} \\
		\frac{-1}{2\sigma^2}
	\end{bmatrix}$
	\item $T(x) = \begin{bmatrix}
		x \\ x^2
	\end{bmatrix}$
\end{itemize}
re-writing in terms of \(\eta\)
$$p(x | \eta)=(\sqrt{2 \pi})^{-\frac{1}{2}} \cdot\left(-2 \eta_{2}\right)^{\frac{1}{2}} \cdot \exp \left\{\frac{\eta_{1}^{2}}{4 \eta_{2}}\right\} \cdot \exp \left\{\eta^{T} T(x)\right\}$$
noting that\\
\begin{itemize}
	\item \(h(x)=(\sqrt{2 \pi})^{-\frac{1}{2}}\)
	\item \(A(\eta)=\left(-2 \eta_{2}\right)^{\frac{1}{2}} \cdot \exp \left\{\frac{\eta_{1}^{2}}{\eta_{2}}\right\}\)
\end{itemize}


\section{Directed Graphical Models}
\subsection{Decision Theory}
We care about probabilities because they help us make decisions.\\
Denote action by $a$, state by $s$, value function by $V(s)$, utility function by $u(a)$, then
$$a^* = \underset{a}{argmax}\, \underbrace{E_{p(s|a, knowledge)}[V(s)]}_{u(a)}$$


\subsection{Joint Distributions}
\theorem[chain rule of probability]
The joint distribution of \(N\) random variables can be computed by the chain rule
\[
p\left(x_{1, \ldots, N}\right)=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{2}, x_{1}\right) \ldots p\left(x_{n} | x_{n-1: 1}\right)
\]
This is true for any joint distribution over any random variables (assuming full dependence between variables).
More formally, in probability the chain rule for two random variables is
\[
p(x, y)=p(x | y) p(y)
\]
and for \(N\) random variables
$$p(x_1, x_2, \hdots, x_N) = \prod_{j=1}^N p(x_j|x_1,x_2,\hdots,x_{j-1})$$
for any ordering of the variables.

\subsubsection{Number of parameters in a joint distribution}
$$p(x_1, \hdots, x_A | y_1, \hdots, y_B)$$
$\#$ parameters = \{($\#$ possible states of $x_{1:A}$) - 1\} $\times$ ($\#$ possible states of $y_{1:B}$)\\
binary: $(2^A - 1)\times 2^B$.

\subsubsection{Conditional Independence}
\definition[conditionally independence]
Two random variables $A, B$ are conditionally independent given a third variable $C$, denoted
$$X_A \perp X_B | X_C$$
if 
\begin{align*}
	&\iff p(X_A, X_B | X_C) = p(X_A|X_C)p(X_B|X_C) \\
	&\iff p(X_A|X_B,X_C) = p(X_A|X_C)\\
	&\iff p(X_B|X_A,X_C) = p(X_B|X_C)
\end{align*}
for all $X_C$.


\subsection{Directed acyclic graphical models (DAGM)}
\paragraph{Graphical models}
Probabilistic graphical models are a concise way to specify and reason about conditional independencies, without worrying about the detailed form of the distribution. There are three flavours:
\begin{itemize}
	\item Undirected
	\item Factor graphs
	\item Directed
\end{itemize}

\definition[directed acyclic graphical model]
A \under{directed acyclic graphical model} implies a restricted factorization of the joint distribution. In a DAG, variables are represented by nodes, and dependence are represented by edges. \\
Recall that for \(N\) random variables,
$$p(x_1, x_2, \hdots, x_N) = \prod_{j=1}^N p(x_j|x_1,x_2,\hdots,x_{j-1})$$
for any ordering of the variables. \\
In the context of DAG, we can write
$$p(x_1, x_2, \hdots, x_N) = \prod_{i=1}^N p(x_i |parents(x_i))$$
where $parents(x_i)$ is the set of nodes with edges pointing to $x_i$.\\\\
In other words, the joint distribution of a DAGM factors into a product of local conditional distributions, where each node (a random variable) is conditionally dependent on its parent  nodes(s), which could be empty.
\remark
We are conditioning on parent nodes as opposed to every node. Therefore, the model that represents this distribution is exponential in the fan-in of each node (the number of nodes in the parent set), instead of in $N$.

\paragraph{Grouping variables}
We can always group variables together into one bigger variable:
$$p(x_i, x_{\pi_i}) = p(x_{\pi_i})p(x_i|x_{\pi_i})$$

\subsubsection{Conditional Independence in DAGM}
The simplest conditional independence relationship encoded in a Bayesian network can be stated as follows: \blue{a node is independent of its ancestors given its parents}:
$$x_i \perp x_{\tilde{\pi_i}} | x_{\pi_i}$$
In general, missing edges imply conditional independence.

\definition[D-Separation]
\under{D-separation}, or \under{directed-separation} is a notion of connectedness in DAGMs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s). \\
$D$-connection implies conditional dependence and d-separation implies conditional independence. \\
In particular, we say that
$$x_A \perp x_B | x_C$$
if every variable in $A$ is d-separated from every variable in $B$ conditioned on all the variables in $C$.\\
To check if an independence is true, we can cycle through each node in $A$, do a depth-first search to read every node in $B$, and examine the path between them. If all of the paths are d-separated(i.e. conditionally independent), then
$$x_A \perp x_B | x_C$$
\example{Chain}
 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{p3.png}
\end{figure}
\noindent Question: When we condition on $y$, are $x$ and $z$ independent? \\
Answer: \\
From the graph, we get
$$P(x,y,z) = P(x)P(y|x)P(z|y)$$
Then
\begin{align*}
	P(z|x,y) &= \frac{P(x,y,z)}{P(x,y)} \\
	&= \frac{P(x)P(y|x)P(z|y)}{P(x)P(y|x)} \\
	&= P(z|y)
\end{align*}
which implies $x \perp z|y$. 
\example{Common Cause}
 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{p4.png}
\end{figure}
\noindent Where we think of $y$ as the ``common cause" of the two independent effects $x$ and $z$. \\
Question: When we condition on $y$, are $x$ and $z$ independent? \\
Answer:
From the graph, we get
$$P(x,y,z) = P(y)P(x|y)P(z|y)$$
which implies
\begin{align*}
	P(x,z|y) &= \frac{P(x,y,z)}{P(y)} \\
	&= \frac{P(y)P(x|y)P(z|y)}{P(y)} \\
	&= P(x|y)P(z|y)
\end{align*}
which implies $x\perp z |y$. 
\example{Explaining Away}
 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{p5.png}
\end{figure}
\noindent Question: $x \perp z$? \\
Answer: \\
\begin{align*}
	P(x,z) &= \sum_y P(x,y,z) \\
	&= \sum_y P(x)P(z)P(y|x,z) \\
	&= P(x)P(z)\sum_y P(y|x,z) \\
	&= P(x)P(z)
\end{align*}
which implies that $x\perp z$.\\
Question: $x \perp z | y$? \\
Answer: 
\begin{align*}
	p(z|x,y) &= \frac{p(x)p(z)p(y|x,z)}{p(x)p(y|x)}\\
	&= \frac{p(z)p(y|x,z)}{p(y|x)} \\
	&= p(z|y)
\end{align*}
which implies $x \not \perp | y$.\\
\red{In fact, $x$ and $z$ are marginally independent, but given $y$ they are conditionally dependent.} This important effect is called \under{explaining away}.
\theorem[Bayes-Balls Algorithm]
In general, the algorithm works as follows:
\begin{enumerate}
	\item Shades all nodes $x_C$
	\item Place ``balls" at each node in $x_A$ (or $x_B$)
	\item Let the ``balls" ``bounce" around according to some rules
	\item If any of the balls reach any of the nodes in $x_B$ from $x_A$ (or $x_A$ from $x_B$), then $x_A \not \perp x_B | x_C$; otherwise $x_A \perp x_B | x_C$.
\end{enumerate}
The rules are as follows:
 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{p6.png}
	\includegraphics[scale=0.3]{p7.png}
\end{figure}
\noindent where arrows indicate paths the balls can travel, and arrows with bars indicate paths the balls cannot travel.
\subsubsection{Example of a DAGM: Markov Chain}
\under{Markov chains} are a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\\
i.e. Conditional on the present state of the system, its future and past states are independent.


\subsubsection{Plates}
Because Bayesian methods treat parameters as random variables, we would like to include them in the graphical model. One way to do this is to \blue{repeat all the iid observations explicitly and show the parameter only once}. A better way is to use \under{plates}, in which repeated quantities that are iid are put in a box.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{p8.png}
\end{figure}
The rules of plates:
\begin{enumerate}
	\item Repeat every structure in a box a number of times given by the integer in the corner of the box, updating the plate index variable as you go. Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure.
	\item Plates can be nested, in which case their arrows get duplicated also.
	\item Plates can also intersect, in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them.
\end{enumerate}


\subsubsection{Unobserved Variables}
Certain variables in our models may be unobserved, either some of the time or always, at training time or at test time.
\paragraph{Partially unobserved variables}
If variables are \tb{occasionally unobserved} then they are \under{missing data}, e.g. undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we marginalize the missing values:
$l(\theta; D) = \sum_{complete} \log p(x^c, y^c |\theta) + \sum_{missing} \log p(x^m|\theta)$
\begin{align*}
	l(\theta; D) &= \sum_{complete} \log p(x^c, y^c |\theta) + \sum_{missing} \log p(x^m|\theta) \\
	&= \sum_{complete} \log p(x^c,y^c|\theta) + \sum_{missing} \log \sum_{y} p(x^m, y|\theta)
\end{align*}

\paragraph{Latent variables}
What to do when a variable $z$ is always unobserved? \\
Depends on where it appears in our model. If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out.\\
\paragraph{Mixture models}
What if the class is unobserved? Then we sum it out
$$p(x;\theta) = \sum_{k=1}^K p(z=k;\theta_z)p(x|z=k;\theta_k)$$
We can use the Bayes' rule to compute the posterior probability of the mixture component given some data:

\paragraph{Hidden Markov Models (HMMs)}
\under{Hidden Markov Model (HMM)} is a statistical Markov model in which the system being modelled is assumed to be a Markov process with unobserved (i.e. hidden) states. It is a very popular type of latent variable model.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p2.png}
\end{figure}
where 
\begin{itemize}
	\item $Z_t$ are hidden states taking on one of $K$ discrete values
	\item $X_t$ are observed variables taking on values in any space
\end{itemize}
The joint probability represented by the graph factorizes according to
$$p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T}|Z_{1:T}) = p(Z_1)\prod_{t=2}^Tp(Z_t|Z_{t-1})\prod_{t=1}^T p(X_T|Z_t)$$

\section{Exact Inference}
\subsection{Inference as Conditional Distribution}
\notation
\begin{align*}
	X_E &= \text{ the observed evidence} \\
	X_F &= \text{ the unobserved variable we want to infer} \\
	X_R &= X - \{X_F, X_E\} = \text{ Remaining variables, extraneous to query }
\end{align*}
where $X_R$ is the set of random variables in our model that are neither part of the query nor the evidence.
\definition[exact inference]
The \under{exact inference} task is defined as:
Given a fully parameterized DAG model over variables $\chi$, $X_F \subseteq \chi, X_E\subseteq \chi$ s.t. $X_E \cap X_F  = \emptyset$ and $\ve \in val(X_E)$:
$$\text{compute } P(X_F|X_E = \ve)$$
$X_E$ can be empty in which case we're after $P(X_F)$.

\subsection{Variable elimination}
Variable elimination is a simple and general exact inference algorithm in any probabilistic graphical model.

\paragraph{Simple Example: Chain}
$$ A \rightarrow B \rightarrow C \rightarrow D$$
where we want to compute $P(D)$, with no observations for other variables. \\
We have
$$X_F = \{D\}, X_E = \{\}, X_R = \{A,B,C\}$$
This graphical model describes the factorization of the joint distribution as:
$$P(A,B,C,D) = p(A)p(B|A)p(C|B)p(D|C)$$
If the goal is to compute the marginal distribution $p(D)$ with no observed variables, then we marginalize over all variables but $D$:
$$p(D) = \sum_{A,B,C} p(A,B,C,D)$$
\tb{Sum naively: $\mc{O}(k^n)$}
\begin{align*}
	p(D) &= \sum_{A,B,C} p(A,B,C,D) \\
	&= \sum_C \sum_B \sum_A p(A)p(B|A)p(C|B)p(D|C)
\end{align*}
\tb{Elimination ordering: $\mc{O}(nk^2)$}
\begin{align*}
	p(D) &= \sum_{A,B,C} p(A,B,C,D) \\
	&= \sum_C p(D|C)\sum_B p(C|B) \sum_A p(A)p(B|A)\\
	&= \sum_C p(D|C)\sum_B p(C|B)P(B)\\
	&= \sum_C p(D|C)p(C)
\end{align*}
\remark
Computing the joint is NP-hard. We can catch up computations that are otherwise computed exponentially many times, but this depends on having a good variable elimination ordering.

\subsubsection{Sum-Product Inference}
Sum-product inference algorithm can be used to compute $P(Y)$ for directed and undirected models: $\forall Y$,
$$\tau(Y) = \sum_z \prod_{\phi \in \Phi} \phi(scope[\phi]\cap Z, scope[\phi]\cap Y)$$
\note{We want to marginalize out variables in $Z$ since they are extraneous} 
\note{e.g. for $\phi(A,B,C)$, scope$[\phi] = \{A, B, C\}$}
where $\Phi$ is a set of potentials or factors.
\paragraph{Directed models}
$\Phi$ is given by the conditional probability distributions for all variables
$$\Phi = \{\phi_{x_i}\}_{i=1}^N = \{p(x_i|parents(x_i))\}_{i=1}^N$$
\note{For DAG, we have $\phi = p$}
where the sum is over the set $Z = X - X_F$. The resulting term $\tau(Y)$ will automatically be normalized.
\paragraph{Undirected models}
$\Phi$ is given by the set of unormalized potentials. Therefore, we must normalize the resulting $\tau(Y)$ by $\sum_Y \tau(y)$.

\example[Directed Graph]
 \begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p1.png}
\end{figure}
$$p(C, D, I, G, S, L, H, J) = p(C)p(D|C)p(I)p(G|D,I)p(L|G)p(S|I)p(J|S,L)p(H|J,G)$$
We can write the conditional distributions as factors
$$\Phi = \{\phi(C), \phi(C, D), \phi(I), \phi(G,D,I), \phi(L,G), \phi(S, I), \phi(J, S, L), \phi(H, J, G)$$
If we are interested in inferring the probability of getting a job, $p(J)$, we can perform exact inference on the joint distribution by marginalizing according to a specific variable elimination ordering: $    \prec \{C, D, I, H, G, S, L\}$
\begin{align*}
	p(J) &=\sum_{L}\sum_{S} \phi(J,L,S) \sum_{G} \phi(L, G) \sum_{H}\phi(H,G,J)\sum_I \phi(S, I)\phi(I)\sum_D\phi(G,D,I)\underbrace{\sum_C\phi(C)\phi(C,D)}_{\tau{(D)}} \\
		 &=\sum_{L}\sum_{S} \phi(J,L,S) \sum_{G} \phi(L, G) \sum_{H}\phi(H,G,J)\sum_I \phi(S, I)\phi(I)\underbrace{\sum_D\phi(G,D,I)\tau(D)}_{\tau (G, I)} \\
		&=\sum_{L}\sum_{S} \phi(J,L,S) \sum_{G} \phi(L, G) \sum_{H}\phi(H,G,J)\sum_I \underbrace{\phi(S, I)\phi(I)\tau(G,I)}_{\tau(S,G)} \\
		&=\sum_{L}\sum_{S} \phi(J,L,S) \sum_{G} \phi(L, G)\blue{\tau(S,G)}\underbrace{\sum_{H}\phi(H,G,J)}_{\tau(G,J)}\\
		&=\sum_{L}\sum_{S} \phi(J,L,S) \underbrace{\sum_{G} \phi(L, G)\tau(S,G)\tau(G,J)}_{\tau(J,L,S)}\\		
		&=\sum_{L}\underbrace{\sum_{S} \phi(J,L,S)\tau(J,L,S)}_{\tau(J,L)}\\
		&=\underbrace{\sum_L \tau(J,L)}_{\tau(J)} \\
		&= \tau(J)
\end{align*}

\fact[Complexity of variable elimination ordering]
The complexity of the VE algorithm is
$$\mc{O}(mk^{N_{max}})$$
\begin{itemize}
	\item $m$ is the number of initial factors $ = |\Phi|$
	\item $k$ is the number of states each random variable takes (assumed to be equal here)
	\item $N_i$ is the number of random variables inside each sum $\sum_i$
	\item $N_{max} = argmax_i N_i$ is the number of random variables inside the largest sum. 
\end{itemize}

\section{Message passing, Hidden Markov Models, and Sampling}
\paragraph{Inference in Trees}
Tree is a general family of graphs for which the optimal elimination ordering is trivial to find, and which has linear cost in the number of nodes.
\subsection{Message Passing \& Belief Propagation}
What if we want to compute the marginal of every variable in a graph: $p(x_i) \, \forall x_i \in X$? \\
Run variable elimination separately for each variable $x_i$ is computationally expensive. \\
\paragraph{Joint distribution for undirected graph models}
For an undirected graph $G = (V, E)$,
$$P(X_{1:n}) = \frac{1}{Z} \prod_i \phi(x_i) \prod_{(j, k) \in E} \phi_{j, k}(x_j, x_k)$$
\paragraph{Message-passing}
Belief propagation is based on message-passing of ``message" between neighboring vertices of the graph. The message sent from variable $j$ to $i \in N(j)$ is
$$m_{j \rightarrow i}(x_i) = \sum_{x_j} \phi_j(x_j)\phi_{ij}(x_i, x_j) \prod_{k \in N(j) \neq i} m_{k \rightarrow j}(x_j)$$

\theorem[Belief propagation algorithm]
As follows:
\begin{enumerate}
	\item Choose root $r$ arbitrarily
	\item Pass messages from leaves to $r$
	\item Pass messages from $r$ to leaves
	\item Compute $\red{p(x_i) \propto \phi_i(x_i) \prod_{j \in N(i)} m_{j \rightarrow i}(x_i), \, \forall i}$
\end{enumerate}

\subsection{Hidden Markov models}
\subsubsection{Sequential data}
$$x_{1:T} = \{x_1, \hdots, x_T\}$$
Recall the general joint factorization via the chain rule
$$p(x_{1:T}) = \prod_{t=1}^T p(x_t|x_{t-1}, \hdots, x_1)$$
\paragraph{First order Markov chain}
$$p(x_t | x_{1:t-1}) = p(x_t | x_{t-1})$$
This assumption greatly simplifies the factors in the joint distribution
$$p(x_{1:T}) = \prod_{t=1}^T p(x_t|x_{t-1})$$

\definition[stationary/time-homogenous Markov chain]
As follows
\begin{itemize}
	\item \tb{Stationary Markov chain: }the distribution generating the data does not change through time:
	$$p(x_t|x_{t-1}) = p(x_{t+k}|x_{t-1+k})\quad \forall t, k$$
	\item \tb{Non-stationary Markov chain: }the distribution generating the data is a function of time.
\end{itemize}

\paragraph{Higher-order Markov chains}
second order:
$$p(x_t|x_{1:t-1}) = p(x_t|x_{t-1},x_{t-2})$$
$m$-order:
$$p(x_t|x_{1:t-1}) = p(x_t|x_{t-m:t-1})$$
\paragraph{Parameterization}
How does the order of temporal dependence affect the number of parameters in our model?\\
Assume $x$ is a discrete random variable with $k$ states. 
\begin{enumerate}
	\item $x_t: k - 1$, as the last state is implicit.
	\item first-order chain: $k(k-1)$, as we need $k$ numbers of parameters for each parameter of $x_t$
	\item $m$-order chain: $k^m(k-1)$, as we need $k^m$ number of parameters for each parameter of $x_t$
\end{enumerate}

\subsubsection{Hidden Markov Models}
\under{Hidden Markov Model (HMM)} hide the temporal dependence by keeping it in the \blue{unobserved} state. For each observation $x_t$, we associate a corresponding unobserved hidden/latent variable $z_t$
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p2.png}
\end{figure}
The joint probability represented by the graph factorizes according to
$$p(X_{1:T}, Z_{1:T}) = p(Z_{1:T})p(X_{1:T}|Z_{1:T}) = p(Z_1)\prod_{t=2}^Tp(Z_t|Z_{t-1})\prod_{t=1}^T p(X_T|Z_t)$$
\blue{Unlike simple Markov chains, the observations are not limited by a Markov assumption of any order, i.e. $x_t$ isn't necessarily independent of any other observation, no matter how many other observations we make.}
\definition[Parameterization of a hidden Markov model]
Assuming we have a homogeneous model, we only have to learn three distributions
\begin{enumerate}
	\item \tb{Initial distribution:} $\pi(i) = p(z_1 = i)$. The probability of the first hidden variable being in state $i$ (often denoted by $\pi$.)
	\item \tb{Transition distribution: } $T(i,j) = p(z_{t+1} = j|z_t = i), \, i \in \{1, \hdots, k\}$. The probability of moving from hidden state $i$ to hidden state $j$.
	\item \tb{Emission probability: } $\epsilon_i(x_t) = p(x_t|z_t=i)$. The probability of an observed random variable $x_t$ given the state of the hidden variable that ``emitted" it.
\end{enumerate}

\subsubsection{Inference in HMMs}
HMMs are just tree-structured DAGs, meaning that inference in them is linear in the number of time steps. We can do exact inference in them.

\paragraph{Main tasks we perform with HMMs}
\begin{enumerate}
	\item Compute the probability of a latent sequence given an observation sequence. (e.g. compute $p(z_i|x_{1:t}) \forall i$ with the \under{Forward-Backward algorithm})
	\item Compute the marginal likelihood $p(x_1, x_2, \hdots, x_T)$ in order to fit parameters
	\item Infer the most likely sequence of hidden states (i.e. compute $Z^* = \underset{z_{1:T}}{arg max} \, p(z_{1:T}|x_{1:T})$ using the Viterbi algorithm)
\end{enumerate}
Assuming that we know the initial $p(z_1)$, transition $p(z_t|z_{t-1})$, and emission $p(x_t|z_t) \forall t\in [1, T]$
This task of hidden state inference breaks down into the following:
\begin{itemize}
	\item \tb{Filtering: }compute posterior over current hidden state, $p(z_t|x_{1:t})$
	\item \tb{Prediction: }compute posterior over future hidden state, $p(z_{t+k}|x_{1:t})$
	\item \tb{Smoothing: }compute posterior over past hidden state, $p(z_n | x_{1:t})\quad 1 < n < t$
\end{itemize}
\paragraph{Prediction}
Let's first take a look at the example where $k = 2$, then
\begin{align*}
	p(z_{t+2}|x_{1:t}) &= \sum_{z_{t+1}}\sum_{z_t}p(z_t,z_{t+1},z_{t+2}|x_{1:t}) \\
	&= \sum_{z_{t+1}}\sum_{z_t}p(z_t|x_{1:t})p(z_{t+1}|z_t, x_{1:t})p(z_{t+2}|z_{t+1},z_t,x_{1:t})\\
	&= \sum_{z_{t+1}}\sum_{z_t}p(z_t|x_{1:t})p(z_{t+1}|z_t)p(z_{t+2}|z_{t+1})
\end{align*}
\theorem[Forward-backward algorithm]
The Forward-backward algorithm is used to efficiently estimate the \blue{latent} sequence given an \blue{observation} sequence under a HMM. That is, we want to compute $$p(z_t|x_{1:T}) \quad \forall t \in [1,T]$$
It is computed in two parts, and then multiplied together:
\begin{itemize}
	\item \tb{Forward Filtering: }computes $p(z_t, x_{1:t})$
	\item \tb{Backward Filtering: }computes $p(x_{1+t:T}|z_t)$
\end{itemize} 
Note that
\begin{align*}
	p(z_t|x_{1:T}) &\propto p(z_t, x_{1:T}) \\
	&= p(z_t, x_{1:t})p(x_{t+1:T}|z_t, x_{1:t}) \\
	&= \underbrace{p(z_t, x_{1:t})}_{forward\, recursion}\underbrace{p(x_{t+1:T}|z_t)}_{backward\,recursion}
\end{align*}


\paragraph{Forward Filtering}
\begin{align*}
	p(z_t, x_{1:t}) &= \sum_{z_{t-1} = 1}^k p(z_{t-1}, z_t, x_{1:t}) \\
	&= \sum_{z_{t-1} = 1}^k p(x_t|z_{t-1}, z_t, x_{1:t-1})p(z_t|z_{t-1}, x_{1:t-1})p(z_{t-1}, x_{1:t-1})
\end{align*}
Define $\alpha_t(z_t):= p(z_t, x_{1:t})$, then
$$\alpha_t(z_t) = p(x_t|z_t)\sum_{z_{t-1} = 1}^k p(z_t|z_{t-1})\alpha_{t-1}(z_{t-1})$$
If we recurse all the way down to $\alpha_1(z_1)$, we get
$$\alpha_1(z_1) = p(z_1, x_1) = p(z_1)p(x_1|z_1)$$
\paragraph{Backward Filtering}
\begin{align*}
	p(x_{t+1:T}|z_t) &= \sum_{z_{t+1} = 1}^k p(z_{t+1}, x_{t+1:T} |z_t)\\
	&= \sum_{z_{t+1} = 1}^k p(x_{t+2:T}|z_{t+1}, z_t,x_{t+1})p(x_{t+1}|z_{t+1}, z_t)p(z_{t+1}|z_t) \\
	&=\sum_{z_{t+1} = 1}^k p(x_{t+2:T}|z_{t+1})p(x_{t+1}|z_{t+1})p(z_{t+1}|z_t) 
\end{align*}
Define $\beta_t(z_t):= p(x_{1 + t:T}|z_t)$, then
$$\beta_t (z_t) = \sum_{z_{t+1}}^k \beta_{t+1}(z_{t+1})p(x_{t+1}|z_{t+1})p(z_{t+1}|z_t)$$
If we recurse all the way down to $\beta_1(z_1)$, we get 
$$\beta_1(z_1) = p(x_{3:T}|z_2)p(x_2|z_2)p(z_2|z_1)$$

\subsection{Sampling}
A sample form a distribution $p(x)$ is a single realization $x$ whose probability distribution is $p(x)$. This contrasts with the alternative usage in statistics, where sample refers to a collection of realization x.

\paragraph{The problems to be solved}
The aims of Monte Carlo methods are to solve one or both of the following problems
\begin{enumerate}
	\item To generate samples $\{x^{(r)}\}_{r=1}^R$ from a given probability distribution $p(x)$.
	\item To estimate expectations of functions, $f(x)$, under distribution $p(x)$: $$E = E_{x\sim p(x)}[f(x)] = \int f(x)p(x)\,dx$$
\end{enumerate}

\subsubsection{Ancestral Sampling}
``Sampling in a topological order"\\
i.e. at each step, sample from any conditional distribution that you haven't visited yet, whose parents have all been sampled. This procedure will always start with the nodes that have no parents.\\
\example
In a chain or HMM, you would always start with $z_1$ and move to the right. In a tree, you would always start from the root.

\paragraph{Generating marginal samples}
If you are only interested in sampling a particular set of nodes, you can simply sample from all the nodes jointly, then ignore the nodes you don't need.

\paragraph{Generating conditional samples}
If you want to sample a variable conditional on a node with no parents, that is also easy - you can simple do ancestral sampling starting from the nodes you have.\\
However, to sample from a DAG conditional on leaf nodes is hard. Finding ways to do this approximately is what a lot of the rest of the course will be about.
\section{Stochastic Variational Inference}
\subsection{Motivation}
In modern machine learning is most often used to infer the conditional distribution over the latent variables given the observations (and parameters). (the posterior distribution) This can be written as
$$p(z|x,\alpha) = \frac{p(z, x|\alpha)}{\int_z p(z,x|\alpha)}$$
Since the integral cannot be easily computed analytically, we will use variational inference. The main idea behind this is to choose a family of distributions over the latent variables $z_{1:m}$ with its own set of variational parameters $v$, i.e., $q(z_{1:m}|v)$. Then, we find the setting of the parameters that makes our approximation closest to the posterior distribution (this is where optimization algorithms come in). Then we can use $q$ with the fitted parameters in place of the posterior (e.g. to form predictions about future data, or to investigate the posterior distribution over the hidden variables, find modes, etc).
\subsection{the TrueSkill latent variable model}
A player ranking system for competitive games.\\
Inferring the skill of a set of players in a competitive game, based only on observing who beats who when they play against each other.  We initially don't know anything about anyone's skill, for simplicity we start with an independent Gaussian prior.

\remark
We never get to observe the player's skill directly, which make this a \ti{latent variable model}. Instead, we observe the outcome of a series of matches between different players.

\paragraph{the model}
Each player has a fixed level of skill, denoted $z_i$.\\
For each game, the probability that player $i$ beats player $j$ is given by
$$\sigma(z_i - z_j)$$
where sigma is the logistic function $\sigma(y) = \frac{1}{1 + \exp{(-y)}}$. So we have
$$p(i \text{ beats } j|z_i, z_j) = \frac{1}{1 + \exp{(-(z_i - z_j))}}$$

\remark
\ti{The exact form of the prior and likelihood aren't particularly important, as long as the higher the skill level, the higher the chance of winning each game.}\\
We can write the entire joint likelihood of set of players and games as:
$$p(z_1, z_2, \hdots, z_N, g_1, g_2, \hdots, g_T) = \left[ \prod_{i=1}^N p(z_i)\right] \left[
 \prod_{player_i, player_j \in g_k} p( i \text{ beats }j |z_i, z_j)\right]$$
Computing the posterior over two player's skills requires integrating over all the other players' skills
$$p(z_1, z_2|g_1, g_2, \hdots,g_T) = p(z_1, z_2|x) = \int \hdots \int p(z_1, z_2, \hdots, z_N|x)\,dz_3 \hdots dz_N$$
where $x$ is a $N \times 2$ matrix and stores $(i, j)$ pairs that $i$ beats $j$.

\subsection{Posterior Inference in Latent Variable Models}
Consider the probabilistic model $p(x,z)$ where
\begin{itemize}
	\item $x_{1:T}$ are the observations
	\item $z_{1:N}$ are the unobserved latent variables
\end{itemize}
The conditional distribution of the unobserved variables given the observed variables (the posterior inference) is
$$p(z|x) = \frac{p(x|z)}{p(x)} = \frac{p(x|z)p(z)}{\int p(x,z)\,dz}$$
which we will denote as $p_\theta(x)$.\\
\blue{Whenever the number of values that $z$ can take is large, the computation $\int p(x,z)\,dz$ is intractable, making the computation of the conditional distribution itself intractable. Thus we have to use approximate inference.}

\subsubsection{Approximating the Posterior Inference with Variational Methods}
Approximating the Posterior Inference with Variational Methods works as follows:
\begin{enumerate}
	\item Introduce a variational family $q_\phi(z)$ with parameters $\phi$.
	\item Encode some notion of 	``distance" between $p(z|x)$ and $q_\phi(z)$.
	\item Minimize this distance.
\end{enumerate}

\remark
This turns Bayesian Inference into an optimization problem. If enough parts parts of our model are differentiable and well-approximated by simple Monte Carlo, we can use stochastic gradient descent to solve this optimization problem scalably.

\subsubsection{Kullback-Leibler Divergence}
We will measure the distance between $q_\phi$ and $p$ using Kullback-Leibler divergence:

\begin{align*}
	D_{KL}(q_\phi||p) &= \int q_\phi(z) \log \frac{q_\phi(z)}{p(z|x)} dz\\
	&= \underset{z \sim q_\phi}{\mb{E}}\left[ \log \frac{q_\phi(z)}{p(z|x)} \right]
\end{align*}
\property
Properties of the KL Divergence:
\begin{enumerate}
	\item $D_{KL}(q_\phi ||p) \geq 0$
	\item $D_{KL}(q_\phi ||p) = 0 \iff q_\phi = p$
	\item $D_{KL}(q_\phi ||p) \neq D_{KL}(p||q_\phi)$
\end{enumerate}

\remark
The significance of the last property is that $D_{KL}$ is \red{not} a true distance measure.

\paragraph{Variational Objective}
We want to approximate $p$ by finding a $q_\phi$ s.t.
$$q_\phi \approx p \implies D_{KL}(q_\phi || p) = 0$$
In other words, we want to find $\phi^*$ s.t. 
$$\phi^* = \arg \min_\phi D_{KL}[q_\phi || p]$$
\remark
\red{The computation of $D_{KL}(q_\phi||p)$ is intractable, because it contains the term $p(z|x)$.}

\paragraph{Evidence Lower Bound (ELBO)}
To do variational inference, we want to minimize the KL divergence between our approximation $q$ and our posterior $p$. However, we cannot actually minimize this quantity directly, but a function that is equal to it up to a constant.
\begin{align*}
	D_{KL}(q_\phi(z|x)||p(z|x)) &= \underset{z \sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(z|x)} \\
	&= \underset{z \sim q_\phi}{\mb{E}} \left[ \log \left( q_\phi(z|x) \cdot \frac{p(x)}{p(z,x)}\right) \right ] \\
	&= \underset{z \sim q_\phi}{\mb{E}} \left[ \log \left( \frac{q_\phi(z|x)}{p(z,x)}\cdot p(x) \right )\right ] \\
	&= \underset{z \sim q_\phi}{\mb{E}} \left[ \log \frac{q_\phi (z|x)}{p(z,x)}\right] + \underset{z \sim q_\phi}{\mb{E}} \left[\log p(x)\right] \\
	&= -\left(\blue{\underset{z \sim q_\phi}{\mb{E}} \left[\log p(z,x)\right]  - \underset{z \sim q_\phi}{\mb{E}} \left[ \log q_\phi (z|x)\right] }\right)+ \underset{z \sim q_\phi}{\mb{E}} \left[ \log p(x) \right]\\
	&= - \underbrace{\mc{L}(\phi;x)}_{ELBO} + \log p(x)
\end{align*}
\tb{Claim:} Maximizing the ELBO $\implies$ minimizing $D_{KL}(q_\phi || p)$.
\begin{proof}
	Rearranging, we get
	\begin{align*}
		D_{KL}(q_\phi(z|x)||p(z|x)) &= -\mc{L}(\phi;x) + \log p(x) \\
		\implies \mc{L}(\phi;x) + D_{KL}(q_\phi(z|x)||p(z|x)) &= \log p(x)
	\end{align*}
	Since $D_{KL}(q_\phi(z|x)||p(z|x)) \geq 0$, we have
	$$\mc{L}(\phi;x) \leq \log p(x)$$
	\unsure{why this proof?}
	Therefore, maximizing the ELBO $\implies$ minimizing $D_{KL}(q_\phi || p)$.
\end{proof} 
\paragraph{Optimizing the ELBO}
We have
\begin{align*}
	\mc{L}(\phi;x) &=  -\underset{z \sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(x,z)}\\
	&= \underset{z \sim q_\phi}{\mb{E}}\left[ \log p(x,z) - \log q_\phi (z|x)\right]
\end{align*}
If we want to optimize this with gradient methods, we will need to compute $\nabla_\phi \mc{L}(\phi)$:
$$\nabla_\phi \mc{L}(\phi) = \nabla_\phi \underset{z \sim q_\phi(z|x)}{\mb{E}}\left[ \log p(x,z) - \log q_\phi (z|x)\right]$$

\paragraph{Pathwise Gradient}
In general, we cannot switch the derivative and the expectation (an integral), if the distribution we are taking the expectation over ($q_\phi(z|x)$) depends on the parameter ($\phi$). \\
So we need to factor out the randomness from $q$ and put it into a parameterless, fixed source of noise $p(\epsilon)$. To do this, we need to find a function $T(\epsilon, \phi)$ such that:
$$\epsilon \sim p(\epsilon), \, z = T(\epsilon, \phi) \implies z \sim q_\phi(z)$$
\remark
\ti{Usually we start with $p(\epsilon)$ being uniform or Normal. It's not always easy to find these functions.}
\example
$$\epsilon \sim \mc{N}(0,1),\,z = \sigma \epsilon + \mu \implies z \sim \mc{N}(\mu, \sigma)$$
Using this trick, we can write our expectation:
\begin{align*}
	\nabla_\phi \mc{L}(\phi) &= \nabla_\phi \mb{E}_{z \sim q_\phi(z|x)}\left[ \log p(x,z) - \log q_\phi (z|x)\right] \\
	&= \nabla_\phi \mb{E}_{\epsilon \sim p(\epsilon)} \left[ \log p(x, T(\phi, \epsilon)) - \log q_\phi (T(\phi, \epsilon |x)\right]\\
	&= \mb{E}_{\epsilon \sim p(\epsilon)} \nabla_\phi \left[ \log p(x, T(\phi, \epsilon)) - \log q_\phi (T(\phi, \epsilon |x)\right]
\end{align*}
Now we have a differentiable function that we can estimate with simple Monte Carlo. 

\remark
Some notable extensions:
\begin{enumerate}
	\item We can make $q_\phi$ arbitrarily expressive, for instance using a mixture of Gaussians, or a normalizing flow.
	\item We can fit both $\phi$ and parameters of $p(z|x)$ at the same time.
\end{enumerate}
\subsection{Tutorial - Alternative forms of the ELBO}
We have that 
$$L(\phi; x) = ELBO = - \underset{z\sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(x,z)}$$
\paragraph{Form 1}
The most general interpretation of the ELBO is given by
\begin{align*}
	L(\phi; x) &= - \underset{z\sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(x,z)}\\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(x,z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(z)p(x|z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \left[ \log p(x|z) + \log p(z) - \log q_\phi(z|x)\right]
\end{align*}

\paragraph{Form 2}
\ti{Recall that \under{Entropy} is a measure of expected ``surprise": How uncertain are we of the value of a draw from this distribution?}
$$H(X) := -\underset{X\sim p}{\mb{E}}[\log p(X)] = -\sum_{x \in X}p(x) \log p(x)$$
Rewrite Form 1 using entropy
\begin{align*}
	L(\phi; x) &= - \underset{z\sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(x,z)}\\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(x,z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(z)p(x|z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \left[ \log p(x|z) + \log p(z) - \log q_\phi(z|x)\right] \\
	&= \underset{z\sim q_\phi}{\mb{E}} \left[ \log p(x|z) + \log p(z)\right] H\left[ q_\phi(z|x) \right]
\end{align*}

\paragraph{Form 3}
\begin{align*}
	L(\phi; x) &= - \underset{z\sim q_\phi}{\mb{E}} \log \frac{q_\phi(z|x)}{p(x,z)}\\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(x,z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \log \frac{p(z)p(x|z)}{q_\phi(z|x)} \\
	&= \underset{z\sim q_\phi}{\mb{E}} \left[ \log p(x|z)\right] -\underset{z\sim q_\phi}{\mb{E}} \left[ \frac{q_\phi(z|x)}{p(z)} \right] \\
	&= \underset{z\sim q_\phi}{\mb{E}} \left[ \log p(x|z)\right] -D_{KL}(q_\phi(z|x)||p(z))
\end{align*}
This frames the ELBO as a tradeoff. The negative of this function is the \blue{loss function we use for training VAEs}. The first term can be thought of as a "reconstruction likelihood", i.e. how probable is $x$ given $z$, which encourages the model to choose the distribution which best reconstructs the data. The second term acts as regularization, by enforcing the idea that our parameterization shouldn't move us too far from the true distribution.

\subsection{Tutorial - Mean Field Variational Inference}
\todo[inline]{so tired...}


\section{Sampling and Monte Carlo Methods}
\subsection{Sampling}
\definition[sample]
A sample from a distribution $p(x)$ is a \red{single} realization $x$ whose probability distribution is $p(x)$.

\paragraph{Problems to be solved}
Monte Carlo methods are computational techniques that make use of random numbers. The aims are to solve one or both of the following problems
\begin{enumerate}
	\item To generate samples $\{x^{(r)}\}_{r=1}^R$ from a given probability distribution $p(x)$.
	\item To estimate expectations of functions, $\phi(x)$, under the distribution $p(x)$.
	$$\Phi = \mb{E}_{x \sim p(x)}[\phi(x)] = \int \phi(x)p(x) \, dx$$
\end{enumerate}

\definition[simple Monte Carlo]
Given $\{x^{(r)}\}_{r=1}^R \sim p(x)$, we estimate the expectation $\mb{E}_{x\sim p(x)}[f(x)]$ using the average sum, and call it estimator $\hat{\Phi}$:
$$\Phi = \mb{E}_{x\sim p(x)}[f(x)] \sim \frac{1}{R}\sum_{r=1}^R f(x^{(r)}) = \hat{\Phi}$$
\property
If the vectors $\{x^{(r)}\}_{r=1}^R$ are generated from $p(x)$ then the expectation of $\hat{\Phi}$ is $\Phi$. In fact, $\hat{\Phi}$ is an unbiased estimator of $\Phi$.
\begin{proof}
\begin{align*}
	\expect{\hat{\Phi}}_{x\sim p(\{x^{(r)}\}_{r=1}^R)} &= \expect{\frac{1}{R}\sum_{r=1}^R f(x^{(r)})}\\
	&= \frac{1}{R}\sum_{r=1}^R \expect{f(x^{(r)})} \\
	&= \frac{1}{R} \sum_{r=1}^R \underset{x \sim p(x)}{\mb{E}}[f(x)]\\
	&= \frac{R}{R}  \underset{x \sim p(x)}{\mb{E}}[f(x)]\\
	&= \Phi
\end{align*}
\end{proof}
\property
As the number of samples of $R$ increases, the variance of $\hat{\Phi}$ will decrease proportional to $\frac{1}{R}$.
\begin{proof}
	\begin{align*}
		Var[\hat{\Phi}] &= Var\left[\frac{1}{R} \sum_{r=1}^R f(x^{(r)})\right] \\
		&= \frac{1}{R^2}Var\left[\sum_{r=1}^R f(x^{(r)})\right]\\
		&= \frac{1}{R^2}\sum_{r=1}^R Var\left[ f(x^{(r)}) \right] \tag{by i.i.d. assumption}\\
		&= \frac{R}{R^2} Var[f(x)] \\
		&= \frac{1}{R} Var[f(x)]
	\end{align*}
\end{proof} 

\remark
The accuracy of the Monte Carlo estimate depends only on the variance of $\phi$, not on the dimensionality of the $x$. So regardless of the dimensionality of $x$, it may be that as few as a dozen independent samples suffice to estimate $\Phi$ satisfactorily.

\subsection{Importance Sampling}
A method for estimating the expectation of a function. It can be viewed as a generalization of the uniform sampling method.

Assume the density from which we wish to draw samples, $p(x)$, can be evaluated within a multiplicative constant. That is, we can evaluate a function $\tilde{p}(x)$ such that 
$$p(x) = \frac{\tilde{p}(x)}{Z}$$
We further assume we have a simpler density, $q(x)$ from which it is easy to sample from and easy to evaluate
$$q(x) = \frac{\tilde{q}(x)}{Z_q}$$
we call such a density $q(x)$ the \under{sampler density}. \\
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.23]{p9.png}
\end{figure}
\noindent In importance sampling, we generate $R$ samples from $q(x)$
$$\{x^{(r)}\}_{r=1}^R \sim q(x)$$
If these points were samples from $p(x)$, then we could estimate $\Phi$ by a simple Monte Carlo estimator:
$$\Phi = \mb{E}_{x\sim p(x)}[f(x)] \sim \frac{1}{R}\sum_{r=1}^R f(x^{(r)}) = \hat{\Phi}$$
But when we generate samples from $q$, values of $x$ where $q(x)$ is greater than $p(x)$ will be $over-represented$ in this estimator, and points where $q(x)$ is less than $p(x)$ will be $under-represented$. To take into account the fact that we have sampled from the \blue{wrong distribution}, we introduce weights:
$$\tilde{w}_r = \frac{\tilde{p}(x^{(r)}}{\tilde{q}(x^{(r)}}$$
Finally, we rewrite our estimator under $q$
%\begin{aligned}
%& \Phi=\int \phi(x) p(x) d x \\
%=& \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x) d x \\
%\approx & \frac{1}{R} \sum_{r=1}^{R} \phi\left(x^{(r)}\right) \frac{p\left(x^{(r)}\right)}{q\left(x^{(r)}\right)}
%\end{aligned}
\begin{align} 
 \Phi&=\int \phi(x) p(x) d x \\
&= \int \phi(x) \cdot \frac{p(x)}{q(x)} \cdot q(x) d x \\
&\approx  \frac{1}{R} \sum_{r=1}^{R} \phi\left(x^{(r)}\right) \frac{p\left(x^{(r)}\right)}{q\left(x^{(r)}\right)}\\
	&= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})} \\ &= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r \\ &= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} \\ &= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot w_r \\ &= \hat \Phi_{iw} \end{align}
where $\frac{Z_p}{Z_q} = \frac{1}{R}\sum_{r=1}^R\tilde{w}_r, w_r = \frac{\tilde{w}_r}{\sum_{r=1}^R \tilde{w}_r}$, and $\hat \Phi_{iw}$ is our importance weighted estimator.

\remark
$\hat \Phi_{iw}$ is biased.

\subsection{Rejection Sampling}
We assume that a one-dimensional density $p(x) = \frac{\tilde{p}(x)}{Z}$ is too complicated a function for us to be able to sample from it directly. Also, we have a simpler proposal density $q(x)$ which we can evaluate (within a multiplicative factor $Z_q$), and from which we can generate samples. We further assume that we know the value of a constant $c$ such that
$$c\tilde{q}(x) > \tilde{p}(x) \quad x$$
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p10.png}
\end{figure}
\noindent The procedure is as follows:
\begin{enumerate}
	\item Generate two random numbers
	\begin{itemize}
		\item $x$ is generated from the proposal density $q(x)$
		\item $u$ is generated uniformly from the interval $[0, c\tilde{q}(x)]$
	\end{itemize}
	\item Evaluate $\tilde{p}(x)$ and accept or reject the sample $x$ by comparing the value of $u$ with the value of $\tilde{p}(x)$
	\begin{itemize}
		\item If $u > \tilde{p}(x)$, then $x$ is rejected
		\item Otherwise $x$ is accepted; $x$ is added to our set of samples $\{x^{(r)}\}$ and the value of $u$ discarded.
	\end{itemize}
\end{enumerate}
\paragraph{Rejection sampling in high dimensions}
In a high-dimensional problem, it is very likely that the requirement that $c\tilde{q}$ be an upper bound for $\tilde{p}$ will force $c$ to be so huge that acceptances will be very rare indeed. Finding such a value of $c$ may be difficult too, since in many problems we know neither where the modes of $\tilde{p}$ are located nor how high they are.\\
In general, $c$ grows exponentially with the dimensionality $N$, so the acceptance rate is expected to be exponentially small in $N$.

\remark
Importance sampling and rejection sampling work well only if the proposal density $q(x)$ is similar to $p(x)$. In high dimensions, it is hard to find one such $q$.

\subsection{Markov Chain Monte Carlo (MCMC)}
Construct a Markov chain over the assignments to a probability function $p$; the chain will have a stationary distribution equal to $p$ itself; by running the chain for some number of time, we will thus sample from $p$.

\subsubsection{Metropolis-Hastings method}
Makes use of a \under{proposal density} $q$ which depends on the current state $x^{(t)}$. The density $q(x'|x^{(t)})$ might be a simple distribution such as a Gaussian centred on the current $x^{(t)}$, but in general can be any fixed density from which we can draw samples.
\remark
\ti{In contrast to importance sampling and rejection sampling, it is not necessary $q(x'|x^{(t)})$ look at all similar to $p(x)$ in order for the algorithm to be practically useful. An example of a proposal density with two different states $(x^{(1)}, x^{(2)})$.}\\\\
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p11.png}
\end{figure}
\noindent We assume we can evaluate $\tilde{p}(x)$ for any $x$. The procedure is as follows:\\
A tentative new state $x'$ is generated from the proposal density $q(x'|x^{(t)})$. To decide whether to accept the new state, we compute
$$a = \frac{\tilde{p}(x')q(x^{(t)}|x')}{\tilde{p}(x^{(t)})q(x'|x^{(t)})}$$
If $a \geq 1$< then the new state is accepted; Otherwise, the new state is accepted with probability $a$.\\
If accepted, set $x^{(t+1)} = x'$. Otherwise, set $x^{(t+1)} = x^{(t)}$.

\theorem
As $t \rightarrow \infty$, $\{x^{(r)}\}_{r=1}^R \rightarrow p(x)$ for any $q(x'|x^{(t)}) \geq 0$.

\remark
\ti{Just as it was difficult to estimate the variance of an importance sampling estimator, so it is difficult to assess whether a Markov chain Monte Carlo method has `converged, and to quantify how long one has to wait to obtain samples that are effectively independent samples from $p$.}

\section{Amortized Inference and Variational Auto-Encoders}
``Amortize" just means ``spread out a cost over time". Instead of doing SVI from scratch every time we see a new datapoint, we're going to try to gradually learn a function that can look at the data for a person $x_i$, and then output an approximate posterior $q_\phi(z_i|x_i)$. We'll call this a ``recognition model".\\
Instead of a separate $\phi_i$ for each data example, we'll just have a single global $\phi$ that specifies the parameters of the recognition model. Because the relationship between data and posteriors is complex and hard to specify by hand, we'll do this with a neural network.\\
\subsection{the algorithm for amortized inference}
\begin{enumerate}
	\item Sample a datapoint
	\item Compute parameters $\phi$ of approximate posterior
	\item Compute gradient of Monte Carlo estimate of ELBO wrt $\phi$
	\item Update $\phi$
\end{enumerate}

\subsection{Optimizing model parameters}
\unsure{Didn't really understand this section}

\subsection{Variational Autoencoder (VAE)}
An autoencoder takes some data as input and discovers some latent state representation of the data. The encoder network takes in the input data (such as an image) and outputs a single value for each encoding dimension. The decoder takes this encoding and attempts to recreate the original input. 
\example{MNIST}
Let's give an explicit model for MNIST images of handwritten digits.\\
\info{step 1: insert a prior of each latent variable; step 2: estimate the distribution of the observed variable; step 3: calculate the posterior}
We will choose our prior on $z$ to be the standard Gaussian $$\mc{N}(0,I)$$
Our likelihood function is
$$p_{\theta}(x_i|z_i) = \prod_{d=1}^D Ber(x_{id}|\mu_{\theta}(z_i))$$
Our approximate posterior is
$$q_\phi(z|x) = \mc{N}(\mu(x), \sigma(x)I)$$
Define our encoder and decoder to be\\
\tb{Encoder}: $g_\phi(x_i) = \phi_i = (\mu_i, \log \sigma_i)$ (learning the distribution of $z$)\\
Inputs $x_i$s are encoded to vectors $\mu$ and $\log \sigma_i$, which parameterize $q_\phi(z|x)$. Before decoding, we draw a sample $z \sim q_\phi(z|x) = \mc{N}(\mu(x). \sigma(x)I)$.\\
\tb{Decoder}: $f_\theta(z_i) = \theta_i$ (reconstructing the distribution of $x$)\\
Then evaluate $p_\theta(x_i|z_i)$. We compute the loos function (negative ELBO) and propagate its derivative with respect to $\theta$ and $\phi$, through the networks during training.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p12.png}
\end{figure}

\paragraph{Deterministic Autoencoders}
An autoencoder takes an input, encodes it into a vector, then decodes to produce something similar to the original data. Or: autoencoders reconstruct their own input using an encoder and a decoder. \\
\tb{Encoder}: $g(x) \rightarrow z$\\
\tb{Decoder}: $f(z) \rightarrow \hat{x}$\\
The encoder, $g(x)$, takes in the input data (such as an image) and outputs a single value for each encoding dimension while the The decoder, $f(z)$ takes this encoding and attempts to recreate the original input.\\
Our goal is to learn $g, f$ from unlabeled data, and usually we specify $f$ and $g$ with neural networks, and minimize squared reconstruction error.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{p13.png}
\end{figure}
\noindent $z$ is the code the model attempts to compress a representation of the input, $x$, into.
It is important that the encoder reduces the dimension, for example by learning how to ignore noise (otherwise, we would just learn the identify function). The big idea is that the code contains only the most important features of the input, such that we can reconstruct the input from the code reliably
$$\tilde{x} = f(g(x)) \approx x$$

\paragraph{Problem 1 - Proximity in data space does not mean proximity in feature space}
$$x_1 \approx x_2 \notimplies z_1 \approx z_2$$
\paragraph{Problem 2: ``White" latent space region}
If the space has regions where no data gets encoded to, and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space.\\
\tb{Solution: Adding noise to autoencoders}
\begin{itemize}
	\item Can add noise to data before encoding, reconstruct original data. But how much noise? 
	\item Can add noise to latents after encoding, reconstruct original data. But how much noise?
\end{itemize}

\paragraph{Solution - Variational Autoencoders (VAEs)}
This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat \blue{vary} on every single pass simply due to \blue{sampling}. \\
\tb{Why does a VAE solve the problems of a deterministic autoencoder?}\\
The VAE generation model learns to reconstruct its inputs not only from the encoded points but also from the area around them. This allows the generation model to generate new data by sampling from an ``area" instead of only being able to generate already seen data corresponding to the particular fixed encoded points.

\section{Normalizing Flows}
Normalizing Flows(NF)\footnote{reference: \url{https://arxiv.org/pdf/1908.09257.pdf}} are a family of generative models with tractable distributions where both sampling and density evaluation can be efficient and exact.

\paragraph{Applications}
\begin{itemize}
	\item Image generation
	\item Video generation
	\item Audio generation
	\item Graph generation
	\item Reinforcement learning
\end{itemize}
%We start with some data generative by a true distribution:
%$$x_i \sim P_{data}$$
%The goal is to learn a distribution $P_X \approx P_{data}$ s.t.
%\begin{enumerate}
%	\item The sampling / generation process $x \sim P_X$ is \tb{tractable}, and $x$ looks like samples from $P_{data}$.
%	\item Good for density estimation, so that $P_X(x)$ can be evaluated, and is high if $x$ looks like data, low if $x$ does not look like data.
%\end{enumerate}

\definition
Normalizing Flows is a learned transformation of a simple distribution (base) to a complex distribution (target) by a sequence of invertible and differentiable mappings.

\subsection{Basics}
Let $Z \in \real^D$ be a random variable with a known and tractable probability density function $p_Z:\real^D \rightarrow \real$. Let $f$ be an invertible function and $Y = f(Z)$. 
\definition[generative direction]
Then using the change of variables formula, one can compute the probability density function of the random variable $Y$:
\begin{equation}\label{eq1}
	p_Y(y) = p_Z(g(y))|\det Dg(y)| = p_Z(g(y))|\det Df(g(y))|^{-1}
\end{equation}
where $g$ is the inverse of $f$, $Dg(u) = \frac{\partial g}{\partial y}$ is the Jacobian of $G$ and $Df(z) = \frac{\partial f}{\partial z}$ is the Jacobian of $f$. This new density function $p_Y(y)$ is called a \under{pushforward} of the density $p_Z$ by the function $f$ and denoted by $f_{\ast}p_Z$.\\
In the context of generative models, the above function $f$ (a generator) ``pushes forward" the base density $p_Z$ to a more complex density. This movement from base density to final complicated density is the \under{generative direction}
\remark
\ti{To generate a data point $y$, one can sample $z$ from the base distribution, and then apply the generator $y = f(z)$.}
\definition[normalizing direction]
The inverse function $g$ moves in the opposite, \under{normalizing direction}: from a complicated and irregular data distribution towards the simpler, more regular or ``normal" form, of the base measure $p_Z$.

\fact
If the transformation $f$ can be arbitrarily complex, one can generate any distribution $p_Y$ from any base distribution $p_Z$.

\paragraph{Constructing flows}
Constructing arbitrarily complicated non-linear invertible functions(bijections) can be difficult. Since the composition of invertible functions is itself invertible, and the determinant of its Jacobian has a specific form, we can use a sequence of simple bijections which are convenient to compute, invert, and calculate the determinant of their Jacobian.\\
In particular, let $f_1, \hdots, f_N$ be a set of $N$ bijective functions and define 
$f = f_N \circ f_{N-1} \circ \hdots \circ f_1$ to be a composition of the functions. It can be shown that $f$ is also bijective, with inverse 
$$g = g_1 \circ \hdots \circ g_{N-1} \circ g_N$$
and the determinant of the Jacobian is
$$\det Dg(y) = \prod_{i=1}^N \det Dg_i(x_i)$$
where $x_i = f_i \circ \hdots \circ f_1(z) = g_{i+1} \circ \hdots \circ g_N(y)$ and so $x_N = y$.

\subsubsection{More formal construction}
\definition If $(Z, \Sigma_Z), (Y, \Sigma_Y)$ are measurable spaces, $f$ is a measurable mapping between them, and $\mu$ is a measure on $Z$, then one can define a measure on $Y$ (called the \under{pushforward measure} and denoted by $f_{\ast}\mu$ by
$$f_{\ast}\mu(U) = \mu(f^{-1}(U)), \quad \text{for all } U \in \Sigma_Y$$\\
Data can be understood as a sample from a measured ``data" space $(Y, \Sigma_Y, v)$ which we want to learn. To do that one can introduce a simpler measured space $(Z, \Sigma_Z, \mu$ and find a function $f: Z \rightarrow Y$ s.t. $v = f_{\ast}\mu$. This function $f$ can be interpreted as a ``generator", and $Z$ as a latent space.\\
Assume $Z = \real^D$, all sigma-algebras are Bore,, and all measures are absolutely continuous with respect to Lebesgue measure(i.e. $\mu = p_Zdz$)
\definition A function $f: \real^D \rightarrow \real^D$ is called a \under{diffeomorphism}, if it is bijective, differentiable, and its inverse is differentiable as well.\\
The pushforward of an absolutely continuous measure $p_Zdz$ by a diffeomorphism $f$ is also absolutely continuous with a density function given by (\ref{eq1}).

\subsection{Applications}
\subsubsection{Density estimation and sampling}
For simplicity assume that only a single flow, $f$, is used and it is parameterized by the vector $\theta$. Further assume that the base measure $p_Z$ is given and is parameterized by the vector $\phi$.\\
Given a set of data observed from some complicated distribution, $D = \{y_i\}_{i=1}^M$, we can then perform likelihood-based estimation of the parameters $\Theta = (\theta, \phi)$. The data likelihood in this case simply becomes
\begin{align*}
	\log p(D|\Theta) &= \sum_{i=1}^M \log p_Y(y_i|\Theta) \\
	&= \sum_{i=1}^M \log p_Z(g(y_i|\theta)|\phi) + \log |\det Dg(y_i|\theta)|
\end{align*}
\blue{During training, the parameters of the flow ($\theta$) and of the base distribution ($\phi$) are adjusted to maximize the log-likelihood.}

\subsection{Methods}
Normalizing Flows should satisfy several conditions in order to be practical. They should
\begin{itemize}
	\item Be invertible (for estimating the density we need to know their inverse)
	\item Be expressive enough to model real distributions
	\item Be computationally efficient: calculation of the Jacobian determinant, sampling from the base distribution, and application of the forward and inverse functions should be tractable.
\end{itemize}
In this section, we describe different types of flows and comment on the above properties.
\subsubsection{Elementwise bijections}
Let $h: \real \rightarrow \real$ be a scalar valued bijection. Then, if $x = (x_1, x_2, \hdots, x_D)^T$,
$$f(x) = (h(x_1), h(x_2), \hdots, h(x_D))^T$$
is also a bijection whose inverse simply requires computing $h^{-1}$ and whose Jacobian is the product of the absolute values of the derivatives of $h$ (since the Jacobian matrix is diagonal).\\
This can also be generalized by allowing each element to have its own distinct bijective function.
\paragraph{Problem}
Elementwise operations cannot express any form of correlation between dimensions.
\subsubsection{Linear Flows}
Linear mappings can express correlation between dimensions:
$$f(x) = Ax + b$$
where $A \in \real^{D \times D}$ and $b \in \real^D$ are parameters. If $A$ is an invertible matrix, the function is invertible.
\paragraph{Problem}
Limited expressiveness.
\example[Gaussian base distribution]
$$p_Z(z) = \mc{N}(z, \mu, \Sigma)$$
After transformation by a linear flow, the distribution remains Gaussian:
$$p_Y = \mc{N}(y, A\mu+b, A^T\Sigma A)$$
\blue{More generally, a linear flow of a distribution from the exponential family remains in the exponential family.}\\
\paragraph{Computing the determinant}
$$|\det Df(x)| = \det(A)$$
which can be computed in $\mc{O}(D^3)$, which is expensive for large $D$. By restricting the form of $A$ (at the expense of expressive power) we can avoid the computational costs:
\paragraph{Diagonal}
If $A$ is diagonal with nonzero diagonal entries, then complexity of computing inverse or the determinant is $\mc{O}(D)$. However, the result is an elementwise transformation and cannot express correlation between dimensions.
\remark
A diagonal linear flow is useful for representing normalization layers, which have become a ubiquitous part of modern neural networks.
\paragraph{Triangular}
More expressive form of linear transformation. Taking the determinant costs $\mc{O}(D)$, inversion costs $\mc{O}(D^2)$ (a single pass of back-substitution).

\paragraph{Permutation and Orthogonal}
The expressiveness of triangular transformations is sensitive to the ordering of dimensions. Reordering the dimensions can be done easily using a permutation matrix (absolute determinant = 1).\\
However, the permutations cannot be directly optimized.\\
A more general alternative is orthogonal transformations, whose inverse and absolute determinant are both trivial to compute.
\paragraph{Factorizations}
Instead of limiting the form of $A$, one can also use the LU factorization:
$$f(x) = PLUx + b$$
where $L$ is lower triangular with ones on the diagonal, $U$ is upper triangular with non-zero diagonal entries, and $P$ is a permutation matrix.\\
The determinant is the product of the diagonal entries of $U$ which can be computed in $\mc{O}(D)$. The inverse of $f$ can be computed using two passes of backward substitution in $\mc{O}(D^2)$. \\
However, the discrete permutation $P$ cannot be easily optimized.
\paragraph{Convolution}
Easy to compute, but it can be difficult to efficiently calculate the determinant or unsure invertibility.
\subsubsection{Planar and Radial Flows}
Relatively simple but inverses aren't easily computed $\implies$ not widely used in practice.\\
\paragraph{Planar flows}
Expand and contract the distribution along certain specific directions:
$$f(x) = x + uh(w^Tx + b)$$
where $u, w \in \real^D$ and $b \in \real$ are parameters and $h: \real \rightarrow \real$ is a smooth non-linearity. The Jacobian determinant for this transformation is
\begin{align*}
	\det \left( \frac{\partial f}{\partial x} \right)&= \det (\id{D} + uh'(w^Tx + b)w^T) \\
	&= 1 + h'(w^Tx + b)u^Tw
\end{align*}
This can be computed in $\mc{O}(D)$ time. \\
\blue{The inversion of this flow isn't possible in closed form and may not exist for certain choices of $h(\cdot)$ and certain parameter settings.}\\\\
\paragraph{Sylvester flows}
A more expressive form of planar flows:
$$f(x) = x + Uh(W^Tx + b)$$
where $U, W \in \real^{D \times M}, b \in \real^M, h:\real^M \rightarrow \real^M$ is an elementwise smooth nonlinearity, where $M \leq D$ is a hyperparameter to choose and which can be interpreted as the dimension of a hidden layer. In this case the Jacobian determinant is
\begin{align*}
	\det \left( \frac{\partial f}{\partial x} \right) &= \det(\id{D} + U diag(h'(W^Tx + b)) W^T) \\
	&= \det (\id{M} + diag(h'(W^Tx + b)) WU^T)
\end{align*}
This can be computationally efficient if $M$ is small.\\\\
\paragraph{Radial flows}
Radial flows instead modify the distribution around a specific point so that
$$f(x) = x + \frac{\beta}{\alpha + ||x - x_0||}(x - x_0)$$
where $x_0 \in \real^D$ is the point around which the distribution is distorted, and $\alpha, \beta \in \real$ are parameters, $\alpha > 0$.\\
The Jacobian determinant can be computed relatively efficiently, but the inverse cannot be given in closed form but does exist under suitable constraints on the parameters.\\

\subsubsection{Coupling Flows}
One of the most widely used flow architectures.
\paragraph{Coupling flows}
Consider a (disjoint) partition of the input $x \in \real^D$ into two subspaces: $(x^A,x^B) \in \real^d \times \real^{D-d}$ and a bijection $\hat{f}(\cdot; \theta): \real^d \rightarrow \real^d$, parametrized by $\theta$. Then one can define a function $f: \real^D \rightarrow \real^D$ by the formula:
\begin{align*}
	y^A &= \hat{f}(x^A;\Theta(x^B))\\
	y^B &= x^B
\end{align*}
where the parameters $\theta$ are defined by arbitrary function $\Theta(x^B)$ which only uses $x^B$ as input. This function is called a \under{conditioner}. The function $\hat{f}$ is called a \under{coupling layer}, and the resulting function $f$ is called a \under{coupling flow}.\\
A coupling flow is invertible if and only if $\hat{f}$ is invertible and has inverse:
\begin{align*}
	x^A &= \hat{f}^{-1}(y^A;\Theta(x^B))\\
	x^B &= y^B
\end{align*}
The Jacobian of $f$ is a block triangular matrix where the diagonal blocks are $D\hat{f}$ and the identity matrix respectively. Hence 
$$\det Df = \det D\hat{f}$$
Most coupling layers are applied to $x^A$ element-wise:
$$\hat{f}(x^A;\theta) = (\hat{f}_1(x_1^A;\theta_1),\hdots,\hat{f}_d(x_d^A;\theta_d))$$
where each $\hat{f}_i(\cdot;\theta_i):\real \rightarrow\real$ is a scalar bijection. In this case a coupling flow is a triangular transformation (i.e., has a triangular Jacobian matrix).\\
\blue{The power of a coupling flow resides in the ability if a conditioner $\Theta(x^B)$ to be arbitrarily complex. In practice it is usually modelled as a neural network (e.g. a shallow ResNet).}

\subsubsection{Autoregressive Flows}
One of the most widely used flow architectures.\\
\paragraph{Direct autoregressive flows}
Let $\hat{f}(\cdot;\theta): \real \rightarrow \real$ be a bijection parameterized by $\theta$. Then an autoregressive model is a function $f: \real^D \rightarrow \real^D$, which outputs each entry of $y = f(x)$ conditioned on the previous entries of the input:
$$y_t = \hat{f}(x_t; \Theta_t(x_{1:t-1}))$$
For each $t = 2, \hdots, D$ we choose arbitrary functions $\Theta_t(\cdot)$ mapping $\real^{t-1}$ to the set of all parameters, and $\Theta_1$ is a constant. The functions $\Theta_t(\cdot)$ are called \under{conditioners}. \\
The Jacobian matrix of the autoregressive transformation $f$ is triangular. Each output $y_t$ only depends on $x_{1:t}$, and so the determinant is just a product of its diagonal entries:
$$\det(Df) = \prod_{t=1}^D \frac{\partial \hat{f}}{\partial x_t}$$
In practice, it is possible to compute all the entires of the direct flow in one pass using a single network with appropriate masks.\\
However, the computation of the inverse is more challenging (involves recursion, cannot be parallelized)

\paragraph{Inverse autoregressive flows (IAF)}
Outputs each entry of $y$ conditioned the previous entries of $y$ (w.r.t. the fixed ordering)
$$y_t = \hat{f}(x_t;\theta_t(y_{1:t-1}))$$
Computation of the IAF is sequential and expensive, but the inverse of IAD (which is a direct autoregressive flow) can be computed relatively efficiently.

\remark
One should use IAFs if fast sampling is needed (e.g. for stochastic variational inference), and DAFs if fast density estimation is desirable. However, the two methods are theoretically equivalent and can learn the same distribution.

\subsubsection{Residual Flows}
Residual networks are compositions of the functions of the form
$$f(x) = x + F(x)$$
Such a function is called a \under{residual connection}, and here the \under{residual block} $F(\cdot)$ is a feed-forward neural network of any kind.\\
The central idea of a reversible network architecture based on residual connections is a variation of additive coupling layers: consider a disjoint partition of $\real^D = \real^d \times \real^{D-d}$ denoted by $x = (x^A, x^B)$ for the input and $y = (y^A, y^B)$ for the output, and define a function:
$$y^A = x^A + F(x^B)$$
$$y^B = x^B + G(y^A)$$
where $F: \real^{D-d} \rightarrow \real^d$ and $G: \real^d \rightarrow \real^{D-d}$ are residual blocks.\\
This network is invertible but computation of the Jacobian is inefficient.
\subsubsection{Infinitesimal(Continuous) Flows}
The residual connections discussed in the previous section can be viewed as discretizations of a first order ordinary differential equation (ODE):
$$\frac{d}{dt} x(t) = F(x(t), \theta(t))$$
where $F: \real^D \times \Theta \rightarrow \real^D$ is a function which determines the dynamic (the \under{evolution function}), $\Theta$ is a set of parameters and $\theta: \real \rightarrow \Theta$ is a parameterization. The discretization of this equation (Euler's method) is
$$x_{n+1} - x_n = \epsilon F(x_n,\theta_n)$$
and this is equivalent to a residual connection with a residual block $\epsilon F(\cdot, \theta_n)$. \\
We can consider the case where we do not discretize but try to learn the continuous dynamical system instead. Such flows are called \under{infinitesimal} or \under{continuous}. There are two distinct types: \tb{ODE-based methods} and \tb{SDE-based methods (Langevin flows)}.













\end{document}